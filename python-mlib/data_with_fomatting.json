[
  {
    "Algorithms_and_Complexity": [
      "dynamic programming (also known as dynamic optimization) is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.",
      "Brute-force search. ... In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement",
      "Instance simplification: the instances of the problem can be transformed into an easier instance to solve. Representation change: the data structure can be transformed so that it is more efficient. Problem reduction: the problem can be transformed to an easier problem to solve.",
      "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order",
      "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time",
      "Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order",
      "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly",
      "An in-place sort algorithm that repeatedly reorders different pairs of items. On each pass swap pairs of items separated by the increment or gap, if needed, and reduce the gap",
      "Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (\"backtracks\") as soon as it determines that c cannot possibly be completed to a valid solution",
      "sorting algorithm is an algorithm that puts elements of a list in a certain order",
      "Self contained sequence of actions to be performed",
      "In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output.",
      "A search algorithm is the step-by-step procedure used to locate specific data among a collection of data",
      "In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists",
      "Reduce problem instance to smaller instance of the same problem and extend solution. Solve smaller instance. Extend solution of smaller instance to obtain solution to original problem",
      "A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum.",
      "In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms.",
      "Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort).",
      "A sorting algorithm that works by first organizing the data to be sorted into a special type of binary tree called a heap. The heap itself has, by definition, the largest value at the top of the tree, so the heap sort algorithm must also reverse the order"
    ]
  },
  {
    "Artificial_intelligence_and_machine_learning": [
      "Data cleansing, data cleaning, or data scrubbing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.",
      "The internal structure of discourse and the computational planning and generation of coherent multisentential paragraphs has been a topic of investigation at USC/ISI since the early 1980's. A theory of the interclausal relationships that govern discourse structure, called Rhetorical Structure Theory (RST), was developed in [Mann and Thompson 88] after extensive analysis of hundreds of texts of various genres. The analysis concluded that English text is coherent by virtue of so-called rhetorical relations that hold between clauses and blocks of clauses, and identified about 25 basic relations for English. These relations, such as Sequence, Purpose, and Elaboration are usually identified in English by key words or phrases (such as \"then\", \"in order to\", and \"e.g.\", respectively).",
      "Knowledge representation and reasoning (KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language.",
      "the branch of technology that deals with the design, construction, operation, and application of robots.\n    \"civilian research on robotics is advancing swiftly\"",
      "includes process image for feature extraction.",
      "In linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings.",
      "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.  Data preprocessing is used database-driven applications such as customer relationship management and rule-based applications (like neural networks).",
      "the study of the forms of things, in particular. a particular form, shape, or structure.",
      "Sentence planning  involves aggregating content into sentence-sized units and the selecting the lexical and syntactic elements that are used in realizing each sentence.",
      "Natural Language Processing (NLP) refers to AI method of communicating with an intelligent systems using a natural language such as English.",
      "the practice of examining large pre-existing databases in order to generate new information.",
      "the system of contrastive relationships among the speech sounds that constitute the fundamental components of a language.     the study of phonological relationships within a language or between different languages.",
      "the branch of linguistics dealing with language in use and the contexts in which it is used, including such matters as deixis, the taking of turns in conversation, text organization, presupposition, and implicature.",
      "Syntax is the level at which we study how words combine to form phrases, phrases combine to form clauses and clauses join to make sentences. Syntactic analysis concerns sentence formation. It deals with how words can be put together to form correct sentences. It also determines what structural role each word plays in the sentence and what phrases are subparts of what other phrases.",
      "In linguistics, realization is the process by which some kind of surface representation is derived from its underlying representation; that is, the way in which some abstract object of linguistic analysis comes to be produced in actual language. Phonemes are often said to be realized by speech sounds. The different sounds that can realize a particular phoneme are called its allophones.",
      "Syntactic ambiguity, also called amphiboly or amphibology, is a situation where a sentence may be interpreted in more than one way due to ambiguous sentence structure",
      "Computer vision is the science that aims to give a similar, if not better, capability to a machine or computer. Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images.",
      "syntax analysis or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech)",
      "Discourse concerns connected sentences. It is a study of chunks of language which are bigger than a single sentence. Dicourse language concerns inter-sentential links that is how the immediately preceding sentences affect the interpretation of the next sentence. Discourse knowledge is important for interpreting pronouns and temporal aspects of the information conveyed.",
      "Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning. ... In machine learning, pattern recognition is the assignment of a label to a given input value.",
      "the branch of linguistics and logic concerned with meaning. The two main areas are logical semantics, concerned with matters such as sense and reference and presupposition and implication, and lexical semantics, concerned with the analysis of word meanings and relations between them.",
      "Pragmatics is a subfield of linguistics and semiotics that studies the ways in which context contributes to meaning. Pragmatics encompasses speech act theory, conversational implicature, talk in interaction and other approaches to language behavior in philosophy, sociology, linguistics and anthropology.",
      "a morphological element considered in respect of its functional relations in a linguistic system.",
      "includes object recognition and 3D scene Interpretation",
      "Word knowledge is nothing but everyday knowledge that all speakers share about the world. It includes the general knowledge about the structure of the world and what each language user must know about the other user’s beliefs and goals. This essential to make the language understanding much better.",
      "Natural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem.\n\nThe process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language",
      "Natural language generation (NLG) is the natural language processing task of generating natural language from a machine representation system such as a knowledge base or a logical form.",
      "the analysis and manipulation of a digitized image, especially in order to improve its quality.\n    \"image processing software\"",
      "Referential ambiguity occurs when a word or phrase, in the context of a particular sentence, could. refer to two or more properties or things. It is sometimes clear from the context which meaning.",
      "the theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.",
      "The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process. ... For general information about ML models and ML algorithms, see Machine Learning Concepts.",
      "Lexical analysis is the first phase of a compiler. It takes the modified source code from language preprocessors that are written in the form of sentences. The lexical analyzer breaks these syntaxes into a series of tokens, by removing any whitespace or comments in the source code.",
      "Lexical ambiguity is the presence of two or more possible meanings within a single word. Also called semantic ambiguity or homonymy. Compare to syntactic ambiguity. Lexical ambiguity is sometimes used deliberately to create puns and other types of word play."
    ]
  },
  {
    "Computational_Biology": [
      "Computational Biology, sometimes referred to as bioinformatics, is the science of using biological data to develop algorithms and relations among various biological systems",
	"tissues may expressed across genes redundant example restricted instance contradictory clarify since combinatorial activating multiple among many regions ages spatial spatiotemporal bhlh different interactions types involved often cooperative measurements combined cooperation appear neuronal differentiation transcription expression 13 set show case factors whose like characterizing family brain required patterns seem 15 complex explained target 14",
	"machine expressed genes automatically using scan based layers celltype spatial exclusively modeling ish database identify detect propose approach learned expression insitu markers genomewide observed known patterns layerspecific hybridization vision method another specific images",
	"applied evaluated genes purkinje inspection molecular associated localized structures four auc accuracy layers main new detected successfully primarily previously identify predictions data matter manual furthermore cerebellum full classify overall classifiers granular 250 markers correctly hundreds never top proposed genome heldout shows class layerspecific layer similar several trained mouse white specific",
	"applications may resolution expressed scales structure 23 genes determined advantage gross structures information classification accuracy multiple many layers spatial “textures” cells analyze different also extracting b take lead analysis aspect scale “fingerlike” texture resolutions time improves sources features exhibit expression contains cerebellum show combining current particular problem observed well image width patterns important layerspecific 4 distribution figure refined downsampling used coarse millimeters",
	"complexity staggeringly across intuit understanding learn make everchanging billion civilization exaggeration without system others lifespans many ways estimated represents social given 100 1 aspect computational synapses environment navigate human allows consisting neurons minds sensory actions greatest entire one connected brain technology world every brains plan complex sense said rise remember collective frontiers us trillion",
	"metadna dna strands natural functionality acid taken program better behavior types acidsenzyme adapted powerful leverages extensive entirely predictively challenges develop possible systems reengineering system abbreviated based biology work versions use modern approach chemistry available extent manipulate fundamental artificial originally programmability synthesize provided understanding nanostructures biochemistry since number acids protein biochemical prior enzymes key later mdna synthetic call goal evolved limited man practice development manipulating desired design advantage offer tools cells also laboratory provide molecules basic component motivation enable biological processes assemble selfassembled one easier difficult far aspects reengineer developing nucleic metabiochemical modify mimic",
	"signal diverge function structure problems similarities sequences unsolved structures using longest detection—is distance structural large biology slowly remote relationships—protein detect strongly studied computational protein data still detection—this due similarity highquality challenge one problem 3d evolutionary always far proteins infer fundamental homology pressure evolutionarily conserved primary subtle correlated sequence faint—socalled much abundant",
	"nodes 57 phylogeny present might therefore simple suited data enumerating none evolution whole identical branches identifying algorithm construct linear notion process root conserved study subsequences tags descent collection develop containing common possible want enumerate example shared discovery genomes share found space single running given use identify amongst however approach except disjoint used called finds guaranteed error substrings since nearlinear large analyze time show set problem evolutionary model finding exist nonmammals captures may tag taggeneration essentially insight stochastic two finally expense microbial phylogenetic tree approximate mammals ways faster generalized generalize generating find programmingbased chance enumeration one small substring sublinear sets",
	"data—namely scores—we two sequences structures consists share structural filtering training database 95 random protein excluded data 7329 scop v159 filter set similarity selected greater resulted 16 category 3d labeled pairwise 97 proteins entirely astral compute identity used test labels",
	"computed improve framework parameters sequences information reflects better multitask methods proximity query ranking data due flexible algorithm sensitive proteins leads test retrieved descent scores performance rather choices using based alignment efficiently space given determining task use label scale domain homologs pairwise stateoftheart homology alignments gradient sequence used default nearby efficient classes coordinates potentially structural enables supplied hhsearch protein time embedding similarity auxiliary protembed learns together known millions accurate stochastic psiblast takes run underlying relationships eg shown training also database faster additional could labeled less method moreover slower precomputed",
	"machine expressed genes automatically using scan based layers celltype spatial exclusively modeling ish database identify detect propose approach learned expression insitu markers genomewide observed known patterns layerspecific hybridization vision method another specific images"


    ]
  },
  {
    "Computer_Architecture_and_Design": [
      "Computer architecture is a specification detailing how a set of software and hardware technology standards interact to form a computer system or platform. In short, computer architecture refers to how a computer system is designed and what technologies it is compatible with.",
	"signal complexity internet dimmable smart cc peak require lives topology iot luminaire design architecture 40w cv 3w drive flickering paper areas system ideal based avoid presented therefore main constant lighting control modeling successfully sector effective 12v scheme conventional mode additional features aux domains novel current advantages voltage supply auxiliary separate continues reduce led connected drivers option train influencing well proposed facilitate cost things proliferate solutions ramp load small flyback driver point implemented view power decouple output integrating highlight nonlinear",
	"availability either state solid lives understanding impact 6 technologies drive cached days paper based growing largescale many hardware durability better different use flash reliability types feature provides collected environment slc order data drives 50nm field including depend sizes failures center errors factors production ranging 24nm critically analyses millions technology models prevalent detailed frequency study spans nand ten mlc continuously years derive",
	"traces complexity saving techniques google’s v8 practi javascript development performance web relation android mobile highperformance websites phones mandating improvement paper generated browsing increased presents discuss recent policies allo work analyze scaling potentials traffic lead although google browsers dvfs dynamic significant consumption management significantly voltage thread workload believe engine case platform considering devices well powergating addressed cal detailed engines browser frequency power study cation volume used years multi much heterogeneous duction processing hmp",
	"demonstrate latency affordability 90 computing becomes across alongside evaluate batch meets evaluated utilizations low isolation server resources maximizing mechanisms services network using shared since present workloads reusing largescale datacenters slowing service multiple hardware enables opportunity given scaling daily latencysensitive spikes scenarios websearch traffic software google contention heracles rarely colocation violate latencycritical dynamically cpu periods ensure resulting underutilization address tasks production without energyefficiency done memory cause hurts targets underutilize technology important objectives load besteffort manages job violations userfacing controller servicelevel feedbackbased average safe",
	"article workshop attempted potential systems commercial dormancy upcoming decadelong reasons today disseminated micro46 ndp many speakers frameworks describes bigdata researched given actively motivates movement data spiked topic attended widely gained compelling examination keynote key identifies well cost organized interest 1990s organizers insights capture realizing neardata careful little processing concept challenges traction",
	"amd port programmability latency nodes security andor performance low router degrading server describe technologies implement degraded network hangs system 8node multiple based result processorinterconnect interconnect different new include minimizing modified attack provide interconnected vulnerability attacks terms however propose component determines servers arriving livelock sockets show hypertransport highbandwidth significantly processor intel various consist together exchange key bandwidth well tradeoff communicate cost modifications alternative solutions packet flexibility routers routing tables procedures table output forwarded qpi packets",
	"applications machine computing simulations high performance considered practical design conceptual architecture 2003 technologies paper presents computer presented dataflow portable use fdtd basic detail 2d time microwave product domain finite one already 3d frequency difference concept industry dedicated",
	"weigh sensing low cubesats solely cots functionality interfacing cyclone account due various commerciallyoff detection supports satellite controlling focus warning surface systems electronics commercial fire perform serve temperature research scientific system based forest space directly include given compactness designed socalled onboard like theshelf obc artificial handful observations controller worth used components visible technologies computer large activities revolutionized standardized expected mainly time autonomous acts satellites 15kg platform purpose missions payloads world power imaging availability real function built proved terrestrial design payload paper capability nano shown discuss remote hardware different sea dual also detail 1 taking cloud component cover forecast processor weather brain cannot aspects every monitoring multifarious thermal",
	"form mentioned titles require organization conceptual fulfill exists information functionality role learning relations behavior sweebok unable semester electronic hand activity powerful challenge deals various engineering introduced curriculum classroom devices visibility required process need study parts appreciate combines specifies possible bridged performance systems impact three wide recommendations course came system 8 work instruction designed workings 3rd integral architectures overall semesters offered earlier detailed viz organization” helped skills calls provided accordingly understanding components 3 since courses ability computer continuous lower semantic mainly knowledge acquire lifetime set profession students “advanced always actually users yet gap intended 4 cheaper processorlevel architecture” processing values next requirement “computer design architecture two operation concepts underlying throughout existed related hardware self play faster within delivery ieee elements processor facets compare",
	"real instead properly 32bit mips reveals design rather architecture pipelined two complete fivestage via 2009 simulators programmable gate course paper system scheduling enforced presents computer proper learning educate many methodology array work milestones independent successfully reveal experiments instructions board achieve 2010 help differs readymade time field completed cpu requires set 21 previous flawless student one students case purpose fpga effectiveness implementation survey utilizing weeks samples ttest models handson class every altera study experimenting de2 effectively project devised enough addition goal",
	"article trend settlement progress application quality comfortable design architecture efficiency ecological points computer degradation greatly also bim analysis architectural environment mainly human environmental accelerate mold green healthy raise inevitable definitely technology makes energy aided backgroud depletion develop",
	"named transaction converted becomes testbench high performance framework components engineers functional design architecture level tlm integrated paper wxpython based input presents computer sufficient c library presented supported scv back hardware new methodology simulation part designed csystemc interprocesscommunication software environment knowledge online time sources gui electronic architectures enhances offers sefca graphical proposed process fundamental systemc viewer vhdl universal model send interface working abstraction testing existing uses user used stimuli verification tool enhancements results"
    ]
  },
  {
    "Computer_Graphics_and_Visualization": [
      "Visualization is the process of representing data graphically and interacting with these representations in order to gain insight into the data. Traditionally, computer graphics has provided a powerful mechanism for creating, manipulating, and interacting with these representations.",
	"real approaches represent potential gii opinions web practical problems become rather massive infrastructure technologies information authors offer wide report interacting structured encouraging computer graphics role large tools must play developers capabilities provide cemetery use effective amount however improving discusses burial site data unstructured helping infinitely otherwise central reaching full interaction long address needs especially effectiveness key meet topics universe go captivating way global distributed world chose user visualization tackling challenges",
	"real approaches represent potential gii opinions web practical problems become rather massive infrastructure technologies information authors offer wide report interacting structured encouraging computer graphics role large tools must play developers capabilities provide cemetery use effective amount however improving discusses burial site data unstructured helping infinitely otherwise central reaching full interaction long address needs especially effectiveness key meet topics universe go captivating way global distributed world chose user visualization tackling challenges",
	"results applied motivating visual form details overview findings impact design psychophysics efficiency research paper discovery exploration role discuss recent graphics directly work see” also analysis foundation lowlevel knowledge data human brief novel consideration perception attention show surveys support effectiveness analytics theories memory image relevance fundamental important viewer insights conclude focus direct visualization produce specific challenges goal images",
	"form creates usability emissive input selection rendered computersimulated unobtrusive provides data traditional field sampling due transparency would algorithm devices 3d outputs display approximated technology frame rays volumetric surface displays traditionally suitable technique producing system dimension directly designed ordinary 2d rates familiar intermediate isotropically effectiveness reduction intuitive emulates introduces accumulative volume results called shading potential usage lumivolumes technical illuminate computer number rendering fields 4d novel show set significantly screen key buffer visualizing creating captured users commodity screens visualizations object highresolution light much lumivolume availability demonstrate closer insight takes perceptually offer special paper capability regular broaden issues many graphics fed discuss scalability xraylike along new means different render voxels delivering formalizes including experimental effects class integrals method visualization encodes",
	"productivity evaluated computing evidence improve potential efficient performance highperformance even drawn arithmetics design serious access technologies wide remains research pointer binding flaws demonstrated without increased graphics educators considerably therefore justintime platforms compiler code also developers implementations libraries java visualisation added researchers however pixels adequate still range dynamic primitives benefit general due consideration inconsistencies extensive one offers penalty belief contrary noted limitations though memory writing offered reading class way speed per portability executable hotspot pixel popular compiled individual byte language",
	"form sensing atmospherical explore evaluate even major enormous therefore showing centre seriesdatasets subsets data environmental offers short developed establishment usercommunity interest presentation aerospace contents example suitable sensors today research created video mind valuable operationally based reason given looking task identify envisat animation projects increase already like timeseries forms used specific results provided efficient relatively gaps prohibitive examples generated secondary interpolation films computer primarily time searchengine essential german climatological stable missions purpose problem atmospheric way synthesis possibility appears comfortable finally series exploration tools remote dfd dlr datasets new different provide multidimensional prove amount quantities vast movement generating bearing phenomena isis remotesensing future visualization tool techniques",
	"format internal may engineers described measuring advantage performing instruments vector information thereby operation device valuable measurement computer engineer space also simulation computersimulated methods designed interactive importantly visualisation software fields operator concerning interpret data field microwave enable actual inaccessible circuit giving interpreted electromagnetic enabling place would placed 3d done package normally display presenting experiment readily similar parts virtual results",
	"programs problems starting basins points multiple program presented iterative behavior methods provides order algebra powerful higher construct proposed fourth implemented study mathematical give illustrated systems attraction equations using scientific system recurrent deliveries running given examine solving iterations dynamic computation mathematica properties applied handling expressions examples since computer number included families time features mathematics cpu problem important several complex finding besides objects real rapidly symbolic function manipulating considered two numerical paper without formula many graphics methodology new also plotting convergence computational including generating solved processes disciplines difficult cannot developing zero convergent visualization nonlinear tool average",
	"submitted information includes presented society order quantity 1975–1984 rational institutional extensive list science technology 1995–2004 implemented darmstadt view project complexity theses divided see content road impact three second teaching research scientific four reports based germany beginning available projects periods 1975 hub” well universität hub years results visual computing relatively described description authors end media europe technological computer number large activities knowledge time recently manage gives enabling leading maps first technische reported seen vc “computer development area establish finally 2014 paper graphics references along different also 40 2004–2015 stages cg discipline developing addressed future axes 1985–1994",
	"properties pico capturing nanosecond applications see resolution usually around impact motion information advances media corners distance paper computer graphics presents recent wealth transport simulation analysis detect researchers fields resolutions material integration made capturetime recently leverage field novel including show temporal domain perspective extreme highlyscattering key proposed infer simulating lost idea name reconstructing vision transient capture imaging light movies techniques objects huge",
	"evaluation slidar evaluate systems require reconstruction depth participants raycasting efficiency present device subjective based accuracy better har showed also positioning faster geometry epipolar although environment movement localization mapping simultaneous higher significantly conducted utilizes slam 3d perfect holdar less required cues experiment call devicecentric method existing object user got test virtual results"
    ]
  },
  {
    "Computer_vision": [
      "includes object recognition and 3D scene Interpretation",
      "includes process image for feature extraction.",
      "Computer vision is the science that aims to give a similar, if not better, capability to a machine or computer. Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images.",
	"usually comfortable two hard created wearing voice comfortability ability system paper determine impaired input many headset recognition also provide effective use freely detect researchers provides indoor time camshift navigate path suffers allows bluetooth researches canes requires get navigation algorithm destination problem devices image required worn visually obstacles way adaptive communication able shortest meanshift uses used continuously processing",
	"handguns prohibited contribution last schemes environments present checkpoints among pros learning places remarks sparse safeguard exploring entrances order data field items tested proposed evaluating vision screening classic common baggage systems 2 representations research explosives based automated fair control aid however human still validation tasks gdx placed xray perfect public attempts bag deep used years results approaches security 3 inspection access operators comparisons computer recognition charge brief show paramount protocol concluding ten object cons importance development make paper others many pattern training new different provide database discussing 1 detect words define generally selected experimental far method testing strategies explanation",
	"applications build attributes body intelligent 2020 framework learn personalized making billion opinion understand recommendations classification paper system attribute includes crfs computer learning included aim ecommerce also provide use crf random fields help expected online hit approach worldwide expect furthermore fashion correlations 35 clothing conditional mark decisions would tasks proposed etc need day vision intricacies ethnicity able expert outfit user caffe deep type purchases industry",
	"degrees sensing algorithms explaining even starting past consider information principles mine present taken synergistic recognised methods foundations pave provides comparative interaction relevant developed ranging studies varying vision optical bridge highlight presentation providing think functional three using based taskcentric biology promising aim correspondence task designed analysis centric approach extent utilise tasks well though needed communities models artificial point insights observations originating existing investigators approaches flow seems nontrivial great revisit source computer biologybased main interested primarily help features literature exercise leading always image modelling investigations gap way several organised much researcher development design level two wellknown paper discuss recent purely segmentation new also inspiration computational biological successful classical perspective compare future",
	"explore problems iii dynamics iv ucf50 motion obtained five complementarity multiple frameworks action scenarios simple methods ucf101 fusing exploring data codebook general encoding appearance construct patterns choice ii vision global 923 study usually represent performance counteract challenging three research using video improvement system based descriptors feature made independently still unknown extracted dense bag pooling stateoftheart local generation results called composed representation visual views crucial kinds realistic meanwhile impressive computer trajectories recognition eight effective hmdb51 comprehensive mainly time features contributing pipeline improved improper exhibits static specifically yet important model final conclude several ten produce extraction hybrid 619 step aims may steps two boosting obtain paper bovw 879 many efforts datasets practices different provide although propose words preprocessing supervector furthermore uncover long v normalization one effects fusion every good strategies popular rate",
	"step approaches applications common evaluation accurate algorithms relatively date depth compares research comparisons video inherently produces subjective many computer limits assess number segmentation manuallysegmented different segmentations reference segmented human preprocessed still whether generally set whole predetermined particular extensive effectiveness one separate supervised algorithm tedious done difficult creating image evaluating process alternative important class visually small vision method another processing compared results images",
	"sensing evaluate algorithms framework quality low past consistently motion information classification obtained environments incorrectly behavior minimal reliability order researches general joint due full activity would correctly office 3d apart proposed camera life process technology performed user joints common applications possible body systems depth sensors research using easy system based postures automated enhance running work detected context risk analysis identify classifier deployment human kinect made projects apply nature outperforms wellbeing individuals existing used differences results applied disorders potential positions understand accuracy fused result number inaccurate large accurately workplace offices workspace factories improving user’s classify support widely errors reduce problem cost facilitate tracked subtle injury complex maxmargin classifying setup false demonstrate smart may high area musculoskeletal obtain appropriate thereby without areas paper shown different provide classifies amount propose movement central ergonomics home previous experimental difficult posture bottleneck healthiness monitoring robustness nowadays method popular integrate alarms",
	"timescales improve suddenly problems algorithms even last multiple learning review better importantly conventional often variety google data field activity current get witnessed devices advanced vision thanks machine started cnns performance longstanding wide research processed old graphicsprocessingunit smarter applicability corresponding barriers significant inspired artificial larger generation deep years gpubased managed article assistants exponential become explain services computer sufficient large neural methodologies enables activities consumer tied part hello fields significantly improved intelligence achievements leading known improvements adopting reasonable triggered break solve availability seen build smart state across development broader combination others tools alexa hardware training new arrival roots growth convolutional networks big techniques sets",
	"signal intelligent problems low roads includes optimized methods autoadjust jams conventional cities get balancing management simulated lost vision focuses condition manhours changing road waiting depending research system based automated running control designed solving environment dynamically predetermined extent significant green assigns according implementation introduces makes load managed congestion jammed great complete fully fixedtime computer number lowcost traffic help vehicles time replace vehicle advantages support reduce image cost able light processing design takes two paper without hardware along new also amount detect busy conditions one factors constructed red techniques saving detects",
	"ur2d diverse framework practical onetoone personalized hypothesis evaluations enrollment presented decreases bidirectional data projection 3d databases probe global uhdb11 onetomany variations frgc performance identification challenging shape using system based alignment space metric 2d frontal registration pose v20 pairwise models orientationbased outperforms local arbitrary used results images applied represented textures generated large recognition normalized comprises face probes allows similarity support landmarkbased scoring image yet model gap gallery light verification correlation demonstrate accurate 2d2d deformable relighting paper 3d2d subjectspecific signatures datasets different database generalize propose texture conditions illumination normalization estimation facial method constructed 3d3d nonlinear",
	"approaches applications overview body mmhci major research paper emerging issues computer discuss recognition review gesture modeling task open emotion field affective expression interaction giving particular perspective audio fusion multimodal facial vision focus user highlighting human–computer gaze challenges"
    ]
  },
  {
    "Data_mining_and_modeling": [
      "Data cleansing, data cleaning, or data scrubbing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.",
      "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.  Data preprocessing is used database-driven applications such as customer relationship management and rule-based applications (like neural networks).",
      "The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process. ... For general information about ML models and ML algorithms, see Machine Learning Concepts.",
      "the practice of examining large pre-existing databases in order to generate new information.",
	"mining problems algorithms practical quality evaluations last learning better showing size provides fraction data requires general approximation items fast updatetime algorithm considers linear optimization maintain study focus tunable maintains give diversification applications machine want performance storing bounds negligible efficacy using theoretical nicefrac12 provable space optimum work insertiononly context w coverage constraints exceeds memory numerous solutions solution uses preserve nontrivial maximizing lowmemory submodular element number constant time summarization allows show particular specifically problem asked cost known millions first model world best arrive streams update spread processing values real demonstrate function high desired design polylogarithmic k nicefrac13 question guarantees lies datasets cardinality formulation different matches provide functions window core including elements one epsilon tradeoff far less sliding good 0 sublinear average",
	"interpretation generalizing structure require learning focused minimal rulebased provides often data multiword thus tail interest mostly compact user 90 providing discover heavy research demonstrated search experiments precision names approach beginning significant collections sheer increase tasks ontologies recognize entities ones introduces countries unsupervised unseen entityattribute us attributes potential efficient organizing 40fold impose invested queries precise number large describes enables grammar semantic 47 parallel recently leverage compositional thousands show set effort sc problem observed generate users scalable way engines per power several rich automatically offer structured paper expressing attribute frequent efforts 100000 new expand compaction curation researchers furthermore long answering rules class method",
	"approaches common limited lowrank mining systems text two prediction paper matrices accuracy approximate computer sum analyze modeling experiments propose approximations parallel show approximation classical recommendation one tasks observed matrix proposed weighted assumption prevalent region vision constructing experiment global procedures local improvements partially tool",
	"3 m3a accurate web 06 framework discover probably parameters level camellog abnormal three normal queries explain describe quantitative five interarrival behaviors paper present strong points submitting search row−is among pattern 30 notice grouplevel behavior twodimensional modeling 1 detect query studied metaclick users’ alice’s userand time propose 2 interpretations million available novel clickthrough combining correlations group anomaly one following iat heavytail alice specifically thus ‘alice’ proposed publicly users detection surprising metamodel model hours per largest distribution capture log landed minutes user marginal distinct bimodal containing",
	"approaches applications content overview particularly around web aligning systems comes schema information using finish structured freshness vs freebasewikidata interesting subjective question discuss answer people strengths open relative thought manner knowledge sources additional unstructured including data combined openclosed lot trust ideas objective supervised talk well though captured databases less bases first similarly way alignments constructed wikipedia differences give techniques extraction relationship",
	"qualitative structure evaluate poor quality links consider mean geographic present points places graph provides google ranking ensure link higher cohesive tail detection study categories raw applications evaluation performance discover perform recommendations system erroneous designed reveal analysis scale precision approach human 42 discussion recommendation relatedness entities models capture results checking” queries remove secondary structural geospatial large live semantic knowledge features “double detailing hundreds users anomalous millions predict model day able relationship function two 29 related different provide detect additional characterize find semantically long suggestions unpopular system’s facets anomalies realworld structurally judges consistent method output",
	"cases complexity may problems timely require understanding capabAle analyzing become describe hri supervisor appropriate increasingly ability intervene video look multiple eg role tools awareness number guidance handle constrain single might activities modeling strategy control provide use basic semiautonomous scale operator autonomous supervise approach questions human data strategic acquire reasonably rts supervision extent fashion operations support initial person one supervised tasks supervisory answering games agents multirobot humans sa maintain robots realtime nature similar efforts situational goal",
	"applied applications mechanism mining provided algorithms understanding schema evolve suitable last decades underlying scientific since tremendous question shown supported main provide database experiments analysis arises comprehensive environment knowledge order coming data adequate extent hand mapping execution accommodate increase various one aimed production reality computerbound tasks less realworld models model priori modelled able results",
	"followed mentioned expressed problems require framework mitigating issue ‘cascade’ information mechanisms classification integrated characteristics present multiple learning operating overhead numbers cascade methods mrrmcfsvm types data quantity respect overcome depend various proposed smaller study yielding integrating challenges regularized machine filtration respective perspectives scope matrices based monotonic efficiently outcomes work referred solving corresponding however approach kernel available constraints significant domain needs parallelized helps greater memory processors introduces called potential described accuracy comparison implies knowledge time prior thousands vectors partial reduce requirements hundreds problem known later model way several much fuzzy seen considered rmcfsvm scaled phase regular issues related svm addressing recent scalability datasets training along new different extend previously overheads reduced mode explicitly especially experimental case accomplished less realworld count applying communication monotonicity big sets",
	"approaches exacting representation machine wordbased may representing mining document represent text area representations term information lda represented termbased pbtm paper based multiple many dirichlet pattern learning single better allocation modeling terms traditional recently topic semantically previous documents needed proposed generate always utilizing utilized enhanced patterns models topics model interest latent able user used years techniques collection word",
	"gravity form investigation mining provided systems considered subscriber mobile belonging novosibirsk hypothesis using obtained estimating matrices based issues tree comparison transport calculation decision iterative extracting methods flows analysis describing functions terms operator knowledge data elements hidden stratification conducted utility factors algorithm matrix city carried estimation modelling patterns stratum model checked constructing regression preference origindestination method type proxy specific population od",
	"amortized nodes algorithms opolylognr links removed complemented present input showing graph size aspect added twitter data range current requires extensive algorithm hundred proposed detection maintain naive study community either subgraphs evaluation microseconds wide theoretical provable densest efficiently work social analysis maximum satisfactory dynamic computation still operations tasks incessantly dense point solution n maintained efficient emerged event media generated number large handle random requiring time missing show executed problem cost scalable important first model omegam per able calling finding update graphs probability high recomputes uniformly total guarantees approximate addressing new contrast within subgraph adversarially experimental primitive 21εapproximation changes friendship realworld highly r viable edges every tweets facebook"
    ]
  },
  {
    "Embedded_systems": [
      "Automation is becoming pervasive in and permeating throughout our society in ways that are often invisible.  An ever-growing number of personal and domestic appliances have embedded behavior that provides convenience and utility in our everyday lives.  From modern automotive vehicles (that contain numerous embedded subsystems--often taken for granted), to automated factories to medical devices to emergency response systems to almost any kind of sophisticated electronic device, embedded systems and software are present.\nThis research domain is a realm that embraces the \"real world\" where the concepts of bug and crash must give way to graceful degradation and fault-tolerance.  At the same time it must accommodate many real-world limitations, some of which are power consumption, response time, physical size, heat dissipation, and local memory capacity (future linkages to cloud environments should not be discounted) while being driven by reliability factors, cost factors, and time to market.   The unbridled growth of embedded technologies and applications is evident. \nCurrent system and software development technologies often do not consider specific requirements and demands of embedded capabilities which are fundamentally different from that which are non-embedded.  It is a domain that requires system, software, and hardware synergies and which is ripe for the research and design focus of Systems Engineering and Software Engineering to improve our lives and society.",
      "The RAM family includes two important memory devices: static RAM (SRAM) and dynamic RAM (DRAM). The primary difference between them is the lifetime of the data they store. SRAM retains its contents as long as electrical power is applied to the chip. If the power is turned off or lost temporarily, its contents will be lost forever. DRAM, on the other hand, has an extremely short data lifetime-typically about four milliseconds. This is true even when power is applied constantly. \n In short, SRAM has all the properties of the memory you think of when you hear the word RAM. Compared to that, DRAM seems kind of useless. By itself, it is. However, a simple piece of hardware called a DRAM controller can be used to make DRAM behave more like SRAM. The job of the DRAM controller is to periodically refresh the data stored in the DRAM. By refreshing the data before it expires, the contents of memory can be kept alive for as long as they are needed. So DRAM is as useful as SRAM after all. \n When deciding which type of RAM to use, a system designer must consider access time and cost. SRAM devices offer extremely fast access times (approximately four times faster than DRAM) but are much more expensive to produce. Generally, SRAM is used only where access speed is extremely important. A lower cost-per-byte makes DRAM attractive whenever large amounts of RAM are required. Many embedded systems include both types: a small block of SRAM (a few kilobytes) along a critical data path and a much larger block of DRAM for everything else. \n Memories in the ROM family are distinguished by the methods used to write new data to them (usually called programming), and the number of times they can be rewritten. This classification reflects the evolution of ROM devices from hardwired to programmable to erasable-and-programmable. A common feature of all these devices is their ability to retain data and programs forever, even during a power failure. \n An EPROM (erasable-and-programmable ROM) is programmed in exactly the same manner as a PROM. However, EPROMs can be erased and reprogrammed repeatedly. To erase an EPROM, you simply expose the device to a strong source of ultraviolet light. (A window in the top of the device allows the light to reach the silicon.) By doing this, you essentially reset the entire chip to its initial-unprogrammed-state. Though more expensive than PROMs, their ability to be reprogrammed makes EPROMs an essential part of the software development and testing process. \n As memory technology has matured in recent years, the line between RAM and ROM has blurred. Now, several types of memory combine features of both. These devices do not belong to either group and can be collectively referred to as hybrid memory devices. Hybrid memories can be read and written as desired, like RAM, but maintain their contents without electrical power, just like ROM. Two of the hybrid devices, EEPROM and flash, are descendants of ROM devices. These are typically used to store code. The third hybrid, NVRAM, is a modified version of SRAM. NVRAM usually holds persistent data. \n EEPROMs are electrically-erasable-and-programmable. Internally, they are similar to EPROMs, but the erase operation is accomplished electrically, rather than by exposure to ultraviolet light. Any byte within an EEPROM may be erased and rewritten. Once written, the new data will remain in the device forever-or at least until it is electrically erased. The primary tradeoff for this improved functionality is higher cost, though write cycles are also significantly longer than writes to a RAM. So you wouldn't want to use an EEPROM for your main system memory.",
      "An embedded system usually performs a specialized operation and does the same repeatedly. For example: A pager always functions as a pager.",
      "In this Software is used for more features and flexibility. Hardware is used for performance and security.\n Embedded controllers for reactive real-time applications are implemented as mixed software-hardware systems. These controllers utilize Micro-processors, Micro-controllers and Digital Signal Processors but are neither used nor perceived as computers. Generally, software is used for features and flexibility, while hardware is used for performance. Some examples of applications of embedded controllers are: \n Consumer Electronics: microwave ovens, cameras, compact disk players. \n Telecommunications: telephone switches, cellular phones. \n Automotive: engine controllers, anti-lock brake controllers. \n Plant Control: robots, plant monitors.\n\n Current methods for designing embedded systems require to specify and design hardware and software separately. A specification, often incomplete and written in non-formal languages, is developed and sent to the hardware and software engineers. Hardware-software partition is decided a priori and is adhered to as much as is possible, because any changes in this partition may necessitate extensive redesign. Designers often strive to make everything fit in software, and off-load only some parts of the design to hardware to meet timing constraints. The problems with these design methods are:",
      "Many embedded systems must continually react to changes in the system's environment and must compute certain results in real time without any delay. Consider an example of a car cruise controller; it continually monitors and reacts to speed and brake sensors. It must compute acceleration or de-accelerations repeatedly within a limited time; a delayed computation can result in failure to control of the car.",
      "All computing systems have constraints on design metrics, but those on an embedded system can be especially tight. Design metrics is a measure of an implementation's features such as its cost, size, power, and performance. It must be of a size to fit on a single chip, must perform fast enough to process data in real time and consume minimum power to extend battery life.",
      "An embedded system is a computer system with a dedicated function within a larger mechanical or electrical system, often with real-time computing constraints. It is embedded as part of a complete device often including hardware and mechanical parts. Embedded systems control many devices in common use today.\nAn embedded system is some combination of computer hardware and software, either fixed in capability or programmable, that is designed for a specific function or for specific functions within a larger system. Industrial machines, agricultural and process industry devices, automobiles, medical equipment, cameras, household appliances, airplanes, vending machines and toys as well as mobile devices are all possible locations for an embedded system.\nEmbedded systems can be microprocessor or microcontroller based. In either case, there is an integrated circuit (IC) at the heart of the product that is generally designed to carry out computation for real-time operations. Microprocessors are visually indistinguishable from microcontrollers, but whereas the microprocessor only implements a central processing unit (CPU) and thus requires the addition of other components such as memory chips, microcontrollers are designed as self-contained systems.",
      "Microprocessors, MPUs are ideal for use in embedded systems, but their structure makes them particularly applicable to certain types of embedded systems.\nThe basic MPU contains the central processing unit and possibly a few additional items but the memory and also the Input Output interface is external. Typically the program is stored in non-volatile memory, such as NAND or serial Flash, and at start-up is loaded into an external DRAM and then commences execution."
    ]
  },
  {
    "Multimedia_Computing": [
      "A (digital) color image is a digital image that includes color information for each pixel.\n\nFor visually acceptable results, it is necessary (and almost sufficient) to provide three samples (color channels) for each pixel, which are interpreted as coordinates in some color space. The RGB color space is commonly used in computer displays, but other spaces such as YCbCr, HSV, and are often used in other contexts. A color image has three values (or channels) per pixel and they measure the intensity and chrominance of light. The actual information stored in the digital image data is the brightness information in each spectral band.\n\nA color image is usually stored in memory as a raster map, a two-dimensional array of small integer triplets; or (rarely) as three separate raster maps, one for each channel.\nSeparate R, G, and B image layers\n\nEight bits per sample (24 bits per pixel) seem adequate for most uses, but faint banding artifacts may still be visible in some smoothly varying images, especially those subject to processing. Particularly demanding applications may use 10 bits per sample or more.\n\nOn the other hand, some widely used image file formats and graphics cards may use only 8 bits per pixel, i.e., only 256 different colors, or 2–3 bits per channel. Converting continuous-tone images like photographs to such formats requires dithering and yields rather grainy and fuzzy results.\n\nGraphics cards that support 16 bits per pixel provide 65536 distinct colors, or 5–6 bits per channel. This resolution seems satisfactory for non-professional uses, even without dithering.",
      "text is created on a computer, so it doesn't really extend a computer system the way audio and video do. But, understanding how text is stored will set the scene for understanding how multimedia is stored. \n    A single way to progress through the text, starting at the beginning and reading to the end.\n Information is represented in a semantic network in which multiple related sections of the next are connected to each other.A user may then browser trough the section of the next, jumping from one text section to another.  A ‘font’ is a collection of characters of a particular size and style belonging to a particular typeface family. A ‘typeface’ is a family of graphic characters that usually includes many type sizes and styles.\n\nCase : UPPER and lower cased letter\nBold, Italic, Underline, Superscript and Subscript\nEmbossed or Shadow\n\nMultimedia application and presentations invariably rely to some extent on the use of text to convey their message to users.\nText has many characteristics that the developer can modify to enhance the user expression.",
      "temporal redundancyPixels in two video frames that have the same values in the same location. Exploiting temporal redundancy is one of the primary techniques in video compression",
      "A pixmap stores and displays a graphical image as a rectangular array of pixel color values. (The term \"pixmap\" is short for \"pixel map\".) A pixmap that uses only a single bit to denote the color of each pixel (resulting in a monochrome image) is often referred to as a bitmap. Bitmap is also sometimes used to refer to any pixmap. \n While the term \"pixmap\" is often used in a general way to describe the actual array of pixels or the image that they create, Common Graphics now uses the term to describe a class of objects that are used to manage pixmap images. \nSpecifies the number of bits that are used to denote each pixel value. Must be either 1, 4, 8, 16, or 24. Must be large enough for the largest pixel value used by the pixmap. \nA vector of RGB color structures that determines the color that will be used for each pixel value. Pixels with the value 0 will use the first color in the color vector, and in general pixels with the value N will use the (N+1)th member of the color vector. The colors vector should contain at least (one more than) as many members as the largest pixel value in the contents list. \n The handle to an optional more efficient version of the pixmap which is managed inside the operating system. This property should not be set directly by an application, but instead is set indirectly by calling open-pixmap-handle on the pixmap to create the handle. The handle can be destroyed later by calling close-pixmap-handle on the pixmap.",
      "Audio is sound within the acoustic range available to humans. An audio frequency (AF) is an electrical alternating current within the 20 to 20,000 hertz (cycles per second) range that can be used to produce acoustic sound. In computers, audio is the sound system that comes with or can be added to a computer. An audio card contains a special built-in processor and memory for processing audio files and sending them to speakers in the computer. An audio file is a record of captured sound that can be played back. Sound is a sequence of naturally analog signals that are converted to digital signals by the audio card, using a microchip called an analog-to-digital converter (ADC). When sound is played, the digital signals are sent to the speakers where they are converted back to analog signals that generate varied sound.\nAudio files are usually compressed for storage or faster transmission. Audio files can be sent in short stand-alone segments - for example, as files in the Wave file format. In order for users to receive sound in real-time for a multimedia effect, listening to music, or in order to take part in an audio or video conference, sound must be delivered as streaming sound. More advanced audio cards support wavetable, or precaptured tables of sound. The most popular audio file format today is MP3 (MPEG-1 Audio Layer-3).",
      "Multimedia is the field concerned with the computer-controlled integration of text, graphics, drawings, still and moving images (Video), animation, audio, and any other media where every type of information can be represented, stored, transmitted and processed digitally.",
      "Reliability is one of the main parameters of the products considered to be important to survive in the competitive market. Reliability in its simple form means the probability that a failure may not occur in a given period of time that is, the component performs adequately without failure. The subsystems or components in a system are arranged in series or parallel depending on the space constraint. However, the reliability of the system with components arranged in parallel is more than that system for which the components are arranged in series. Redundancy is the method of arranging the components in a subsystem in parallel such that if one component fails then the other component automatically comes into operation. Through redundancy, any desired level of reliability can be obtained, but in doing so, we have to invest money or other material resources to achieve the desired reliability. A designer has to consider the economic views of the organization in designing a system with high reliability. In this paper, a complex system (series-parallel system) is considered with stochastic reliability for its components. For a particular configuration, reliabilities of the components are generated and evaluated by the system reliability, after which simulation is run and repeated for various runs, before finally consolidating the configuration reliability and resource utilization. The simulation is carried for various feasible configurations and each configuration evaluated. The configuration with best reliability within resource restrictions is selected. \n\n.",
      "Visual multimedia source that combines a sequence of images to form a moving picture. The video transmits a signal to a screen and processes the order in which the screen captures should be shown. Videos usually have audio components that correspond with the pictures being shown on the screen.\nVideo is an electronic medium for the recording, copying, playback, broadcasting, and display of moving visual media.\n\nVideo was first developed for mechanical television systems, which were quickly replaced by cathode ray tube (CRT) systems which were later replaced by flat panel displays of several types.\n\nVideo systems vary in display resolution, aspect ratio, refresh rate, color capabilities and other qualities. Analog and video variants exist and video can be carried on a variety of media, including radio broadcast, tapes, DVDs, computer files and network streaming.\n\nFrame rate, the number of still pictures per unit of time of video, ranges from six or eight frames per second (frame/s) for old mechanical cameras to 120 or more frames per second for new professional cameras. PAL standards (Europe, Asia, Australia, etc.) and SECAM (France, Russia, parts of Africa etc.) specify 25 frame/s, while NTSC standards (USA, Canada, Japan, etc.) specify 29.97 frames.[citation needed] Film is shot at the slower frame rate of 24 frames per second, which slightly complicates the process of transferring a cinematic motion picture to video. The minimum frame rate to achieve a comfortable illusion of a moving image is about sixteen frames per second.\n\nVideo can be interlaced or progressive. Interlacing was invented as a way to reduce flicker in early mechanical and CRT video displays without increasing the number of complete frames per second, which would have sacrificed image detail to remain within the limitations of a narrow bandwidth. The horizontal scan lines of each complete frame are treated as if numbered consecutively, and captured as two fields: an odd field (upper field) consisting of the odd-numbered lines and an even field (lower field) consisting of the even-numbered lines.\n\nAnalog display devices reproduce each frame in the same way, effectively doubling the frame rate as far as perceptible overall flicker is concerned. When the image capture device acquires the fields one at a time, rather than dividing up a complete frame after it is captured, the frame rate for motion is effectively doubled as well, resulting in smoother, more lifelike reproduction (although with halved detail) of rapidly moving parts of the image when viewed on an interlaced CRT display, but the display of such a signal on a progressive scan device is problematic.\n\nNTSC, PAL and SECAM are interlaced formats. Abbreviated video resolution specifications often include an i to indicate interlacing. For example, PAL video format is often specified as 576i50, where 576 indicates the total number of horizontal scan lines, i indicates interlacing, and 50 indicates 50 fields (half-frames) per second.\n\nIn progressive scan systems, each refresh period updates all scan lines in each frame in sequence. When displaying a natively progressive broadcast or recorded signal, the result is optimum spatial resolution of both the stationary and moving parts of the image. When displaying a natively interlaced signal, however, overall spatial resolution is degraded by simple line doubling—artifacts such as flickering or \"comb\" effects in moving parts of the image appear unless special signal processing eliminates them. A procedure known as deinterlacing can optimize the display of an interlaced video signal from an analog, DVD or satellite source on a progressive scan device such as an LCD Television, digital video projector or plasma panel. Deinterlacing cannot, however, produce video quality that is equivalent to true progressive scan source material.\nAspect ratio describes the dimensions of video screens and video picture elements. All popular video formats are rectilinear, and so can be described by a ratio between width and height. The screen aspect ratio of a traditional television screen is 4:3, or about 1.33:1. High definition televisions use an aspect ratio of 16:9, or about 1.78:1. The aspect ratio of a full 35 mm film frame with soundtrack (also known as the Academy ratio) is 1.375:1.\n\nPixels on computer monitors are usually square, but pixels used in digital video often have non-square aspect ratios, such as those used in the PAL and NTSC variants of the CCIR 601 digital video standard, and the corresponding anamorphic widescreen formats. Therefore, a 720 by 480 pixel NTSC DV image displayes with the 4:3 aspect ratio (the traditional television standard) if the pixels are thin, and displays at the 16:9 aspect ratio (the anamorphic widescreen format) if the pixels are fat.\n\nThe popularity of viewing video on mobile phones has led to the growth of vertical video. Mary Meeker, a partner at Silicon Valley venture capital firm Kleiner Perkins Caufield & Byers, highlighted the growth of vertical video viewing in her 2015 Internet Trends Report – growing from 5% of video viewing in 2010 to 29% in 2015. Vertical video ads like Snapchat’s are watched in their entirety 9X more than landscape video ads.[5] The format was rapidly taken up by leading social platforms and media publishers such as Mashable[6] In October 2015 video platform Grabyo launched technology to help video publishers adapt horizotonal 16:9 video into mobile formats such as vertical and square.\nColor model name describes the video color representation. YIQ was used in NTSC television. It corresponds closely to the YUV scheme used in NTSC and PAL television and the YDbDr scheme used by SECAM television.\n\nThe number of distinct colors a pixel can represent depends on the number of bits per pixel (bpp). A common way to reduce the amount of data required in digital video is by chroma subsampling (e.g., 4:4:4, 4:2:2, 4:2:0/4:1:1). Because the human eye is less sensitive to details in color than brightness, the luminance data for all pixels is maintained, while the chrominance data is averaged for a number of pixels in a block and that same value is used for all of them. For example, this results in a 50% reduction in chrominance data using 2 pixel blocks (4:2:2) or 75% using 4 pixel blocks(4:2:0). This process does not reduce the number of possible color values that can be displayed, it reduces the number of distinct points at which the color changes.\nUncompressed video delivers maximum quality, but with a very high data rate. A variety of methods are used to compress video streams, with the most effective ones using a Group Of Pictures (GOP) to reduce spatial and temporal redundancy. Broadly speaking, spatial redundancy is reduced by registering differences between parts of a single frame; this task is known as intraframe compression and is closely related to image compression. Likewise, temporal redundancy can be reduced by registering differences between frames; this task is known as interframe compression, including motion compensation and other techniques. The most common modern standards are MPEG-2, used for DVD, Blu-ray and satellite television, and MPEG-4, used for AVCHD, Mobile phones (3GP) and Internet.",
      "Nature, artworks, and man-made environments are full of redundant visual information, where an object appears in the context of other identical or similar objects. How is visual perception affected by whether the surrounding items are identical to it, different from it, or absent? This study addresses the role of visual redundancy in the perception of human faces and reveals opposite effects on identity perception and emotion perception. Participants in Experiment 1 identified the gender of a single face presented at fixation. This “target” face was preceded by three types of masked prime displays: a single face at a randomly selected visual quadrant, four identical faces, one in each quadrant, or four different faces, one in each quadrant. All faces had neutral expression. Priming was indexed by faster gender discrimination of the “target” face when it was identical to one of the prime faces, than when it was a different gender. Experiment 1 found that gender priming was greater when the prime display contained four identical faces than when it contained a single face or four different faces, suggesting that face identification was enhanced by redundant visual input. In Experiment 2, participants viewed prime displays containing a single face, four identical faces, or four different faces, but these faces were either neutral or fearful in facial expression. Participants identified the facial expression of a “target” face, whose expression was either consistent or inconsistent with that of the prime display. Facial expression priming was significantly greater when the prime display contained a single face than when it contained four identical faces or four different faces. In fact, priming in the emotion task was eliminated when the prime display contained four identical faces. These results show that visual redundancy facilitates the perception of face identities, but impairs the perception of facial emotions.",
      "The pixel (a word invented from \"picture element\") is the basic unit of programmable color on a computer display or in a computer image. Think of it as a logical - rather than a physical - unit. The physical size of a pixel depends on how you've set the resolution for the display screen. If you've set the display to its maximum resolution, the physical size of a pixel will equal the physical size of the dot pitch (let's just call it the dot size) of the display. If, however, you've set the resolution to something less than the maximum resolution, a pixel will be larger than the physical size of the screen's dot (that is, a pixel will use more than one dot).\nThe specific color that a pixel describes is some blend of three components of the color spectrum - RGB. Up to three bytes of data are allocated for specifying a pixel's color, one byte for each major color component. A true color or 24-bit color system uses all three bytes. However, many color display systems use only one byte (limiting the display to 256 different colors).\nA bitmap is a file that indicates a color for each pixel along the horizontal axis or row (called the x coordinate) and a color for each pixel along the vertical axis (called the y coordinate). A Graphics Interchange Format file, for example, contains a bitmap of an image (along with other data).\nScreen image sharpness is sometimes expressed as dpi (dots per inch). (In this usage, the term dot means pixel, not dot as in dot pitch.) Dots per inch is determined by both the physical screen size and the resolution setting. A given image will have lower resolution - fewer dots per inch - on a larger screen as the same data is spread out over a larger physical area. On the same size screen, the image will have lower resolution if the resolution setting is made lower - resetting from 800 by 600 pixels per horizontal and vertical line to 640 by 480 means fewer dots per inch on the screen and an image that is less sharp. (On the other hand, individual image elements such as text will be larger in size.)",
      "A simulation of movement created by displaying a series of pictures, or frames. Cartoons on television is one example of animation. Animation on computers is one of the chief ingredients of multimedia presentations. There are many software applications that enable you to create animations that you can display on a computer monitor.\n\nOne of the most exciting forms of pictoral presentation is animation. Animation refers to a simulated motion picture depictingmovementof drawn (or simulated) objects. The main features of this definition are as follows: (1) picture - an animation is a kind of pictorial representation; (2) motion - an animation depicts apparent movement; and (3) simulated - an animation consists of objects that are artificially created through drawing or some other simulation method.\nMultimedia principle: Deeper learning from animation and narration than from narration alone.\nSpatial contiguity principle: Deeper learning when corresponding text and animation are presented near rather than far from each other on the screen\nTemporal contiguity principle: Deeper learning when corresponding narration and animation are presented simultaneously rather than successively\nCoherence principle: Deeper learning when extraneous narration, sounds, and video are excluded rather than included\nModality principle: Deeper learning from animation and narration than from animation and on-screen text.\n Redundancy principle: Deeper learning from animation and narration than from animation, narration, and on-screen text.\n Personalization principle: Deeper learning when narration or on-screen text is conversational rather than formal.",
      "spatial redundancy, Elements that are duplicated within a structure, such as pixels in a still image and bit patterns in a file. Exploiting spatial redundancy is how compression is performed.",
      "A binary image is a digital image that has only two possible values for each pixel. Typically, the two colors used for a binary image are black and white, though any two colors can be used. The color used for the object(s) in the image is the foreground color while the rest of the image is the background color.[1] In the document-scanning industry, this is often referred to as \"bi-tonal\".\n\nBinary images are also called bi-level or two-level. This means that each pixel is stored as a single bit—i.e., a 0 or 1. The names black-and-white, B&W, monochrome or monochromatic are often used for this concept, but may also designate any images that have only one sample per pixel, such as grayscale images. In Photoshop parlance, a binary image is the same as an image in \"Bitmap\" mode.[2]\n\nBinary images often arise in digital image processing as masks or as the result of certain operations such as segmentation, thresholding, and dithering. Some input/output devices, such as laser printers, fax machines, and bilevel computer displays, can only handle bilevel images.\n\nA binary image can be stored in memory as a bitmap, a packed array of bits. A 640×480 image requires 37.5 KiB of storage. Because of the small size of the image files, fax machine and document management solutions usually use this format. Most binary images also compress well with simple run-length compression schemes.\n\nBinary images can be interpreted as subsets of the two-dimensional integer lattice Z2; the field of morphological image processing was largely inspired by this view.",
      "Graphics are visual images or designs on some surface, such as a wall, canvas, screen, paper, or stone to inform, illustrate, or entertain. In contemporary usage it includes: pictorial representation of data, as in computer-aided design and manufacture, in typesetting and the graphic arts, and in educational and recreational software. Images that are generated by a computer are called computer graphics.\nExamples are photographs, drawings, Line art, graphs, diagrams, typography, numbers, symbols, geometric designs, maps, engineering drawings, or other images. Graphics often combine text, illustration, and color. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flyer, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style.\nDrawing generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. In which a tool is always used as if there were no tools it would be art. Graphical drawing is an instrumental guided drawing.\nWoodblock printing, including images is first seen in China after paper was invented (about A.D. 105). In the West the main techniques have been woodcut, engraving and etching, but there are many others.\nEtching is an intaglio method of printmaking in which the image is incised into the surface of a metal plate using an acid. The acid eats the metal, leaving behind roughened areas, or, if the surface exposed to the acid is very thin, burning a line into the plate. The use of the process in printmaking is believed to have been invented by Daniel Hopfer (c. 1470–1536) of Augsburg, Germany, who decorated armour in this way.\n\nEtching is also used in the manufacturing of printed circuit boards and semiconductor devices.\nLine art is a rather non-specific term sometimes used for any image that consists of distinct straight and curved lines placed against a (usually plain) background, without gradations in shade (darkness) or hue (color) to represent two-dimensional or three-dimensional objects. Line art is usually monochromatic, although lines may be of different colors.",
      "Image is a two-dimensional representation of a scene, as seen by people."
    ]
  },
  {
    "Distributed_Systems_and_Parallel_computing": [
      "Uniform memory access (UMA) is a shared memory architecture used in parallel computers. All the processors in the UMA model share the physical memory uniformly. In a UMA architecture, access time to a memory location is independent of which processor makes the request or which memory chip contains the transferred data. Uniform memory access computer architectures are often contrasted with non-uniform memory access (NUMA) architectures. In the UMA architecture, each processor may use a private cache. Peripherals are also shared in some fashion. The UMA model is suitable for general purpose and time sharing applications by multiple users. It can be used to speed up the execution of a single large program in time-critical applications.\nUMA using bus-based symmetric multiprocessing (SMP) architectures;\nUMA using crossbar switches.\nUMA using multistage interconnection networks.",
      "The client-server model is appropriate for service-oriented situations. However, there are other computational goals for which a more equal division of labor is a better choice. The term peer-to-peer is used to describe distributed systems in which labor is divided among all the components of the system. All the computers send and receive data, and they all contribute some processing power and memory. As a distributed system increases in size, its capacity of computational resources increases. In a peer-to-peer system, all components of the system contribute some processing power and memory to a distributed computation.",
      "NUMA (non-uniform memory access) is a method of configuring a cluster of microprocessor in a multiprocessing system so that they can share memory locally, improving performance and the ability of the system to be expanded. NUMA is used in a symmetric multiprocessing ( SMP ) system. An SMP system is a \"tightly-coupled,\" \"share everything\" system in which multiple processors working under a single operating system access each other's memory over a common bus or \"interconnect\" path. Ordinarily, a limitation of SMP is that as microprocessors are added, the shared bus or data path get overloaded and becomes a performance bottleneck. NUMA adds an intermediate level of memory shared among a few microprocessors so that all data accesses don't have to travel on the main bus.\n\nNUMA can be thought of as a \"cluster in a box.\" The cluster typically consists of four microprocessors (for example, four Pentium microprocessors) interconnected on a local bus (for example, a Peripheral Component Interconnect bus) to a shared memory (called an \"L3 cache \") on a single motherboard (it could also probably be referred to as a card ). This unit can be added to similar units to form a symmetric multiprocessing system in which a common SMP bus interconnects all of the clusters. Such a system typically contains from 16 to 256 microprocessors. To an application program running in an SMP system, all the individual processor memories look like a single memory.",
      "Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time.\n\n\nParallel processing is generally implemented in operational environments/scenarios that require massive computation or processing power. The primary objective of parallel computing is to increase the available computation power for faster application processing or task resolution. Typically, parallel computing infrastructure is housed within a single facility where many processors are installed in a server rack or separate servers are connected together. The application server sends a computation or processing request that is distributed in small chunks or components, which are concurrently executed on each processor/server. Parallel computation can be classified as bit-level, instructional level, data and task parallelism.",
      "The two architectures we have just considered -- peer-to-peer and client-server -- are designed to enforce modularity. Modularity is the idea that the components of a system should be black boxes with respect to each other. It should not matter how a component implements its behavior, as long as it upholds an interface: a specification for what outputs will result from inputs.",
      "Parallel processing is also associated with data locality and data communication. Parallel Computer Architecture is the method of organizing all the resources to maximize the performance and the programmability within the limits given by technology and the cost at any instance of time.",
      "In computer science, distributed memory refers to a multiprocessor computer system in which each processor has its own private memory. Computational tasks can only operate on local data, and if remote data is required, the computational task must communicate with one or more remote processors.",
      "In distributed systems, components communicate with each other using message passing. A message has three essential parts: the sender, the recipient, and the content. The sender needs to be specified so that the recipient knows which component sent the message, and where to send replies. The recipient needs to be specified so that any computers who are helping send the message know where to direct it. The content of the message is the most variable. Depending on the function of the overall system, the content can be a piece of data, a signal, or instructions for the remote computer to evaluate a function with some arguments.",
      "The client-server architecture is a way to dispense a service from a central source. There is a single server that provides a service, and multiple clients that communicate with the server to consume its products. In this architecture, clients and servers have different jobs. The server's job is to respond to service requests from clients, while a client's job is to use the data provided in response in order to perform some task.\nAlthough the client/server model can be used by programs within a single computer, it is a more important concept for networking.  In this case, the client establishes a connection to the server over a local area network (LAN) or wide-area network (WAN), such as the Internet. Once the server has fulfilled the client's request, the connection is terminated. Your Web browser is a client program that has requested a service  from a server; in fact, the service and resouce the server provided is the delivery of this Web page.",
      "In computer science, shared memory is memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies. Shared memory is an efficient means of passing data between programs.\nFor example, a client process may have data to pass to a server process that the server process is to modify and return to the client. Ordinarily, this would require the client writing to an output file (using the buffers of the operating system) and the server then reading that file as input from the buffers to its own work space. Using a designated area of shared memory, the data can be made directly accessible to both processes without having to use the system services. To put the data in shared memory, the client gets access to shared memory after checking a semaphore value, writes the data, and then resets the semaphore to signal to the server (which periodically checks shared memory for possible input) that data is waiting. In turn, the server process writes data back to the shared memory area, using the semaphore to indicate that data is ready to be read.",
      "In computer science, distributed shared memory (DSM) is a form of memory architecture where physically separated memories can be addressed as one logically shared address space. Here, the term \"shared\" does not mean that there is a single centralized memory, but that the address space is \"shared\" (same physical address on two processors refers to the same location in memory). Distributed global address space (DGAS), is a similar term for a wide class of software and hardware implementations, in which each node of a cluster has access to shared memory in addition to each node's non-shared private memory.",
      "A distributed system is a network of autonomous computers that communicate with each other in order to achieve a goal. The computers in a distributed system are independent and do not physically share memory or processors. They communicate with each other using messages, pieces of information transferred from one computer to another over a network. Messages can communicate many things: computers can tell other computers to execute a procedures with particular arguments, they can send and receive packets of data, or send signals that tell other computers to behave a certain way."
    ]
  },
  {
    "Operating_Systems": [
      "Error handling refers to the anticipation, detection, and resolution of programming, application, and communications errors. Specialized programs, called error handlers, are available for some applications. The best programs of this type forestall errors if possible, recover from them when they occur without terminating the application, or (if all else fails) gracefully terminate an affected application and save the error information to a log file.",
      "Time-sharing is a technique which enables many people, located at various terminals, to use a particular computer system at the same time. Time-sharing or multitasking is a logical extension of multiprogramming. Processor's time which is shared among multiple users simultaneously is termed as time-sharing.",
      "A network operating system (NOS) is a computer operating system system that is designed primarily to support workstation, personal computer, and, in some instances, older terminal that are connected on a local area network (LAN).",
      "Short for input/output. The term I/O is used to describe any program, operation or device that transfers data to or from a computer and to or from a peripheral device. Every transfer is an output from one device and an input into another. ... I/O adapter",
      "Computer security, also known as cyber security or IT security, is the protection of computer systems from the theft or damage to their hardware, software or information, as well as from disruption or misdirection of the services they provide. \n\nCyber security includes controlling physical access to the hardware, as well as protecting against harm that may come via network access, data and code injection. Also, due to malpractice by operators, whether intentional, accidental, IT security is susceptible to being tricked into deviating from secure procedures through various methods",
      "An Operating System (OS) is an interface between a computer user and computer hardware. An operating system is a software which performs all the basic tasks like file management, memory management, process management, handling input and output, and controlling peripheral devices such as disk drives and printers.",
      "Firm/soft real time systems can miss some deadlines, but eventually performance will degrade if too many are missed. A good example is the sound system in your computer. If you miss a few bits, no big deal, but miss too many and you're going to eventually degrade the system. Similar would be seismic sensors. If you miss a few datapoints, no big deal, but you have to catch most of them to make sense of the data. More importantly, nobody is going to die if they don't work correctly.",
      "In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to allow the processes to manage the shared data.",
      "A hard real-time system (also known as an immediate real-time system) is hardware or software that must operate within the confines of a stringent deadline. The application may be considered to have failed if it does not complete its function within the allotted time span.",
      "In computer programming, resource management refers to techniques for managing resources (components with limited availability).\n\nComputer programs may manage their own resources by using features exposed by programming languages (Elder, Jackson & Liblit (2008) is a survey article contrasting different approaches), or may elect to manage them by a host – an operating system or virtual machine – or another program.\n\nHost-based management is known as resource tracking, and consists of cleaning up resource leaks: terminating access to resources that have been acquired but not released after use. This is known as reclaiming resources, and is analogous to garbage collection for memory. On many systems the operating system reclaims resources after the process makes the exit system call.",
      "it covers exploring the directory structure, finding out what files and folders are present, and performing other file-related operations, such as moving, copying, and deleting objects from the disk. The core motive behind this article is to explore types defined in the System.IO namespace and to promote an understanding of various ways to read from and write to character-based, binary-based, and string-based data stores.",
      "A real-time operating system (RTOS) is an operating system (OS) intended to serve real-time applications that process data as it comes in, typically without buffer delays. Processing time requirements (including any OS delay) are measured in tenths of seconds or shorter increments of time.",
      "A distributed operating system is a software over a collection of independent, networked, communicating, and physically separate computational nodes. Each individual node holds a specific software subset of the global aggregate operating system.",
      "Execution in computer and software engineering is the process by which a computer or a virtual machine performs the instructions of a computer program. The instructions in the program trigger sequences of simple actions on the executing machine.",
      "Batch processing is the execution of a series of jobs in a program on a computer without manual intervention (non-interactive). ... However, this distinction has largely been lost, and the series of steps in a batch process are often called a \"job\" or \"batch job\".",
	"divided smart body command structure transparent relation indicates written make 8051 usage three information access using files system variablelength sibling single addressing operating large supported c main modest enables allocation directory designed use functions 1 amount types card child data manage actual parent contains get fpgabased execution management needs tested processor reduce hierarchy connected memory facilitate fixedlength file every block blocks implemented record method rooted uses table used parts byte type bat condition relationship language",
	"algorithms highlevel mechanisms present operating people methods interactive often grasp third management student tractable newly science display abstractions process performed interest frame disconnected need relating bridge user test project undergraduate applications providing details systems second structures research theoretical course shared system homework experiments software lowlevel involved environment outline still programming theory tasks implementation complicated memory synchronization curricula planned skills provided acquired anchor examples courses computer describing improving lack taught features essential connect accessible students requirements actually teach first gap way os improvements plan internal concrete may particularly practice development design architecture describe concepts run tutoring paper many discuss struggle propose performs processes difficult class assignments future textbook tool",
	"conceived particularly mechanism structure written systems design viewpoint appropriate interacting embedded paper uniform system principles applicable environments operating describes activation control simple assertative although expectation suspension data dynamic kind processes introduced meet objectives directed language",
	"unlike announced structure systems establish structured present based many operatingsystem hardware single computer operating layers new processoriented environment approach parallel available unixlike multimicroprocessor market permits construction hierarchical proposed dependencies environmentis distributed almost local distribution method existing user designer concept heterogeneous",
	"injected finegrained 90 next mechanism transparently transparent problems timely performance virtualization recovers protects faulty isolation machines inside vm architecture advantage access faults act records page wrappers paper vms strong system write permissions isolates isolated operating monitors describes establishes secondly operation different control detected provide effective reliability manner provides however time examines including correctness finally show prevents due operations reusability experimental achieves losses chariot technology first driver retains shadow monitoring capture table resides effectively virtual illegal spread solve results",
	"real machine structure problems systems written description associated highlevel major nevertheless os6 special paper system runs fact many single operating large considerably therefore control simple interpreter feature however time features including one multiprogramming avoids bcpl hierarchical jobcontrol entirely important interest nature need simplified avoided implemented almost several user copes virtual language",
	"ros applications internet suggested evaluation qualitative handling alongside framework practical iot architecture resources wide quantitative information services advances increasingly paper system recovery behind eg interesting awareness presents becoming operating overhead array reason dyknow capabilities minimal analysis robot computational data available standard show advantages set support effectiveness considering offers aimed case empirical stream introduced studies changes varying extends reasoning fault things reconfiguration important need sensor situation runtime areas compared",
	"applications may migrate mechanism nodes subspaces potential entry restrict shrinking evaluate 44 server stores evolve information page technique using transactions integrated network paper system topologies transfers guarantees precise scientific filtering operating broadcast snoop coherence different simple scaling provide change although overheads phases propose however adjust unordered dynamically sharer requests contains due fast reduces token sharers difficult memory generate always protocols obsolete mesh robust subspace removes supports cachetocache threads snooping table networks snoopbased subset type designs 16core average snoops called",
	"language considerations suggested may development systems built components framework sharing described design resources treated understand structured since system interrelationships fact tree among presented operating must resourcetoprocess given different provide designed use basic socalled within levels approach environment defined controlled pdp1145 distinguish emphasises developed introduced growth ‘individualised’ construction upon allowed facilitate inevitable process resource later need way communication tables abstraction useful differently used type virtual terminology concept specific evolved",
	"forever complexity harder applications possible explore machine paravirtops trend coordinate linux increasing systems virtualization make making isolation purposes vm functionalities hard advances airplanes operation run embedded safecritical paper system vms guarantees growing many number hardware single multiple handle operating realm proper control also different units cars implies use confined hyperepos bring provides however hypervisor define human including manage deal current whole interaction paravirtualization general microcontroller capacity platform requirements purpose reduction thus problem tasks top cost connectivity technology realtime mcus able interface virtual virtualized",
	"striking framework information itdetermines operating overhead turn size x8664 provides range pagebased apis tlb 1gb frequency automaticadjustment pairs give either applications linux ofthe example tocall system dpsa workloads based systemlevel establishes work use change dynamic split benefit occupy operations intel programmers influences adjustment well memory translation another characteristic keeps virtual initially footprints map technologies access via page pair pages unit supported large exposed balance physical granularity miss support widely free key memoryintensive faultsand modes os runtime contiguous aims freedom orders checkpoint application architecture apisaccording paper track pattern regions also incremental basic amount functions hit mode enable mitigate address fixed penalty tradeoff 2mb monitoring moreover copying rate merge huge"
    ]
  },
  {
    "Programming_languages": [
      "Objects are key to understanding object-oriented technology. Look around right now and you'll find many examples of real-world objects: your dog, your desk, your television set, your bicycle.\n\nReal-world objects share two characteristics: They all have state and behavior. Dogs have state (name, color, breed, hungry) and behavior (barking, fetching, wagging tail). Bicycles also have state (current gear, current pedal cadence, current speed) and behavior (changing gear, changing pedal cadence, applying brakes). Identifying the state and behavior for real-world objects is a great way to begin thinking in terms of object-oriented programming.\n\nTake a minute right now to observe the real-world objects that are in your immediate area. For each object that you see, ask yourself two questions: \"What possible states can this object be in?\" and \"What possible behavior can this object perform?\". Make sure to write down your observations. As you do, you'll notice that real-world objects vary in complexity; your desktop lamp may have only two possible states (on and off) and two possible behaviors (turn on, turn off), but your desktop radio might have additional states (on, off, current volume, current station) and behavior (turn on, turn off, increase volume, decrease volume, seek, scan, and tune). You may also notice that some objects, in turn, will also contain other objects. These real-world observations all translate into the world of object-oriented programming.",
      "A package is a namespace that organizes a set of related classes and interfaces. Conceptually you can think of packages as being similar to different folders on your computer. You might keep HTML pages in one folder, images in another, and scripts or applications in yet another. Because software written in the Java programming language can be composed of hundreds or thousands of individual classes, it makes sense to keep things organized by placing related classes and interfaces into packages.\n\nThe Java platform provides an enormous class library (a set of packages) suitable for use in your own applications. This library is known as the \"Application Programming Interface\", or \"API\" for short. Its packages represent the tasks most commonly associated with general-purpose programming. For example, a String object contains state and behavior for character strings; a File object allows a programmer to easily create, delete, inspect, compare, or modify a file on the filesystem; a Socket object allows for the creation and use of network sockets; various GUI objects control buttons and checkboxes and anything else related to graphical user interfaces. There are literally thousands of classes to choose from. This allows you, the programmer, to focus on the design of your particular application, rather than the infrastructure required to make it work.\n\nThe Java Platform API Specification contains the complete listing for all packages, interfaces, classes, fields, and methods supplied by the Java SE platform. Load the page in your browser and bookmark it. As a programmer, it will become your single most important piece of reference documentation.",
      "An interface is one of the more overloaded and confusing terms in development.\n\nIt is actually a concept of abstraction and encapsulation. For a given \"box\", it declares the \"inputs\" and \"outputs\" of that box. In the world of software, that usually means the operations that can be invoked on the box (along with arguments) and in some cases the return types of these operations.\n\nWhat it does not do is that it does not define what the semantics of these operations are, although it is commonplace (and very good practice) to document them in proximity to the declaration (e.g., via comments), or to pick good naming conventions. Nevertheless, there are no guarantees that these intentions would be followed.\n\nHere is an analogy: Take a look at your television when it is off. Its interface are the buttons it has, the various plugs, and the screen. Its semantics and behavior are that it takes inputs (e.g., cable programming) and has outputs (display on the screen, sound, etc.). However, when you look at a TV that is not plugged in, you are projecting your expected semantics into an interface. For all you know, the TV could just explode when you plug it in. However, based on its \"interface\" you can assume that it won't make any coffee since it doesn't have a water intake.\n\nIn object oriented programming, an interface generally defines the set of methods (or messages) that an instance of a class that has that interface could respond to.\n\nWhat adds to the confusion is that in some languages, like Java, there is an actual interface with its language specific semantics. In Java, for example, it is a set of method declarations, with no implementation, but an interface also corresponds to a type and obeys various typing rules.\n\nIn other languages, like C++, you do not have interfaces. A class itself defines methods, but you could think of the interface of the class as the declarations of the non-private methods. Because of how C++ compiles, you get header files where you could have the \"interface\" of the class without actual implementation. You could also mimic Java interfaces with abstract classes with pure virtual functions, etc.\n\nAn interface is most certainly not a blueprint for a class. A blueprint, by one definition is a \"detailed plan of action\". An interface promises nothing about an action! The source of the confusion is that in most languages, if you have an interface type that defines a set of methods, the class that implements it \"repeats\" the same methods (but provides definition), so the interface looks like a skeleton or an outline of the class.",
      "Dart is an open source, structured Web programming language developed by Google and announced to the public in 2011. Google intended this Web programming language to replace JavaScript, as it addresses several key problems with the JavaScript language. The Dart language is class-based and object oriented language with C-style syntax. Google engineers describe Dart as \"easy to write development tools for, well-suited to modern app development and capable of high-performance implementations.\"",
      "A programming language is a formal computer language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.",
      "Python is an interpreted, object-oriented programming language similar to PERL, that has gained popularity because of its clear syntax and readability. Python is said to be relatively easy to learn and portable, meaning its statements can be interpreted in a number of operating systems, including UNIX-based systems, Mac OS, MS-DOS, OS/2, and various versions of Microsoft Windows 98. Python was created by Guido van Rossum, a former resident of the Netherlands, whose favorite comedy group at the time was Monty Python's Flying Circus. The source code is freely available and open for modification and reuse. Python has a significant number of users.  A notable feature of Python is its indenting of source statements to make the code easier to read. Python offers dynamic data type, ready-made class, and interfaces to many system calls and libraries. It can be extended, using the C or C++ language.\n\nPython can be used as the script in Microsoft's Active Server Page (ASP) technology. The scoreboard system for the Melbourne (Australia) Cricket Ground is written in Python. Z Object Publishing Environment, a popular Web application server, is also written in the Python language.",
      "Imperative programming is a paradigm of computer programming in which the program describes a sequence of steps that change the state of the computer. Unlike declarative programming, which describes \"what\" a program should accomplish, imperative programming explicitly tells the computer \"how\" to accomplish it. Programs written this way often compile to binary executables that run more efficiently since all CPU instructions are themselves imperative statements.\n\nTo make programs simpler for a human to read and write, imperative statements can be grouped into sections known as code blocks. In the 1950s, the idea of grouping a program's code into blocks was first implemented in the ALGOL programming language. They were originally called \"compound statements,\" but today these blocks of code are known as procedures. Once a procedure is defined, it can be used as a single imperative statement, abstracting the control flow of a program and allowing the developer to express programming ideas more naturally. This type of imperative programming is called procedural programming, and it is a step towards higher-level abstractions such as declarative programming.",
      "1) Functional programming is a style of programming that emphasizes the evaluation of expressions rather than the execution of commands. Erlang programming language is described as a functional programming language. Erlang avoids the use of global variables that can be used in common by multiple functions since changing such a variable in part of a program may have unexpected effects in another part.\n2) In an earlier definition from the ITU-TS, functional programming is \"a method for structuring programs mainly as sequences of possibly nested function procedure calls.\" A function procedure is a relatively simple program that is called by other programs and derives and returns a value to the program that called it.",
      "Perl is a script programming language that is similar in syntax to the C language and that includes a number of popular UNIX facilities such as sed, awk, and tr. Perl is an interpreted language that can optionally be compiled just before execution into either C code or cross-platform bytecode. When compiled, a Perl program is almost (but not quite) as fast as a fully precompiled C language program. Perl is regarded as a good choice for developing common gateway interface (CGI) programs because it has good text manipulation facilities (although it also handles binary files). It was invented by Larry Wall.  In general, Perl is easier to learn and faster to code in than the more structured C and C++ languages. Perl programs can, however, be quite sophisticated. Perl tends to have devoted adherents. A plug-in can be installed for some servers (Apache, for example) so that Perl is loaded permanently in memory, thus reducing compile time and resulting in faster execution of CGI Perl scripts.  Perl was originally said to stand for \"Practical Extraction and Reporting Language\" but that name is no longer used. Larry Wall prefers the usage of an upper-case \"Perl\" for the language itself and lower-case \"perl\" for any interpreter or compiler of Perl.",
      "In general, encapsulation is the inclusion of one thing within another thing so that the included thing is not apparent. Decapsulation is the removal or the making apparent a thing previously encapsulated.\n1) In object-oriented programming, encapsulation is the inclusion within a program object of all the resources need for the object to function - basically, the methods and the data. The object is said to \"publish its interfaces.\" Other objects adhere to these interfaces to use the object without having to be concerned with how the object accomplishes it. The idea is \"don't tell me how you do it; just do it.\" An object can be thought of as a self-contained atom. The object interface consists of public methods and instantiated data.2) In telecommunication, encapsulation is the inclusion of one data structure within another structure so that the first data structure is hidden for the time being. For example, a TCP/IP-formatted data packet can be encapsulated within an ATM frame (another kind of transmitted data unit). Within the context of transmitting and receiving the ATM frame, the encapsulated packet is simply a stream of bits beween the ATM data that describes the transfer.",
      "Modular programming is the process of subdividing a computer program into separate sub-programs.\n\nA module is a separate software component. It can often be used in a variety of applications and functions with other components of the system. Similar functions are grouped in the same unit of programming code and separate functions are developed as separate units of code so that the code can be reused by other applications.\n\nObject-oriented programming (OOP) is compatible with the modular programming concept to a large extent. Modular programming enables multiple programmers to divide up the work and debug pieces of the program independently.\n\n\nModules in modular programming enforce logical boundaries between components and improve maintainability. They are incorporated through interfaces. They are designed in such a way as to minimize dependencies between different modules. Teams can develop modules separately and do not require knowledge of all modules in the system.\n\nEach and every modular application has a version number associated with it. This provides developers flexibility in module maintenance. If any changes have to be applied to a module, only the affected subroutines have to be changed. This makes the program easier to read and understand.\n\nModular programming has a main module and many auxiliary modules. The main module is compiled as an executable (EXE), which calls the auxiliary module functions. Auxiliary modules exist as separate executable files, which load when the main EXE runs. Each module has a unique name assigned in the PROGRAM statement. Function names across modules should be unique for easy access if functions used by the main module must be exported.\n\nLanguages that support the module concept are IBM Assembler, COBOL, RPG, FORTRAN, Morpho, Zonnon and Erlang, among others.\n\nThe benefits of using modular programming include:\n\n    Less code has to be written.\n    A single procedure can be developed for reuse, eliminating the need to retype the code many times.\n    Programs can be designed more easily because a small team deals with only a small part of the entire code.\n    Modular programming allows many programmers to collaborate on the same application.\n    The code is stored across multiple files.\n    Code is short, simple and easy to understand.\n    Errors can easily be identified, as they are localized to a subroutine or function.\n    The same code can be used in many applications.\n    The scoping of variables can easily be controlled.",
      "Haskell /ˈhæskəl/ is a standardized, general-purpose purely functional programming language, with non-strict semantics and strong static typing. It is named after logician Haskell Curry. The latest standard of Haskell is Haskell 2010. ... Its main implementation is the Glasgow Haskell Compiler.",
      "Elixir is a dynamic, functional language designed for building scalable and maintainable applications.\n\nElixir leverages the Erlang VM, known for running low-latency, distributed and fault-tolerant systems, while also being successfully used in web development and the embedded software domain.\nDue to their lightweight nature, it is not uncommon to have hundreds of thousands of processes running concurrently in the same machine. Isolation allows processes to be garbage collected independently, reducing system-wide pauses, and using all machine resources as efficiently as possible (vertical scaling).\n\nProcesses are also able to communicate with other processes running on different machines in the same network. This provides the foundation for distribution, allowing developers to coordinate work across multiple nodes (horizontal scaling).\nThe unavoidable truth about software running in production is that things will go wrong. Even more when we take network, file systems, and other third-party resources into account.",
      "Java is a widely used programming language expressly designed for use in the distributed environment of the internet. It is the most popular programming language for Android smartphone applications and is among the most favored for edge device and internet of things development. Java was designed to have the look and feel of the C++ language, but it is simpler to use than C++ and enforces an object-oriented programming model. Java can be used to create complete applications that may run on a single computer or be distributed among servers and clients in a network. It can also be used to build a small application module or applet for use as part of a webpage. Programs created in Java offer portability in a network. The source code is compiled into what Java calls bytecode, which can be run anywhere in a network on a server or client that has a Java virtual machine (JVM). The JVM interprets the bytecode into code that will run on computer hardware. In contrast, most programming languages, such as COBOL, C++, Visual Basic or Smalltalk, compile code into a binary file. Binary files are platform-specific, so a program written for an Intel-based Windows machine cannot on run a Mac, a Linux-based machine or an IBM mainframe. The JVM includes an optional just-in-time (JIT) compiler that dynamically compiles bytecode into executable code as an alternative to interpreting one bytecode instruction at a time. In many cases, the dynamic JIT compilation is faster than the virtual machine interpretation.",
      "Object-oriented programming (OOP) is a programming language model organized around objects rather than \"actions\" and data rather than logic. Historically, a program has been viewed as a logical procedure that takes input data, processes it, and produces output data.\nThe programming challenge was seen as how to write the logic, not how to define the data. Object-oriented programming takes the view that what we really care about are the objects we want to manipulate rather than the logic required to manipulate them. Examples of objects range from human beings (described by name, address, and so forth) to buildings and floors (whose properties can be described and managed) down to the little widgets on a computer desktop (such as buttons and scroll bars).  Definition\nobject-oriented programming (OOP)\nPosted by: Margaret Rouse\nWhatIs.com\n\nObject-oriented programming (OOP) is a programming language model organized around objects rather than \"actions\" and data rather than logic. Historically, a program has been viewed as a logical procedure that takes input data, processes it, and produces output data.\nDownload this free guide\nDownload Our Guide: Application Integration for SaaS Adoption\n\nMore and more organizations looking to hop on the cloud wagon are facing application and data integration roadblocks. Getting around them is difficult but possible—and a step-by-step approach can help.\n\nThe programming challenge was seen as how to write the logic, not how to define the data. Object-oriented programming takes the view that what we really care about are the objects we want to manipulate rather than the logic required to manipulate them. Examples of objects range from human beings (described by name, address, and so forth) to buildings and floors (whose properties can be described and managed) down to the little widgets on a computer desktop (such as buttons and scroll bars).\n\nThe first step in OOP is to identify all the objects the programmer wants to manipulate and how they relate to each other, an exercise often known as data modeling. Once an object has been identified,  it is generalized as a class of objects (think of Plato's concept of the \"ideal\" chair that stands for all chairs) which defines the kind of data it contains and any logic sequences that can manipulate it. Each distinct logic sequence is known as a method. Objects communicate with well-defined interfaces called messages.\n\nThe concepts and rules used in object-oriented programming provide these important benefits:\n\n    The concept of a data class makes it possible to define subclasses of data objects that share some or all of the main class characteristics. Called inheritance, this property of OOP forces a more thorough data analysis, reduces development time, and ensures more accurate coding.\n    Since a class defines only the data it needs to be concerned with, when an instance of that class (an object) is run, the code will not be able to accidentally access other program data. This characteristic of data hiding provides greater system security and avoids unintended data corruption.\n    The definition of a class is reuseable not only by the program for which it is initially created but also by other object-oriented programs (and, for this reason, can be more easily distributed for use in networks).\n    The concept of data classes allows a programmer to create any new data type that is not already defined in the language itself.\n\nSimula was the first object-oriented programming language. Java, Python, C++, Visual Basic .NET and Ruby are the most popular OOP languages today. The Java programming language is designed especially for use in distributed applications on corporate networks and the Internet. Ruby is used in many Web applications. Curl, Smalltalk, Delphi and Eiffel are also examples of object-oriented programming languages",
      "In object-oriented programming, a method is a programmed procedure that is defined as part of a class and included in any object of that class. A class (and thus an object) can have more than one method. A method in an object can only have access to the data known to that object, which ensures data integrity among the set of objects in an application. A method can be re-used in multiple objects.",
      "PHP (recursive acronym for PHP: Hypertext Preprocessor) is a widely-used open source general-purpose scripting language that is especially suited for web development and can be embedded into HTML.  Instead of lots of commands to output HTML (as seen in C or Perl), PHP pages contain HTML with embedded code that does \"something\" (in this case, output \"Hi, I'm a PHP script!\"). The PHP code is enclosed in special start and end processing instructions <?php and ?> that allow you to jump into and out of \"PHP mode.\"\n\nWhat distinguishes PHP from something like client-side JavaScript is that the code is executed on the server, generating HTML which is then sent to the client. The client would receive the results of running that script, but would not know what the underlying code was. You can even configure your web server to process all your HTML files with PHP, and then there's really no way that users can tell what you have up your sleeve.\n\nThe best things in using PHP are that it is extremely simple for a newcomer, but offers many advanced features for a professional programmer. Don't be afraid reading the long list of PHP's features. You can jump in, in a short time, and start writing simple scripts in a few hours.\n\nAlthough PHP's development is focused on server-side scripting, you can do much more with it. Read on, and see more in the What can PHP do? section, or go right to the introductory tutorial if you are only interested in web programming.",
      "COBOL (Common Business Oriented Language) was the first widely-used high-level programming language for business applications.\n\nWhile the language has been updated over the years, COBOL programs are generally viewed as being outdated. Today, however, a majority of payroll, accounting and other business application programs still use COBOL despite the growing popularity of more modern programming languages such as Java, C++ and .NET. In fact, there are more existing lines of programming code still in use written in COBOL than in any other programming language.\n\n\nCOBOL evolved from the pioneering work of Rear Admiral Grace Hopper in the 1940's. Hopper felt it was important to have a programming language that resembled natural English -- one that would be easy to write and easy to read. In years immediately preceding the year 2000, many COBOL programs required change to accommodate the new century and programmers with COBOL skills were in high demand to prepare legacy code for Y2K. After the turn of the century, the demand for COBOL programmers was not as great and many schools stopped teaching the language. To many people's surprise, COBOL is once again being taught in universities -- this time to support the DevOps movement which requires employees to have both development and system operation skills.",
      "In object-oriented programming, inheritance is the concept that when a class of objects is defined, any subclass that is defined can inherit the definitions of one or more general classes. This means for the programmer that an object in a subclass need not carry its own definition of data and methods that are generic to the class (or classes) of which it is a part. This not only speeds up program development; it also ensures an inherent validity to the defined subclass object (what works and is consistent about the class will also work for the subclass).",
      "Visual Basic (VB) is a programming environment from Microsoft in which a programmer uses a graphical user interface (GUI) to choose and modify preselected sections of code written in the BASIC programming language.",
      "Pascal is a procedural programming language that supports structured programming and data structures to encourage good programming practices. Pascal was originally developed in 1970 by Niklaus Wirth and is named after the famous French mathematician Blaise Pascal.\n\nWhile Pascal is a reliable and efficient programming language, it is mainly used to teach programming techniques. In fact, it is the first language that many programmers learn. There are commercial versions of Pascal that are used, but in general, most developers favor Java, C#, C, C++, etc.\n\n\nPascal uses control structures with reserved words which include if, then, else, while, etc. It also supports data structures and abstractions like records, pointers, type definitions, sets and enumeration. Similar to any other object-oriented programming language, Pascal also has special program structures and control statements. They start with the keyword \"program\" followed by the main block holding the begin and end statements. Data types permit a range of values a variable can hold and are capable of storing and defining a set of operations that can be performed on the data type. Predefined data types supported by Pascal are integer, real, char and Boolean. Pascal has certain unique types like the set type, and user-defined types are defined from other types using the type declaration.",
      "A procedural language is a type of computer programming language that specifies a series of well-structured steps and procedures within its programming context to compose a program. It contains a systematic order of statements, functions and commands to complete a computational task or program.\n\nProcedural language is also known as imperative language.\n\n\nA procedural language, as the name implies, relies on predefined and well-organized procedures, functions or sub-routines in a program’s architecture by specifying all the steps that the computer must take to reach a desired state or output.\n\nThe procedural language segregates a program within variables, functions, statements and conditional operators. Procedures or functions are implemented on the data and variables to perform a task. These procedures can be called/invoked anywhere between the program hierarchy, and by other procedures as well. A program written in procedural language contains one or more procedures.\n\nProcedural language is one of the most common types of programming languages in use, with notable languages such as C/C++, Java, ColdFusion and PASCAL.",
      "In object-oriented programming , a class is a template definition of the method s and variable s in a particular kind of object . Thus, an object is a specific instance of a class; it contains real values instead of variables.\n\nThe class is one of the defining ideas of object-oriented programming. Among the important ideas about classes are:\n\n    A class can have subclasses that can inherit all or some of the characteristics of the class. In relation to each subclass, the class becomes the superclass.\n    Subclasses can also define their own methods and variables that are not part of their superclass.\n    The structure of a class and its subclasses is called the class hierarchy.",
      ".NET is both a business strategy from Microsoft and its collection of programming support for what are known as Web services, the ability to use the Web rather than your own computer for various services. Microsoft's goal is to provide individual and business users with a seamlessly interoperable and Web-enabled interface for applications and computing devices and to make computing activities increasingly Web browser-oriented. The .NET platform includes servers; building-block services, such as Web-based data storage; and device software. It also includes Passport, Microsoft's fill-in-the-form-only-once identity verification service. \n\n.NET\nPosted by: Margaret Rouse\nWhatIs.com\nThis definition is part of our Essential Guide: .NET programming language tutorial: Making more with .NET development\n\n.NET is both a business strategy from Microsoft and its collection of programming support for what are known as Web services, the ability to use the Web rather than your own computer for various services. Microsoft's goal is to provide individual and business users with a seamlessly interoperable and Web-enabled interface for applications and computing devices and to make computing activities increasingly Web browser-oriented. The .NET platform includes servers; building-block services, such as Web-based data storage; and device software. It also includes Passport, Microsoft's fill-in-the-form-only-once identity verification service.\nDownload this free guide\nThe Benefits of a DevOps Approach\n\nBringing development and IT ops together can help you address many app deployment challenges. Our expert guide highlights the benefits of a DevOps approach. Explore how you can successfully integrate your teams to improve collaboration, streamline testing, and more.\n\nThe .NET platform was designed to provide:\n\n    The ability to make the entire range of computing devices work together and to have user information automatically updated and synchronized on all of them\n    Increased interactive capability for Web sites, enabled by greater use of XML (Extensible Markup Language) rather than HTML\n    A premium online subscription service, that will feature customized access and delivery of products and services to the user from a central starting point for the management of various applications, such as e-mail, for example, or software, such as Office .NET\n    Centralized data storage, which will increase efficiency and ease of access to information, as well as synchronization of information among users and devices\n    The ability to integrate various communications media, such as e-mail, faxes, and telephones\n    For developers, the ability to create reusable modules, which should increase productivity and reduce the number of programming errors\n\nAccording to Bill Gates, Microsoft expects that .NET will have as significant an effect on the computing world as the introduction of Windows. One concern being voiced is that although .NET's services will be accessible through any browser, they are likely to function more fully on products designed to work with .NET code.\n\nThe full release of .NET is expected to take several years to complete, with intermittent releases of products such as a personal security service and new versions of Windows and Office that implement the .NET strategy coming on the market separately. Visual Studio .NET is a development environment that is now available. Windows XP supports certain .NET capabilities.",
      "Smalltalk is a programming language that was designed expressly to support the concepts of object-oriented programming . In the early 1970's, Alan Kay led a team of researchers at Xerox to invent a language that let programmers envision the data objects they intended to manipulate. Unlike C++ , Smalltalk was not built on the syntax of a procedural language; it is a \"pure\" object-oriented language with more rigorously enforced rules than C++, which permits some of the procedural constructs of the C language.\n\nAlthough Smalltalk may continue to attract a loyal following, Java , a derivative of C++ designed for distributed systems, has become the most prevalent object-oriented language on the Web.",
      "Swift is a general-purpose, multi-paradigm, compiled programming language developed by Apple Inc. for iOS, macOS, watchOS, tvOS, and Linux. Swift is designed to work with Apple's Cocoa and Cocoa Touch frameworks and the large body of extant Objective-C (ObjC) code written for Apple products. It is built with the open source LLVM compiler framework and has been included in Xcode since version 6. On platforms other than Linux,[9] it uses the Objective-C runtime library which allows C, Objective-C, C++ and Swift code to run within one program.[10]\n\nSwift is intended to be more resilient to erroneous code (\"safer\") than Objective-C, and more concise. However it supports many core concepts that are associated with Objective-C; notably dynamic dispatch, widespread late binding, extensible programming and similar features. For safety, Swift helps address common programming errors like null pointers, and provides syntactic sugar to avoid the pyramid of doom that can otherwise result. More fundamentally, Swift adds the concept of protocol extensibility, an extensibility system that can be applied to types, structs and classes. Apple promotes this as a real change in programming paradigms they term \"protocol-oriented programming\".[11]\n\nSwift was introduced at Apple's 2014 Worldwide Developers Conference (WWDC).[12] It underwent an upgrade to version 1.2 during 2014 and a more major upgrade to Swift 2 at WWDC 2015. Initially a proprietary language, version 2.2 was made open-source software under the Apache License 2.0 on December 3, 2015, for Apple's platforms and Linux.[13][14]\n\nIn March 2017, less than three years after its official debut, Swift made the top 10 in the monthly TIOBE index ranking of popular programming languages.[15]",
      "C is a high-level and general-purpose programming language that is ideal for developing firmware or portable applications. Originally intended for writing system software, C was developed at Bell Labs by Dennis Ritchie for the Unix Operating System (OS) in the early 1970s. \n\n\nC belongs to the structured, procedural paradigms of languages. It is proven, flexible and powerful and may be used for a variety of different applications. Although high-level, C and assembly language share many of the same attributes.\n\nSome of C's most important features include:\n\n    Fixed number of keywords, including a set of control primitives, such as if, for, while, switch and do while\n    Multiple logical and mathematical operators, including bit manipulators\n    Multiple assignments may be applied in a single statement.\n    Function return values are not always required and may be ignored if unneeded.\n    Typing is static. All data has type but may be implicitly converted.\n    Basic form of modularity, as files may be separately compiled and linked\n    Control of function and object visibility to other files via extern and static attributes",
      "MATLAB is a fourth-generation programming language and numerical analysis environment.\n\nUses for MATLAB include matrix calculations, developing and running algorithms, creating user interfaces (UI) and data visualization. The multi-paradigm numerical computing environment allows developers to interface with programs developed in different languages, which makes it possible to harness the unique strengths of each language for various purposes.\n\nMATLAB is used by engineers and scientists in many fields such as image and signal processing, communications, control systems for industry, smart grid design, robotics as well as computational finance.\n\nCleve Moler, a professor of Computer Science at the University of New Mexico, created MATLAB in the 1970s to help his students. MATLAB's commercial potential was identified by visiting engineer Jack little in 1983. Moler, Little and Steve Bangart founded MathWorks and rewrote MATLAB in C under the auspices of their new company in 1984.",
      "C++ is a general-purpose object-oriented programming (OOP) language, developed by Bjarne Stroustrup, and is an extension of the C language. It is therefore possible to code C++ in a \"C style\" or \"object-oriented style.\" In certain scenarios, it can be coded in either way and is thus an effective example of a hybrid language.\n\nC++ is considered to be an intermediate-level language, as it encapsulates both high- and low-level language features. Initially, the language was called \"C with classes\" as it had all the properties of the C language with an additional concept of \"classes.\" However, it was renamed C++ in 1983.\n\nIt is pronounced \"see-plus-plus.\"\n\n\n\nC++ is one of the most popular languages primarily utilized with system/application software, drivers, client-server applications and embedded firmware.\n\nThe main highlight of C++ is a collection of predefined classes, which are data types that can be instantiated multiple times. The language also facilitates declaration of user-defined classes. Classes can further accommodate member functions to implement specific functionality. Multiple objects of a particular class can be defined to implement the functions within the class. Objects can be defined as instances created at run time. These classes can also be inherited by other new classes which take in the public and protected functionalities by default.\n\nC++ includes several operators such as comparison, arithmetic, bit manipulation and logical operators. One of the most attractive features of C++ is that it enables the overloading of certain operators such as addition.\n\nA few of the essential concepts within the C++ programming language include polymorphism, virtual and friend functions, templates, namespaces and pointers.",
      "Declarative Programs are context-independent. Because they only declare what the ultimate goal is, but not the intermediary steps to reach that goal, the same program can be used in different contexts. This is hard to do with imperative programs, because they often depend on the context (e.g. hidden state).\n\nTake yacc as an example. It's a parser generator aka. compiler compiler, an external declarative DSL for describing the grammar of a language, so that a parser for that language can automatically be generated from the description. Because of its context independence, you can do many different things with such a grammar:\n\n    Generate a C parser for that grammar (the original use case for yacc)\n    Generate a C++ parser for that grammar\n    Generate a Java parser for that grammar (using Jay)\n    Generate a C# parser for that grammar (using GPPG)\n    Generate a Ruby parser for that grammar (using Racc)\n    Generate a tree visualization for that grammar (using GraphViz)\n    simply do some pretty-printing, fancy-formatting and syntax highlighting of the yacc source file itself and include it in your Reference Manual as a syntactic specification of your language\n\nBecause you don't prescribe the computer which steps to take and in what order, it can rearrange your program much more freely, maybe even execute some tasks in parallel. A good example is a query planner and query optimizer for a SQL database. Most SQL databases allow you to display the query that they are actually executing vs. the query that you asked them to execute. Often, those queries look nothing like each other. The query planner takes things into account that you wouldn't even have dreamed of: rotational latency of the disk platter, for example or the fact that some completely different application for a completely different user just executed a similar query and the table that you are joining with and that you worked so hard to avoid loading is already in memory anyway.\n\nThere is an interesting trade-off here: the machine has to work harder to figure out how to do something than it would in an imperative language, but when it does figure it out, it has much more freedom and much more information for the optimization stage.",
      "Logic programming is a type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, Answer set programming (ASP) and Datalog",
      "Generic programming means that you are not writing source code that is compiled as-is but that you write \"templates\" of source codes that the compiler in the process of compilation transforms into source codes. The simplest example for generic programming are container classes like arrays, lists or maps that contain a collection of other objects. But there's much more to generic programming. In the context of C++ (and called meta programming) it means to write programs that are evaluated at compile time.\n\nA basic example of generic programming are templates of containers: In a statically typed language like C++ you would have to declare separate containers that hold integers, floats, and other types or deal with pointers to void and therefore losing all type information. Templates which are the C++ way of generic programming leverage this constraint by letting you define classes where one or more parameters are unspecified at the time you define the class. When you instance the template later you tell the compiler which type it should use to create the class out of the template",
      "C#\n\nC# (pronounced \"C-sharp\") is an object-oriented programming language from Microsoft that aims to combine the computing power of C++ with the programming ease of Visual Basic. C# is based on C++ and contains features similar to those of Java.\n\nC#\nPosted by: Margaret Rouse\nWhatIs.com\n\nC# (pronounced \"C-sharp\") is an object-oriented programming language from Microsoft that aims to combine the computing power of C++ with the programming ease of Visual Basic. C# is based on C++ and contains features similar to those of Java.\nDownload this free guide\nThe Benefits of a DevOps Approach\n\nBringing development and IT ops together can help you address many app deployment challenges. Our expert guide highlights the benefits of a DevOps approach. Explore how you can successfully integrate your teams to improve collaboration, streamline testing, and more.\n\nC# is designed to work with Microsoft's .Net platform. Microsoft's aim is to facilitate the exchange of information and services over the Web, and to enable developers to build highly portable applications. C# simplifies programming through its use of Extensible Markup Language (XML) and Simple Object Access Protocol (SOAP) which allow access to a programming object or method without requiring the programmer to write additional code for each step. Because programmers can build on existing code, rather than repeatedly duplicating it, C# is expected to make it faster and less expensive to get new products and services to market.\n\nMicrosoft is collaborating with ECMA, the international standards body, to create a standard for C#. International Standards Organization (ISO) recognition for C# would encourage other companies to develop their own versions of the language. Companies that are already using C# include Apex Software, Bunka Orient, Component Source, devSoft, FarPoint Technologies, LEAD Technologies, ProtoView, and Seagate Software.",
      "Structured programming is a logical programming method that is considered a precursor to object-oriented programming (OOP). Structured programming facilitates program understanding and modification and has a top-down design approach, where a system is divided into compositional subsystems. \n\nStructured programming is a procedural programming subset that reduces the need for goto statements. In many ways, OOP is considered a type of structured programming that deploys structured programming techniques. Certain languages – like Pascal, Algorithmic Language (ALGOL) and Ada – are designed to enforce structured programming.\n\nThe structured programming concept was formalized in 1966 by Corrado Böhm and Giuseppe Jacopini, who demonstrated theoretical computer program design through loops, sequences and decisions. In the late 1960s-early 1970s, Edsger W.Dijkstra developed structural programming functionality as a widely used method, in which a program is divided into multiple sections with multiple exits and one access point.\n\nModular programming is another example of structural programming, where a program is divided into interactive modules.",
      "Ruby is an open source, interpreted , object-oriented programming language created by Yukihiro Matsumoto, who chose the gemstone's name to suggest \"a jewel of a language.\" Ruby is designed to be simple, complete, extensible, and portable . Developed mostly on Linux , Ruby works across most platforms, such as most UNIX -based platforms, DOS , Windows , Macintosh , BeOS , and OS/2 , for example. According to proponents, Ruby's simple syntax (partially inspired by Ada and Eiffel ), makes it readable by anyone who is familiar with any modern programming language.\n\nRuby is considered similar to Smalltalk and Perl . The authors of the book Programming Ruby: The Pragmatic Programmer's Guide , David Thomas and Andrew Hunt say that it is fully object-oriented, like Smalltalk, although more conventional to use, and as convenient as Perl, but fully object-oriented, which leads to better structured and easier-to-maintain programs. To be compliant with the principles of extreme programming (XP), Ruby allows portions of projects to be written in other languages if they are better suited.",
      "The R programming language is an open source scripting language for predictive analytics and data visualization.\n\nThe initial version of R was released in 1995 to allow academic statisticians and others with sophisticated programming skills to perform complex data statistical analysis and display the results in any of a multitude of visual graphics. The \"R\" name is derived from the first letter of the names of its two developers, Ross Ihaka and Robert Gentleman, who were associated with the University of Auckland at the time.The R programming language includes functions that support linear modeling, non-linear modeling, classical statistics, classifications, clustering and more. It has remained popular in academic settings due to its robust features and the fact that it is free to download in source code form under the terms of the Free Software Foundation's GNU general public license. It compiles and runs on UNIX platforms and other systems including Linux, Windows and macOS.\n\nThe appeal of the R language has gradually spread out of academia into business settings, as many data analysts who trained on R in college prefer to continue using it rather than pick up a new tool with which they are inexperienced.\nThe R software environment\n\nThe R language programming environment is built around a standard command-line interface. Users leverage this to read data and load it to the workspace, specify commands and receive results. Commands can be anything from simple mathematical operators, including +, -, * and /, to more complicated functions that perform linear regressions and other advanced calculations.\n\nUsers can also write their own functions. The environment allows users to combine individual operations, such as joining separate data files into a single document, pulling out a single variable and running a regression on the resulting data set, into a single function that can be used over and over.\n\nLooping functions are also popular in the R programming environment. These functions allow users to repeatedly perform some action, such as pulling out samples from a larger data set, as many times as the user wants to specify.\nR language pros and cons\n\nMany users of the R programming language like the fact that it is free to download, offers sophisticated data analytics capabilities and has an active community of users online where they can turn to for support.",
      "Polymorphism describes a pattern in object oriented programming in which classes have different functionality while sharing a common interface.\n\nThe beauty of polymorphism is that the code working with the different classes does not need to know which class it is using since they’re all used the same way. A real world analogy for polymorphism is a button. Everyone knows how to use a button: you simply apply pressure to it. What a button “does,” however, depends on what it is connected to and the context in which it is used — but the result does not affect how it is used. If your boss tells you to press a button, you already have all the information needed to perform the task.\n\nIn the programming world, polymorphism is used to make applications more modular and extensible. Instead of messy conditional statements describing different courses of action, you create interchangeable objects that you select based on your needs. That is the basic goal of polymorphism.",
      "An array is a data structure that contains a group of elements. Typically these elements are all of the same data type, such as an integer or string. Arrays are commonly used in computer programs to organize data so that a related set of values can be easily sorted or searched.\n\nFor example, a search engine may use an array to store Web pages found in a search performed by the user. When displaying the results, the program will output one element of the array at a time. This may be done for a specified number of values or until all the values stored in the array have been output. While the program could create a new variable for each result found, storing the results in an array is much more efficient way to manage memory.",
      "FORTRAN (FORmula TRANslation) is a third-generation ( 3GL ) programming language that was designed for use by engineers, mathematicians, and other users and creators of scientific algorithms. It has a very succinct and spartan syntax. Today, the C language has largely displaced FORTRAN.",
      "GNU Octave is software featuring a high-level programming language, primarily intended for numerical computations. Octave helps in solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. It may also be used as a batch-oriented language. Since it is part of the GNU Project, it is free software under the terms of the GNU General Public License. In addition to use on desktops for personal scientific computing, Octave is used in academia and industry. For example, Octave was used on a massive parallel computer at Pittsburgh supercomputing center to find vulnerabilities related to guessing social security numbers.",
      "Scala is an acronym for “Scalable Language”. This means that Scala grows with you. You can play with it by typing one-line expressions and observing the results. But you can also rely on it for large mission critical systems, as many companies, including Twitter, LinkedIn, or Intel do.\n\nTo some, Scala feels like a scripting language. Its syntax is concise and low ceremony; its types get out of the way because the compiler can infer them. There’s a REPL and IDE worksheets for quick feedback. Developers like it so much that Scala won the ScriptBowl contest at the 2012 JavaOne conference.\n\nAt the same time, Scala is the preferred workhorse language for many mission critical server systems. The generated code is on a par with Java’s and its precise typing means that many problems are caught at compile-time rather than after deployment.\n\nAt the root, the language’s scalability is the result of a careful integration of object-oriented and functional language concepts.\nScala is a pure-bred object-oriented language. Conceptually, every value is an object and every operation is a method-call. The language supports advanced component architectures through classes and traits.\n\nMany traditional design patterns in other languages are already natively supported. For instance, singletons are supported through object definitions and visitors are supported through pattern matching. Using implicit classes, Scala even allows you to add new operations to existing classes, no matter whether they come from Scala or Java!\n\nEven though its syntax is fairly conventional, Scala is also a full-blown functional language. It has everything you would expect, including first-class functions, a library with efficient immutable data structures, and a general preference of immutability over mutation.\n\nUnlike with many traditional functional languages, Scala allows a gradual, easy migration to a more functional style. You can start to use it as a “Java without semicolons”. Over time, you can progress to gradually eliminate mutable state in your applications, phasing in safe functional composition patterns instead. As Scala programmers we believe that this progression is often a good idea. At the same time, Scala is not opinionated; you can use it with any style you prefer.\n\nScala runs on the JVM. Java and Scala classes can be freely mixed, no matter whether they reside in different projects or in the same. They can even mutually refer to each other, the Scala compiler contains a subset of a Java compiler to make sense of such recursive dependencies.\n\nJava libraries, frameworks and tools are all available. Build tools like ant or maven, IDEs like Eclipse, IntelliJ, or Netbeans, frameworks like Spring or Hibernate all work seamlessly with Scala. Scala runs on all common JVMs and also on Android.\n\nThe Scala community is an important part of the Java ecosystem. Popular Scala frameworks, including Akka, Finagle, and the Play web framework include dual APIs for Java and Scala.\n\nScala’s approach is to develop a small set of core constructs that can be combined in flexible ways. This applies also to its object-oriented and functional natures. Features from both sides are unified to a degree where Functional and Object-oriented can be seen as two sides of the same coin.\n\nSome examples: Functions in Scala are objects. The function type is just a regular class. The algebraic data types found in languages such as Haskell, F# or ML are modelled in Scala as class hierarchies. Pattern matching is possible over arbitrary classes."
    ]
  },
  {
    "Security-_Privacy_and_Cryptography": [
      "The known-plaintext attack (KPA) is an attack model for cryptanalysis where the attacker has access to both the plaintext (called a crib), and its encrypted version (ciphertext). These can be used to reveal further secret information such as secret keys and code books.",
      "In cryptography, Triple DES (3DES), officially the Triple Data Encryption Algorithm (TDEA or Triple DEA), is a symmetric-key block cipher, which applies the Data Encryption Standard (DES) cipher algorithm three times to each data block.",
      "DES key length and brute-force attacks. The Data Encryption Standard is a block cipher, meaning a cryptographic key and algorithm are applied to a block of data simultaneously rather than one bit at a time. To encrypt a plaintext message, DES groups it into 64-bit blocks.",
      "In mathematics, a finite field or Galois field (so-named in honor of Évariste Galois) is a field that contains a finite number of elements. As with any field, a finite field is a set on which the operations of multiplication, addition, subtraction and division are defined and satisfy certain basic rules. The most common examples of finite fields are given by the integers mod p when p is a prime number.",
      "In cryptography, a side-channel attack is any attack based on information gained from the physical implementation of a cryptosystem, rather than brute force or theoretical weaknesses in the algorithms (compare cryptanalysis).",
      "An encryption algorithm is a component for electronic data transport security. Actual mathematical steps are taken and enlisted when developing algorithms for encryption purposes, and\nvarying block ciphers are used to encrypt electronic data or numbers.\n\nEncryption algorithms help prevent data fraud, such as that perpetrated by hackers who illegally obtain electronic financial information. These algorithms are a part of any company’s risk management protocols and are often found in software applications.\nEncryption algorithms assist in the process of transforming plain text into encrypted text, and then back to plain text for the purpose of securing electronic data when it is transported over networks. By coding or encrypting data, hackers or other unauthorized users are generally unable to access such information. Some encryption algorithms are considered faster than others, but as long as algorithm developers, many of whom have math backgrounds, stay on top of advancements in this technology, this type of encryption should continue to flourish as hackers continue to become more sophisticated.\n\nIn 1977, RSA became one of the first encryption algorithms developed by U.S. mathematicians Ron Rivest, Adi Shamir and Len Adleman. RSA has had ample staying power as it is still widely used for digital signatures and public key encryption. Encryption algorithms can vary in length, but the strength of an algorithm is usually directly proportional to its length.",
      "RSA is an algorithm used by modern computers to encrypt and decrypt messages. It is an asymmetric cryptographic algorithm. Asymmetric means that there are two different keys. This is also called public key cryptography, because one of them can be given to everyone. The other key must be kept private.",
      "In cryptography, power analysis is a form of side channel attack in which the attacker studies the power consumption of a cryptographic hardware device (such as a smart card, tamper-resistant \"black box\", or integrated circuit).",
      "Decryption is the process of transforming data that has been rendered unreadable through encryption back to its unencrypted form. In decryption, the system extracts and converts the garbled data and transforms it to texts and images that are easily understandable not only by the reader but also by the system. Decryption may be accomplished manually or automatically. It may also be performed with a set of keys or passwords.\n One of the foremost reasons for implementing an encryption-decryption system is privacy. As information travels over the World Wide Web, it becomes subject to scrutiny and access from unauthorized individuals or organizations. As a result, data is encrypted to reduce data loss and theft. Some of the common items that are encrypted include email messages, text files, images, user data and directories. The person in charge of decryption receives a prompt or window in which a password may be entered to access encrypted information.",
      "Cryptography is a method of storing and transmitting data in a particular form so that only those for whom it is intended can read and process it.Cryptography is closely related to the disciplines of cryptology and cryptanalysis. Cryptography includes techniques such as microdots, merging words with images, and other ways to hide information in storage or transit. However, in today's computer-centric world, cryptography is most often associated with scrambling plaintext (ordinary text, sometimes referred to as cleartext) into ciphertext (a process called encryption), then back again (known as decryption). Individuals who practice this field are known as cryptographers.\n\nModern cryptography concerns itself with the following four objectives:\n\n1) Confidentiality (the information cannot be understood by anyone for whom it was unintended)\n\n2) Integrity (the information cannot be altered in storage or transit between sender and intended receiver without the alteration being detected)\n\n3) Non-repudiation (the creator/sender of the information cannot deny at a later stage his or her intentions in the creation or transmission of the information)\n4) Authentication (the sender and receiver can confirm each other?s identity and the origin/destination of the information)\n\nProcedures and protocols that meet some or all of the above criteria are known as cryptosystems. Cryptosystems are often thought to refer only to mathematical procedures and computer programs; however, they also include the regulation of human behavior, such as choosing hard-to-guess passwords, logging off unused systems, and not discussing sensitive procedures with outsiders.\n\nThe word is derived from the Greek kryptos, meaning hidden. The origin of cryptography is usually dated from about 2000 BC, with the Egyptian practice of hieroglyphics. These consisted of complex pictograms, the full meaning of which was only known to an elite few. The first known use of a modern cipher was by Julius Caesar (100 BC to 44 BC), who did not trust his messengers when communicating with his governors and officers. For this reason, he created a system in which each character in his messages was replaced by a character three positions ahead of it in the Roman alphabet.\n\nIn recent times, cryptography has turned into a battleground of some of the world's best mathematicians and computer scientists. The ability to securely store and transfer sensitive information has proved a critical factor in success in war and business.\n\nBecause governments do not wish certain entities in and out of their countries to have access to ways to receive and send hidden information that may be a threat to national interests, cryptography has been subject to various restrictions in many countries, ranging from limitations of the usage and export of software to the public dissemination of mathematical concepts that could be used to develop cryptosystems. However, the Internet has allowed the spread of powerful programs and, more importantly, the underlying techniques of cryptography, so that today many of the most advanced cryptosystems and ideas are now in the public domain.",
      "Ciphertext is encrypted text. Plaintext is what you have before encryption, and ciphertext is the encrypted result. The term cipher is sometimes used as a synonym for ciphertext, but it more properly means the method of encryption rather than the result.",
      "A cryptographic attack is a method for circumventing the security of a cryptographic system by finding a weakness in a code, cipher, cryptographic protocol or key management scheme. This process is also called \"cryptanalysis\". See also Category:Computer security exploits, Category:Malware.",
      "A secret key algorithm (sometimes called a symmetric algorithm) is a cryptographic algorithm that uses the same key to encrypt and decrypt data. The best known algorithm is the U.S. Department of Defense's Data Encryption Standard (DES).",
      "In computer security, a man-in-the-middle attack (MITM, sometimes called a \"bucket brigade attack\") is an attack where the attacker secretly relays and possibly alters the communication between two parties who believe they are directly communicating with each other.",
      "A birthday attack is a name used to refer to a class of brute-force attacks. It gets its name from the surprising result that the probability that two or more people in a group of 23 share the same birthday is greater than 1/2; such a result is called a birthday paradox.",
      "In cryptography, a ciphertext-only attack (COA) or known ciphertext attack is an attack model for cryptanalysis where the attacker is assumed to have access only to a set of ciphertexts.",
      "In cryptography, a timing attack is a side channel attack in which the attacker attempts to compromise a cryptosystem by analyzing the time taken to execute cryptographic algorithms. ... Avoidance of timing attacks involves design of constant-time functions and careful testing of the final executable code.",
      "The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\nThe problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography, applied mathematics, and daily fantasy sports.\n\nThe knapsack problem has been studied for more than a century, with early works dating as far back as 1897.[1] The name \"knapsack problem\" dates back to the early works of mathematician Tobias Dantzig (1884–1956),[2] and refers to the commonplace problem of packing your most valuable or useful items without overloading your luggage.",
      "A brute force attack is a trial-and-error method used to obtain information such as a user password or personal identification number (PIN). In a brute force attack, automated software is used to generate a large number of consecutive guesses as to the value of the desired data.",
      "A chosen-plaintext attack (CPA) is an attack model for cryptanalysis which presumes that the attacker can obtain the ciphertexts for arbitrary plaintexts. The goal of the attack is to gain information that reduces the security of the encryption scheme.",
      "Asymmetric cryptography, also known as public key cryptography, uses public and private keys to encrypt and decrypt data. The keys are simply large numbers that have been paired together but are not identical (asymmetric). One key in the pair can be shared with everyone; it is called the public key.",
      "Plaintext is a term used in cryptography that refers to a message before encryption or after decryption. That is, it is a message in a form that is easily readable by humans. Encryption is the process of obscuring messages to make them unreadable in the absence special knowledge.",
      "A decryption key is digital information used to recover the plaintext from the corresponding ciphertext by decryption.",
      "A dictionary attack is a method of breaking into a password-protected computer or server by systematically entering every word in a dictionary as a password. A dictionary attack can also be used in an attempt to find the key necessary to decrypt an encrypted message or document.\nDictionary attacks work because many computer users and businesses insist on using ordinary words as passwords. Dictionary attacks are rarely successful against systems that employ multiple-word phrases, and unsuccessful against systems that employ random combinations of uppercase and lowercase letters mixed up with numerals. In those systems, the brute-force method of attack (in which every possible combination of characters and spaces is tried up to a certain maximum length) can sometimes be effective, although this approach can take a long time to produce results.\n\nVulnerability to password or decryption-key assaults can be reduced to near zero by limiting the number of attempts allowed within a given period of time, and by wisely choosing the password or key. For example, if only three attempts are allowed and then a period of 15 minutes must elapse before the next three attempts are allowed, and if the password or key is a long, meaningless jumble of letters and numerals, a system can be rendered immune to dictionary attacks and practically immune to brute-force attacks.",
      "Search Results\nDifferential fault analysis (DFA) is a type of side channel attack in the field of cryptography, specifically cryptanalysis. The principle is to induce faults—unexpected environmental conditions—into cryptographic implementations, to reveal their internal states.",
      "An encryption key is a random string of bits created explicitly for scrambling and unscrambling data. Encryption keys are designed with algorithms intended to ensure that every key is unpredictable and unique. The longer the key built in this manner, the harder it is to crack the encryption code."
    ]
  },
  {
    "Software_Engineering": [
      "HLD -- High Level Design (HLD) is the overall system design - covering the system architecture and database design. It describes the relation between various modules and functions of the system. data flow, flow charts and data structures are covered under HLD.",
      "The Prototyping Model is a systems development method (SDM) in which a prototype (an early approximation of a final system or product) is built, tested, and then reworked as necessary until an acceptable prototype is finally achieved from which the complete system or product can now be developed.",
      "Software architecture is the structure of structures of an. information system consisting of entities and their externally. visible properties, and the relationships among them. ... A software architecture is a description of the subsystems and. components of a software system and the relationships between.",
      "Computer-aided design (CAD) is the use of computer systems (or workstations) to aid in the creation, modification, analysis, or optimization of a design.[1] CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing.[2] CAD output is often in the form of electronic files for print, machining, or other manufacturing operations. The term CADD (for Computer Aided Design and Drafting) is also used",
      "The application  of  a  systematic,  disciplined,   quantifiable  approach  to the  development, operation and maintenance of software.",
      "Object Oriented Design is the concept that forces programmers to plan out their code in order to have a better flowing program. The origins of object oriented design is debated, but the first languages that supported it included Simula and SmallTalk.",
      "Software design is the process of implementing software solutions to one or more sets of problems. One of the main components of software design is the software requirements analysis (SRA). SRA is a part of the software development process that lists specifications used in software engineering.",
      "The spiral model is a risk-driven process model generator for software projects. Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.",
      "The waterfall model is a popular version of the systems development life cycle model for software engineering. Often considered the classic approach to the systems development life cycle, the waterfall model describes a development method that is linear and sequential. Waterfall development has distinct goals for each phase of development. Imagine a waterfall on the cliff of a steep mountain. Once the water has flowed over the edge of the cliff and has begun its journey down the side of the mountain, it cannot turn back. It is the same with waterfall development. Once a phase of development is completed, the development proceeds to the next phase and there is no turning back. The advantage of waterfall development is that it allows for departmentalization and managerial control. A schedule can be set with deadlines for each stage of development and a product can proceed through the development process like a car in a carwash, and theoretically, be delivered on time. Development moves from concept, through design, implementation, testing, installation, troubleshooting, and ends up at operation and maintenance. Each phase of development proceeds in strict order, without any overlapping or iterative steps.\n\nThe disadvantage of waterfall development is that it does not allow for much reflection or revision. Once an application is in the testing stage, it is very difficult to go back and change something that was not well-thought out in the concept stage. Alternatives to the waterfall model include joint application development (JAD), rapid application development (RAD), synch and stabilize, build and fix, and the spiral model.",
      "Though it varies according to design approach (function oriented or object oriented, yet It may have the following steps involved: A solution design is created from requirement or previous used system and/or system sequence diagram.",
      "The waterfall model is a sequential (non-iterative) design process, used in software development processes, in which progress is seen as flowing steadily downwards (like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, production/implementation and maintenance.",
      "Evolutionary software models are iterative. They are characterized in manner that enables the software engineers to develop increasingly more complete version of a software. That is, initially a rapid version of the product is being developed and then the product is developed to more accurate version with the help of the reviewers who review the product after each release and submit improvements. Specification, development and validation are interleaved rather than separate in evolutionary software process model. These models are applied because as the requirements often change so the end product will be unrealistic, where a complete version is impossible due to tight market deadlines it is better to introduce a limited version to meet the pressure. Thus the software engineers can follow a process model that has been explicitly designed to accommodate a product that gradually complete over time.",
	"seen feedback aims latest cases google’s findings improve development frequently churn quality even 2 impact 3 resources understand “closer” individually report reliance checkins without enormous increased write fail result ever multiple continuous code lag unable developers modified change size 1 feature inform userstools often time tests integration data breaks certain leverage codechange recently show processes generally programming correlations resulting empirically workload languages reduce breakages growth testexecution changes frequencies cause distilling maintain model regression controlling compromising testing rate test project exist ci results dedicated",
	"fixing development high considerable make top5 perform automatically cycles intervention 140 paper critical based automated introduce discuss large among therefore work code developers minimizing momentum identify change software scale lifecycle help google time however propose human provides recently repositories manual novel projects thousands keep expensive especially demand various one regressions identifying tedious tasks algorithm case studies identifies changes proposed introduced 82 performed unsuitable need regression shortcomings rapid candidates savings consuming quickly techniques",
	"conceived assurance practical even privacy purposes abuse success mechanisms determine look adopted changed learning behavior methods exact questions third developed empirical seem dictate 1970’s affirmatively either possible details privilege least providing machine scales benefits discover example outlines three second policy spam demonstrated times critical briefly policies efficiently layers statistics constrain use take modern differential software principle however approach well like models larger countless detailed used security behave efficient learn revisit explain via end services since computer large describes comprises primarily effective expectations help worthwhile comprehensive users’ time thousands whether set particular monitor specifically datadriven users first model intended fighting—but possibility complex radically intruding demonstrate concrete raises practice finally concepts thereby without paper security—as many ways means developers answered provide behaving libraries foundation basis online core could one unclear far realworld monitoring good",
	"computed specification nested 23 meeting positioned report invocations includes multiple input behavior iterative strengths reproduction methods scenarios statically java 10 path invocation interleavings sequential due full inputs goals identifying client retaining iteration reach refinement concurrencyinduced either combines iteratively towards challenging associated 6 three created googleguava reports erroneous versions analysis solving approach maximum combined toward nesting iterations satisfactory dynamic coverage dynamically unknown engine well analyses merging intricacies neither openjdk deep 11 latest synthesize crafting alongside finds failing classes subgoals guaranteed bugs via prototype accuracy comparison beyond location help concurrent crashinducing thread static observed generate validated scalable fuzzingbased subtle manifest synthesis able directed several deeply plan complex traversed break minutes goal accepts step named real across detecting high paths reveals total paper tools library 31 manifestation new previously libraries detect crashes propose expose conditions including generating constraint minion correctness true execution multithreaded welltested 9 executions cause overapproximate defects refines applying method testing popular techniques",
	"programs applications pointsto javascript andor performance causes even 2 imprecision finishes describe ie complete contextsensitive technique constructs accuracy practicality program library comprise specialized graph examine analysis lead functions within precision 10 often improving approach sources find dynamic suggest derived static identifies construction analyses less process challenged first root apply seconds monitoring call able unacceptable moreover minutes original focuses specific results language",
	"productionreadiness machine considerations systems actionable even host examples using research present system based issues found learning large assessing experiments help tests ml offline score set quantify production key complicated realworld small monitoring toy rubric testing test much enough",
	"average build specifies 1010 least problems identified decompose efficient targets—an manually artifact builds modular components 19994 two automatically 2 information total greedy files paper triggers save result introduce found large nphard library code overly iterative 50 effective decomposing prove strongly slow software although java however google 1 time tests triggered mitigate show decomposition analyzes execution unification dependents programmers errorprone one tedious algorithm dependency problem connected decomposed needed would targets less important underutilized smaller best artifacts finding unnecessary test 40000 focuses tool minutes proposes target results",
	"vocabulary common meaning falling systems application considered area isoiec responsible restricted organization definitions suitable information s2escthe scope technical applicable ieeecs 1sc might committee computer directly work chosen neededterms english provide parochial within software provides society terms excluded words 7 international trademarked component company defined circumstances standard ieee general concerned could field iso group one engineering multiword whose technology jtc consistent proprietary definition standards specialization dictionary inferred terminology concept specific",
	"applications findings qualitative improve increasing systems indepth mobile engineers think design motivated major contextualize abb usage correspondingly research services report embedded perspectives paper 18 write tools supported implications describes practices use microsoft interviews centerbased software researchers collected help google data employees field quantitativetargeted 464 directions current consumption concern green ibm suggest engineering requirements expands empirical construct aiming survey known first maintain practitioners energy study strategies existing test little develop",
	"paretobased problems algorithms non evaluate practical quality diverse guide dominated review methods foundations six genetic collected current challenge extensive conducted engineering algorithm derived empirical complementary optimization ii eventually user categories contexts fronts community selects hypervolume performance 2 multiobjective indicators three largely consists using theoretical four recognized based pareto search front assess found solving software domains already solutions direction nature selecting specific results industrial 3 number select large eight lacks literature sbse set key works regard produce sorting extended real practice evidence two appropriate paper eg shown presents many searchbased assessing different 1 despite following one applying nondominated experiment good published"
    ]
  },
  {
    "Data_Structures": [
      "In computer science, a record (also called struct or compound data) is a basic data structure. A record is a collection of fields, possibly of different data types, typically in fixed number and sequence. ... Fields may also be called elements, though these risk confusion with the elements of a collection.",
      "A data structure is specialized form of storing the data",
      "In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored so that the position of each element can be computed from its index tuple by a mathematical formula.",
      "In computer science, a linked list is a linear collection of data elements, called nodes, each pointing to the next node by means of a pointer. It is a data structure consisting of a group of nodes which together represent a sequence.",
      "Queue is an abstract data structure, somewhat similar to Stacks. Unlike stacks, a queue is open at both its ends. One end is always used to insert data (enqueue) and the other is used to remove data (dequeue). Queue follows First-In-First-Out methodology, i.e., the data item stored first will be accessed first.",
      "In computer science, a linked list is a linear collection of data elements, called nodes, each pointing to the next node by means of a pointer. It is a data structure consisting of a group of nodes which together represent a sequence. ... Linked lists are among the simplest and most common data structures.",
      "A stack is a limited access data structure - elements can be added and removed from the stack only at the top. push adds an item to the top of the stack, pop removes the item from the top.",
      "A data structure is a specialized format for organizing and storing data. General data structure types include the array, the file, the record, the table, the tree, and so on. Any data structure is designed to organize data to suit a specific purpose so that it can be accessed and worked with in appropriate ways.",
      "In computer science, a tree is a widely used abstract data type (ADT)—or data structure implementing this ADT—that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.",
	"demonstrate efficient algorithms application performance transformations tailor functional numerical two special vector abstract tailored matrices automated program optimized array implementations use sparse manual sequential emphasis execution programming significant specifications derived attain placed processors highly imperative used mathematical derive techniques superior",
	"viewed originality application systems relation leibniz visible consider via wellknown structures obtained abstract equivalence paper throughout relies represents states part sorted generalize approach data generalization standard set theory hidden finite case algebraic truth deductive designated kdata endowed nerode algebras subset logic congruence automata heterogeneous values",
	"viewed module finegrained possible may handled illustrated improve illustrate stepwise performance systems waiting transmitting instance server datatype consider response background serve coarsegrained housekeeping noninterference abstract proof scheduling system runs using foreground relyguarantee activities code use augmented manner provides clients proceeding approach conditions servers consisting requests processes set periods searchtable processor facilities occur one case developed tasks client simplify process performed distributed maintenance abstraction method concurrency collection dedicated",
	"instead expressed structure motivated major sequences records characteristics among hierarchies data management list ‘axis’ thus base need variations collection common possible systems unique comparing ndimensional structures technique teaching matrices based space concretely affixing determining lessened analysis names myriad kind programming value aggregates well descriptive selecting similar useful another characteristic called provided understanding since describes differentiated arrays concentrate qualities simply actual axis hopefully set contemporary particular languages longer upon observed model idea name answers individual may make structuring paper question exactly delineates ‘aggregate’ along different aggregate basic proffers including defined one advisable class ntuples popular ‘analysis’ sets",
	"applications improve structure parallelism level collector ahead integrates gather structures efficiency energyefficient run decoupled characteristics critical emerging dataset throttle based hardware pes array sqrl enables iterative accelerator instructions aspect software delivery propose knowledge lastlevelcache data exploits kernel tile find eliminate elements computation fashion adaptively llc engine locality memory lightweight datacentric makes ii energy imperative compute structurespecific rate refill processing objects fetch",
	"parallelism structure dynamics even guide gpu information present among overhead coAde statically memorylevel provides 7 data representative transformed optimization discretized transformation grid applications calculations performance example stencil equations structures using c99 wellvectorizable system space include given modern differential syntax languagedefined enabled available dynamically already dense memory 560 distribution codes representation allocated components transformations substantial access automatic arrays effective standardized concurrent improving parallel partial languages worst fluid important model complex memorybound solve heat underlying structured regular variablelength many pattern hardware array compiler also achieved requests distributes layout address case gain speedup choose observe primary good increases tool",
	"nodes sourcelevel solely computes characteristics program bound therefore selection optimizations behavior code incorporate subsets measurements data incomplete limiting done optimization patterns process cache percent integrating 38 latency evaluation usually performance today structures using critical based c chips however centric counters clusters enabled computation domain effectiveness memory analyses targets models fortran adding multichip uses used traces metrics rodina access complete represented realistic ranged targeting benchmarks gains knowledge parallel lowoverhead specifically segments diagnoses important bottlenecks captures accurate application asci appropriate behaviors paper segmentbased nas measurement presents tools hardware multicore pattern also explicitly previous execution one factors easier highly macpo tool",
	"contain approaches applications descriptions require contents great postpetascale machines two describe managing compressed structures scientific middleware paper present layouts footprint compression decreases hpc must specialized turn also overhead 89× amount libraries reduced approach core incorporated data deal compressing cpu consumption layout increase 25 reduce observing reduction implementation compressible memory highly per mpiio used romio entropy techniques opposed",
	"expresses illustrate potential performance non towards become describe finally implementing cachecoherent structures manycore end using generic characteristics present might scalability different also metric implementations reveal experiments java achieve comprehensive scheme approach improving data recently contributing general consumption architectures distill collections nice considering experimental hierarchical top memory package proposed aspects makes performed model utilities distributed future variants energy study power observations concurrency partially techniques collection goal",
	"format may typed heuristics manipulating storing create design functional modification stored term structures concepts examples predicate regarded class’ automatic includes program describes analogous outlined functions approach data enumerating simpler defined lisp ‘first invent lend languages key writing synonymous ‘concept’ legitimate ‘type’ objects concept language",
	"particularly nodes structure steps costs allow considered associated two chained structures constructs minimising supporting tree search length result presented implications main also updating methods discussed time data organisation dynamic activity terminal investigated algorithm optimal static problem doubly weighted file variable maintenance ramifications realisation organised unequal"
    ]
  },
  {
    "High_performance_Computing": [
      "A  Grid  is  a  type  of  parallel  and distributed  system  that  enables the  sharing,  selection,  and aggregation   of   geographically distributed   ‘autonomous’resources   dynamically   at   runtime depending  on  their  availability,capability,  performance,  cost,  and users’  quality-of-service requirements.\nGrid computing is a distributed architecture of large numbers of computers connected to solve a complex problem. In the grid computing model, servers or personal computers run independent tasks and are loosely linked by the Internet or low-speed networks. Computers may connect directly or via scheduling systems.\nMost applications for grid computing projects have no time dependency, and large projects typically deploy across many countries and continents. Search programs and others use the idle power of computers, also known as cycle-scavenging, running in the background for many weeks.\nThe SETI@home project, a search for extraterrestrial intelligence based at the University of California at Berkeley, is an example of grid computing. Millions of PCs run a search program on a segmented piece of radio telescope data with no set completion date. Grids bring computing power to address other projects such as genetics research, drug-candidate matching, even the search -- unsuccessfully -- for Genghis Khan's tomb.\nHowever, the distributed model works well for only a narrow subset of applications. For the application of grid computing, ensure redundancy and robust failure recovery are built into the model, since it's highly probable that some number of compute nodes will disconnect or fail.\nThe grid computing concept differs from parallel computing in supercomputers. Supercomputers run highly connected applications, rather than independently functional nodes. They operate over high-speed networks, and are typically housed in one specialized data center. Grid computers, on the other hand, exchange little or no data and feed the project over Internet connections from geographically dispersed locations.\nCloud computing is another form of distributed computing, falling on the spectrum between grids and supercomputing. Clouds are much more granular and handle time-dependent workloads. Cloud resources can be geographically distributed, but only to a few locations, as compared to thousands or millions feeding a grid project.",
      "Definition: Load balancing is a concept that aims to make a network more efficient. Load balancing distributes of traffic load evenly across a network with multiple-paths, in order to get optimal resource utilization, maximize throughput and minimize response time. Thus load-balancing will split the traffic down the configured paths equally towards the destination. E.g., with two 768 kpbs links and 800 kpbs traffic at any point, conceptually with load-balancing each path should have 400 kpbs worth of traffic. But is that what happens?\n\nDefinition: Load sharing is inherent to the forwarding process of a router to share the forwarding of traffic, if the routing table has multiple paths to a destination. If equal paths, the forwarding process will decide the manner of forwarding and forward packets based on the load-sharing algorithm used. This still bears the possibility of unbalanced forwarding. If unequal paths, the traffic is distributed inversely proportionally to the cost of the routes. That is, paths with lower costs (metrics) are assigned more traffic, and paths with higher costs are assigned less traffic. \nDespite the idea of load balancing, it is not always true. Load sharing is technically more correct terminology, in that traffic is shared across multiple paths even if in a unequal fashion. If you were to look at two comparing traffic graphs, with load balancing the two graphs should be near identical, but in reality with load sharing they might be similar but the traffic flow pattern would be different.\nAnother very important fact to keep in mind is that any load-sharing/load-balancing configurations are UNI-directional, influencing traffic in one direction. If the return traffic had to be balanced in the same manner, the same configuration would be needed in the opposite direction.",
      "High-performance computing (HPC) is the use of parallel processing for running advanced application programs efficiently, reliably and quickly. The term applies especially to systems that function above a teraflop or 1012 floating-point operations per second.\nThe term HPC is occasionally used as a synonym for supercomputing, although technically a supercomputer is a system that performs at or near the currently highest operational rate for computers. Some supercomputers work at more than a petaflop or 1015 floating-point operations per second\nThe most common users of HPC systems are scientific researchers, engineers and academic institutions. Some government agencies, particularly the military, also rely on HPC for complex applications. High-performance systems often use custom-made components in addition to so-called commodity components. As demand for processing power and speed grows, HPC will likely interest businesses of all sizes, particularly for transaction processing and data warehouses. An occasional techno-fiends might use an HPC system to satisfy an exceptional desire for advanced technology.",
      "A  cluster  is  a  type  of  parallel  and  distributed  system, which  co\nnsists  of  a  collection  of  inter-connected stand-alone computers working together as a single integrated computing resource.\nIn its most basic form, a cluster is a system comprising two or more computers or systems (called nodes) which work together to execute applications or perform other tasks, so that users who use them, have the impression that only a single system responds to them, thus creating an illusion of a single resource (virtual machine). This concept is called transparency of the system. As key features for the construction of these platforms is included elevation : reliability, load balancing and performance.\nHigh Availability (HA) and failover clusters, these models are built to provide an availability of services and resources in an uninterrupted manner through the use of implicit redundancy to the system. The general idea is that if a cluster node fail (failover), applications or services may be available in another node. These types are used to cluster data base of critical missions, mail, file and application servers.\nThe load balancing among servers is part of a comprehensive solution in an explosive and increasing use of network and Internet. Providing an increased network capacity, improving performance. A consistent load balancing is shown today as part of the entire Web Hosting and eCommerce project. But you cannot get stuck with the ideas that it is only for providers, we should take their features and bring into the enterprise this way of using technology to heed internal business customers.\nThis technique redirects the requests to the lowest based on the number of requests / server connections. For example, if server 1 is currently handling 50 requests / connections, and server 2 controls 25 requests / connections, the next request / connection will be automatically directed to the second server, since the server currently has fewer requests / connections active.\nThis method uses the technique of always direct requests to the next available server in a circular fashion. For example, incoming connections are directed to the server 1, server 2 and then finally server 3 and then the server 1 returns.\nThis technique directs the requests to the load based on the requests of each and the responsiveness of the same (performance) For example, if the servers server 1 is four times faster in servicing requests from the server 2, the administrator places a greater burden of work for the server 1 to server 2.\nThis combined solution aims to provide a high performance solution combined with the possibility of not having critical stops. This combined cluster is a perfect solution for ISPs and network applications where continuity of operations is very critical.",
      "A Cloud is a type of parallel and distributed system consisting\nof a collection of inter-connected and virtualized computers\nthat are dynamically provisioned and presented as one or more unified computing   resource(s)   based   on   service-level\nagreements   established   through   negotiation between the\nservice provider and consumers.\nSelf-service provisioning: End users can spin up compute resources for almost any type of workload on demand. This eliminates the traditional need for IT administrators to provision and manage compute resources.\n\nElasticity: Companies can scale up as computing needs increase and scale down again as demands decrease. This eliminates the need for massive investments in local infrastructure which may or may not remain active.\nPay per use: Compute resources are measured at a granular level, allowing users to pay only for the resources and workloads they use."
    ]
  },
  {
    "Human_computer_interaction": [
      "HCI (human-computer interaction) is the study of how people interact with computers and to what extent computers are or are not developed for successful interaction with human beings. A significant number of major corporations and academic institutions now study HCI. Historically and with some exceptions, computer system developers have not paid much attention to computer ease-of-use. Many computer users today would argue that computer makers are still not paying enough attention to making their products \"user-friendly.\" However, computer system developers might argue that computers are extremely complex products to design and make and that the demand for the services that computers can provide has always outdriven the demand for ease-of-use. \n One important HCI factor is that different users form different conceptions or mental models about their interactions and have different ways of learning and keeping knowledge and skills (different \"cognitive styles\" as in, for example, \"left-brained\" and \"right-brained\" people). In addition, cultural and national differences play a part. Another consideration in studying or designing HCI is that user interface technology changes rapidly, offering new interaction possibilities to which previous research findings may not apply. Finally, user preferences change as they gradually master new interfaces.",
      "Usability is all about how users interact with technology, and usability engineering studies the human-computer interface (HCI) in depth. Usability engineering requires a firm knowledge of computer science and psychology and approaches product development based on customer feedback. A usability engineer works hand-in-hand with customers, working to develop a better understanding of the functionality and design requirements of a product in order to build more reliable data for it. According to Sun Usability Labs, six general attributes define usability: \nUtility\nLearn-ability\nEfficiency\nRetain-ability \nErrors\nCustomer satisfaction \n\nAccording to Dr. Andreas Holzinger, author of a textbook on software usability engineering, usability must be determined before prototyping takes place. A technique called Usability Context Analysis provides important data for product development. Dr. Holzinger explains that the earlier important design flaws are detected, the greater the chance that these flaws can be corrected. \nUsability is evaluated through testing, inspection and inquiry. It is often approached by having users work on typical tasks within the system or prototype. The evaluators use results gained from testing to determine how well the user interface supports the user. The usability inspection approach helps determine any usability-related advantages and negative issues with the prototype. The inquiry process helps evaluators determine users' likes, dislikes and needs, as well as their understanding of the system. Often, inquiry-process feedback is given in written or verbal form.",
      "Computer Supported Cooperative Work (CSCW) is a community of behavioral researchers and system builders at the intersection of collaborative behaviors and technology. The collaboration can involve a few individuals or a team, it can be within or between organizations, or it can involve an online community that spans the globe. CSCW addresses how different technologies facilitate, impair, or simply change collaborative activities.",
      "User Interface (UI) Design focuses on anticipating what users might need to do and ensuring that the interface has elements that are easy to access, understand, and use to facilitate those actions. UI brings together concepts from interaction design, visual design, and information architecture.\nChoosing Interface Elements\n\nUsers have become familiar with interface elements acting in a certain way, so try to be consistent and predictable in your choices and their layout. Doing so will help with task completion, efficiency, and satisfaction.\n\nInterface elements include but are not limited to:\n\n    Input Controls: buttons, text fields, checkboxes, radio buttons, dropdown lists, list boxes, toggles, date field\n    Navigational Components: breadcrumb, slider, search field, pagination, slider, tags, icons\n    Informational Components: tooltips, icons, progress bar, notifications, message boxes, modal windows\n    Containers: accordion\n\nThere are times when multiple elements might be appropriate for displaying content.  When this happens, it’s important to consider the trade-offs.  For example, sometimes elements that can help save you space, put more of a burden on the user mentally by forcing them to guess what is within the dropdown or what the element might be. \n\n     Keep the interface simple. The best interfaces are almost invisible to the user. They avoid unnecessary elements and are clear in the language they use on labels and in messaging.\n    Create consistency and use common UI elements. By using common elements in your UI, users feel more comfortable and are able to get things done more quickly.  It is also important to create patterns in language, layout and design throughout the site to help facilitate efficiency. Once a user learns how to do something, they should be able to transfer that skill to other parts of the site. \n    Be purposeful in page layout.  Consider the spatial relationships between items on the page and structure the page based on importance. Careful placement of items can help draw attention to the most important pieces of information and can aid scanning and readability.\n    Strategically use color and texture. You can direct attention toward or redirect attention away from items using color, light, contrast, and texture to your advantage.\n    Use typography to create hierarchy and clarity. Carefully consider how you use typeface. Different sizes, fonts, and arrangement of the text to help increase scanability, legibility and readability.\n    Make sure that the system communicates what’s happening.  Always inform your users of location, actions, changes in state, or errors. The use of various UI elements to communicate status and, if necessary, next steps can reduce frustration for your user. \n    Think about the defaults. By carefully thinking about and anticipating the goals people bring to your site, you can create defaults that reduce the burden on the user.  This becomes particularly important when it comes to form design where you might have an opportunity to have some fields pre-chosen or filled out.",
      "Too often, systems are designed with a focus on business goals, fancy features, and the technological capabilities of hardware or software tools. All of these approaches to system design omit the most important part of the process – the end user. User-Centered Design (UCD) is the process of designing a tool, such as a website’s or application’s user interface, from the perspective of how it will be understood and used by a human user. Rather than requiring users to adapt their attitudes and behaviors in order to learn and use a system, a system can be designed to support its intended users’ existing beliefs, attitudes, and behaviors as they relate to the tasks that the system is being designed to support. The result of employing UCD to a system design is a product that offers a more efficient, satisfying, and user-friendly experience for the user, which is likely to increase sales and customer loyalty.",
      "The design of artefacts and in particular artefacts involving computing technology is usually focused on how the artefacts should be used. The aim of cognitive task design is to go beyond that by emphassing the need of considering not only how an artefact is used, but also how the use of the artefact changes the way we think about it and work with it. This is similar to the envisioned world problem. i.e. the paradox that the artefacts we design change very assumptions on which they were designed. the ambition is not to make CTD a new discipline or methodology but rather to offer a unified perspective in existing models, theories and methods that can be instrumental in developing improved systems. In this context, cognition is not defined as a psychological process unique to humans but as a characteristic of system performance, namely the ability to maintain control. The focus of CTD is therefore the joint cognitive sysem, rather that the individual user. CTD has the same roots as cognitive task analysis, but the focus is on macro cognition rather than the knowledge, thought process, and goal structures of the humans in the system.",
      "User Experience Design (UXD) entails conducting user research exercises with intended users of a system. User research reveals users’ needs and preferences through user observations, one-on-one interviews, and creative activities that encourage users to express their emotions, motivations, and underlying concepts and beliefs about the steps involved in task procedures. By understanding the human emotions, motivations, and beliefs that surround a task, a user interface can be designed to accommodate and support user behaviors in a way that users will experience as natural and satisfying.",
      "Human-machine interface (HMI) is a component of certain devices that are capable of handling human-machine interactions. The interface consists of hardware and software that allow user inputs to be translated as signals for machines that, in turn, provide the required result to the user. Human-machine interface technology has been used in different industries like electronics, entertainment, military, medical, etc. Human-machine interfaces help in integrating humans into complex technological systems. Human-machine interface is also known as man-machine interface (MMI), computer-human interface or human-computer interface. \n\nin HMI, the interactions are basically of two types, i.e., human to machine and machine to human. Since HMI technology is ubiquitous, the interfaces involved can include motion sensors, keyboards and similar peripheral devices, speech-recognition interfaces and any other interaction in which information is exchanged using sight, sound, heat and other cognitive and physical modes are considered to be part of HMIs.\n\nAlthough considered as a standalone technological area, HMI technology can be used as an adapter for other technologies. The basis of building HMIs largely depends on the understanding of human physical, behavioral and mental capabilities. In other words, ergonomics forms the principles behind HMIs. Apart from enhancing the user experience and efficiency, HMIs can provide unique opportunities for applications, learning and recreation. In fact, HMI helps in the rapid acquisition of skills for users. A good HMI is able to provide realistic and natural interactions with external devices.\n\nThe advantages provided by incorporating HMIs include error reduction, increased system and user efficiency, improved reliability and maintainability, increased user acceptance and user comfort, reduction in training and skill requirements, reduction in physical or mental stress for users, reduction in task saturation, increased economy of production and productivity, etc.\n\nTouchscreens and membrane switches can be considered as examples of HMIs. HMI technology is also widely used in virtual and flat displays, pattern recognition, Internet and personal computer access, data input for electronic devices, and information fusion.\n\nProfessional bodies like GEIA and ISO provide standards and guidelines applicable for human-machine interface technology.",
      "Bringing Design to Software implies that the object of design is software, leaving out considerations of the interface devices that are the inevitable embodiment of software for the user. Design cannot be neatly divided into compartments for software and for devices: The possibilities for software are both created and constrained by the physical interfaces. In today's world of computer applications, the vast majority of applications present themselves to users in a standard way—a visual display with a keyboard and mouse. But the future of computing will bring richer resources to physical human–computer interactions. Some new devices are already in use on a modest scale—for example, pen-based personal digital assistants (PDAs), virtual-reality goggles and gloves, and computers embedded in electromechanical devices of all kinds. Researchers are exploring further possibilities, including tactile input and output devices, immersive environments, audio spaces, wearable computers, and a host of gadgets that bear little resemblance to today's personal computer or workstation. Many current textbooks and reading collections on human–computer interaction (for example, Shneiderman, 1992; Dix et al., 1993; Preece et al., 1994; and Baecker et al., 1995) include sections on interface devices. As experience with a wider variety of devices accumulates, the design of interaction based on new combinations of devices and software will be an important emerging topic in what we have—for the moment—called software design.\nWhenever someone designs software that interacts with people, the effects of the design extend beyond the software itself to include the experiences that people will have in encountering and using that software. A person encountering any artifact applies knowledge and understanding, based on wide variety of cognitive mechanisms grounded in human capacities for perception, memory, and action. Researchers in human–computer interaction have studied the mental worlds of computer users, developing approaches and methods for predicting properties of the interactions and for supporting the design of interfaces. Although it would be overstating the case to say that the cognitive analysis of human–computer interaction has led to commonly accepted and widely applied methods, there is a substantial literature that can be of value to anyone designing interactive software (for example, Card et al., 1983; Norman and Draper, 1986; Carroll, 1991; Helander, 1988). Practical applications of HCI research are promoted by organizations such as ACM SIGCHI, the Human Factors and Ergonomics Society (see Perlman et al., 1995), and the Usability Professionals Association.\nPerhaps even more difficult than the task of defining software is the task of defining design. A dictionary provides several loosely overlapping meanings, and a glance at the design section in a library or bookstore confuses the issue even more. Although we label it with a noun, design is not a thing. The questions that we can ask fruitfully are about the activity of designing. The authors of our chapters did not produce a simple definition; rather, each contributes to an answer by providing a perspective on what people do when they design.",
      "Ergonomics is sometimes described as “fitting the system to the human,” meaning that through informed decisions; equipment, tools, environments and tasks can be selected and designed to fit unique human abilities and limitations. Typical examples in the “physical ergonomics” arena include designing a lifting job to occur at or near waist height, selecting a tool shape that reduces awkward postures, and reducing unnecessary tasks and movements to increase production or reduce errors and waste. “Cognitive ergonomics,” on the other hand, focuses on the fit between human cognitive abilities and limitations and the machine, task, environment, etc. Example cognitive ergonomics applications include designing a software interface to be “easy to use,” designing a sign so that the majority of people will understand and act in the intended manner, designing an airplane cockpit or nuclear power plant control system so that the operators will not make catastrophic errors.\n Cognitive ergonomics is especially important in the design of complex, high-tech, or automated systems. A poorly designed cellular phone user-interface may not cause an accident, but it may well cause great frustration on the part of the consumer and result in a marketplace driven business failure. A poor interface design on industrial automated equipment, though, may result in decreased production and quality, or even a life threatening accident.\n Complex automated systems create interesting design challenges, and research and post accident analysis indicate that the human role in automated systems must be closely considered. Automation can result in increased operator monitoring and vigilance requirements, complex decision-making requirements, and other issues that can increase the likelihood of errors and accidents. \n Another interesting effect in automation is that humans will sometimes over-trust or mistrust an automated system.\n\nThe Three Mile Island nuclear power plant accident is in part an example of the effect of people over-trusting a system. During that event, the control panel indicated that an important valve had operated as instructed, and the control room operators trusted the system was reporting accurately. Actually, the valve had not operated as instructed, and it became a key point in the failure that resulted in a serious mishap. (Interestingly, some will blame the operators, when in fact, under the mental load created by the evolving accident, they performed as an ergonomist would expect. The actual cause of the accident is a control system design error that provided incorrect information to the operators).\n\nAn example of mistrusting a system occurred at a medium security women’s prison in Oregon, USA, when a new surveillance system was installed. The alarm was triggered whenever it sensed motion in particular areas of the facility. During the first few weeks, the alarm was repeatedly triggered by everything from birds to leaves blowing in the wind. The guards became conditioned to the fact that it often triggered in error, and began to ignore it. Using this to her advantage, a prisoner climbed over the fences knowing that the alarm would go off, but that the guards would most likely ignore it long enough for her to escape. It worked. When this same mistrust effect occurs with something as important as a fire alarm, the results can be deadly.\n\nPhysical ergonomics issues, primarily in the workplace, dominate the public view and understanding of ergonomics. Fortunately, ergonomists are busy behind the scenes working to improve all human-machine interfaces, including the cognitive aspects. Unfortunately, many companies, engineers, regulators, and other decision makers fail to recognize the human factor in design, and many unnecessary errors, accidents, product failures and other business costs are the predictable result.",
      "Interaction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\"1 While the digital side of this statement is true, interaction design is also useful when creating physical (non-digital) products, exploring how a user might interact with it. Common topics of interaction design include design, human–computer interaction, and software development. While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior:1 Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what clearly marks IxD as a design field as opposed to a science or engineering field.\nGoal-oriented design (or Goal-Directed™ design) \"is concerned with satisfying the needs and desires of the users of a product or service.\"[1]:xviii\n\nAlan Cooper argues in The Inmates Are Running the Asylum that we need a new approach to solving interactive software-based problems.[9]:1 The problems with designing computer interfaces are fundamentally different from those that do not include software (e.g., hammers). Cooper introduces the concept of cognitive friction, whereby we anthropomorphize (treat as human) products that are complex enough that users cannot always understand how they behave. Computer interfaces often qualify.\nThe cognitive dimensions framework[13] provides a vocabulary to evaluate and modify design solutions. Cognitive dimensions offer a lightweight approach to analysis of a design quality, rather than an in-depth, detailed description. They provide a common vocabulary for discussing notation, user interface or programming language design.\n\nDimensions provide high-level descriptions of the interface and how the user interacts with it: examples include consistency, error-proneness, hard mental operations,viscosity and premature commitment. These concepts aid the creation of new designs from existing ones through design maneuvers that alter the design within a particular dimension.\nDesigners must be aware of elements that influence user emotional responses. For instance, products must convey positive emotions while avoiding negative ones.[14] Other important aspects include motivational, learning, creative, social and persuasive influences. One method that can help convey such aspects is for example, the use of dynamic icons, animations and sound to help communicate, creating a sense of interactivity. Interface aspects such as fonts, color palettes and graphical layouts can influence acceptance. Studies showed that affective aspects can affect perceptions of usability.[14]\n\nEmotion and pleasure theories exist to explain interface responses. These include Don Norman's emotional design model, Patrick Jordan's pleasure model and McCarthy and Wright's Technology as Experience framework.",
      "the differences between users and designers. \n the perceptual, cognitive and physical limitations of users. \n the role of interface design in the software engineering lifecycle. \n how to identify user tasks from informal requirements. \n how to use text, forms, menus and graphics in interactive systems. \n how to evaluate the utility and usability of human computer interfaces. \n HCI - Human Computer Interaction is concerned with studying and improving the many factors that influence the effectiveness and efficiency of computer use. It combines techniques from psychology, sociology, physiology, engineering, computer science, linguistics\n\n Perception involves the use of our senses to detect information. We have to make sure that people can see or hear displays if they are to use them. In some environments this causes huge problems. For instance, most aircraft produce over 15 audible warnings. It is relatively easy to confuse them under stress. Background noise can be over 100db. Although such observations may be worrying for the business traveller, what significance do they have for more general HCI design? We must ensure that signals are redundant. If we display critical information by small changes to the screen then many people will not detect the change. If you rely upon audio signals to inform users about critical events then you exclude deaf consumers. You may irritate users in large offices and baffle users who have the sound turned down.\n\n The study of cognition focuses upon two different phenomena: short and long term memory. \n\n Short term memory has a relatively low capacity. We'll find out exactly how much in a moment. It is fast, if we have something on our mind then we can talk about it almost instantly: what do the letter HCI stand for? If we have to trawl it up from our long term memory it may involve several moments thought: name the seven dwarfs? Short term memory also has a relatively short retention period. This is because we actually have to work to keep items in it. \n\n Long term memory, in contrast, has a relatively high capacity. As its name suggests it can store information over a much longer period of time. Access is much slower."
    ]
  },
  {
    "Natural_Language_Processing": [
      "The internal structure of discourse and the computational planning and generation of coherent multisentential paragraphs has been a topic of investigation at USC/ISI since the early 1980's. A theory of the interclausal relationships that govern discourse structure, called Rhetorical Structure Theory (RST), was developed in [Mann and Thompson 88] after extensive analysis of hundreds of texts of various genres. The analysis concluded that English text is coherent by virtue of so-called rhetorical relations that hold between clauses and blocks of clauses, and identified about 25 basic relations for English. These relations, such as Sequence, Purpose, and Elaboration are usually identified in English by key words or phrases (such as \"then\", \"in order to\", and \"e.g.\", respectively).",
      "Discourse concerns connected sentences. It is a study of chunks of language which are bigger than a single sentence. Dicourse language concerns inter-sentential links that is how the immediately preceding sentences affect the interpretation of the next sentence. Discourse knowledge is important for interpreting pronouns and temporal aspects of the information conveyed.",
      "the branch of linguistics and logic concerned with meaning. The two main areas are logical semantics, concerned with matters such as sense and reference and presupposition and implication, and lexical semantics, concerned with the analysis of word meanings and relations between them.",
      "In linguistics, semantic analysis is the process of relating syntactic structures, from the levels of phrases, clauses, sentences and paragraphs to the level of the writing as a whole, to their language-independent meanings.",
      "Pragmatics is a subfield of linguistics and semiotics that studies the ways in which context contributes to meaning. Pragmatics encompasses speech act theory, conversational implicature, talk in interaction and other approaches to language behavior in philosophy, sociology, linguistics and anthropology.",
      "the study of the forms of things, in particular. a particular form, shape, or structure.",
      "Sentence planning  involves aggregating content into sentence-sized units and the selecting the lexical and syntactic elements that are used in realizing each sentence.",
      "Natural Language Processing (NLP) refers to AI method of communicating with an intelligent systems using a natural language such as English.",
      "the system of contrastive relationships among the speech sounds that constitute the fundamental components of a language.     the study of phonological relationships within a language or between different languages.",
      "a morphological element considered in respect of its functional relations in a linguistic system.",
      "the branch of linguistics dealing with language in use and the contexts in which it is used, including such matters as deixis, the taking of turns in conversation, text organization, presupposition, and implicature.",
      "Word knowledge is nothing but everyday knowledge that all speakers share about the world. It includes the general knowledge about the structure of the world and what each language user must know about the other user’s beliefs and goals. This essential to make the language understanding much better.",
      "Natural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem.\n\nThe process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language",
      "Natural language generation (NLG) is the natural language processing task of generating natural language from a machine representation system such as a knowledge base or a logical form.",
      "Syntax is the level at which we study how words combine to form phrases, phrases combine to form clauses and clauses join to make sentences. Syntactic analysis concerns sentence formation. It deals with how words can be put together to form correct sentences. It also determines what structural role each word plays in the sentence and what phrases are subparts of what other phrases.",
      "In linguistics, realization is the process by which some kind of surface representation is derived from its underlying representation; that is, the way in which some abstract object of linguistic analysis comes to be produced in actual language. Phonemes are often said to be realized by speech sounds. The different sounds that can realize a particular phoneme are called its allophones.",
      "Referential ambiguity occurs when a word or phrase, in the context of a particular sentence, could. refer to two or more properties or things. It is sometimes clear from the context which meaning.",
      "Syntactic ambiguity, also called amphiboly or amphibology, is a situation where a sentence may be interpreted in more than one way due to ambiguous sentence structure",
      "Lexical analysis is the first phase of a compiler. It takes the modified source code from language preprocessors that are written in the form of sentences. The lexical analyzer breaks these syntaxes into a series of tokens, by removing any whitespace or comments in the source code.",
      "Lexical ambiguity is the presence of two or more possible meanings within a single word. Also called semantic ambiguity or homonymy. Compare to syntactic ambiguity. Lexical ambiguity is sometimes used deliberately to create puns and other types of word play.",
      "syntax analysis or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech)",
	"segmentbysegment decoder intelligent improve practical quality natural endtoend better methods often data adds full higher necessary proportion short selfattention algorithm studies agents process maintain baseline lengthpromotion conversation 23b evaluation road systems web challenging inject using generic generalpurpose introduce length work task lets approach combined toward human spectrum overall explicit diversity earlier models generation trained messages lengths sequencetosequence us called reranking language excellent building great generated strategy large neural coherence recently set rated longer promise datadriven interact yet mined humans produce stochastic glimpsemodel produces shown presents decoding datasets purely training new scaling dialogue propose backoff acceptable long beamsearch responses corpora method increases compared",
	"candidate mentioned evaluate f1 input among attack query disambiguation puts ranking joint full irregularities achieves implemented study word possible web discover towards 2 distance based search introduce smaph directly context piggybacking beddings approach spelling mentions engine relatedness entities sure ones mea makes larger stateoftheart outperforms algorithmic sense wikipedia occurring linkback us language alleviates inventory drawn queries via complete webquery benchmarks handle opendomain gerdaq websearch semantic margins features allows em novel show set secondorder effort errors supervised upon problem key specifically known first publish model idea linking erdsigir2014 several collective articles smaph2 step demonstrate built make confine prediction concepts paper approximate dataset benchmark also characterize annotation easier one pertinent entity lying noise really optimizing comparable crowdsourcing",
	"word labels submitted finer evaluation detecting performance handling identification excluding rather 790 classification paper system task1 shared relationships fmeasure found ambiguity describes neural better multitask relations also training task semantic experiments random suited classifier data performs available cnn show set true assume deterministic similarity could vectors cogalex2016 corpusbased binary identifying 760 analogy grained linear publicly convolutional 423 regression answers benefit frequency manytomany networks multiclass onetomany balancing test 881 embeddings try results task2",
	"structure clearly evaluate natural present multiple implications selection longshort incorporate snapshot google variety contextual exhibit sequential get abstractions sentences appropriately baseline subset word applications representing indicate performance systems paraphrase example three demonstrates wide network using recurrent utterance constitute context task lstm experiments benefit significant hierarchy tasks memory like infer models larger extension generation wikipedia specific results language meaning text 18 accuracy dialog neural relative improves features 21 documents clstm model topics improvements sentence sections paragraphs articles next two term prediction run paper question eg dataset recent nl news english levels words topic fragments answering nlp corpora abstraction completion",
	"develop approaches annotated word possible predicted evaluation accurate lossy across improve performance morphological las rankbased 06 rich resources natural suitable parser morphologically using multilingual without inherently projecting related tools discriminative learning weaklysupervised training given par experiments despite embeddingbased downstream propose often linguistic lack approach projection performs supervision show constraints wsabie lowresource languages token hmm partofspeech tagsets extends required infer scalable models model baseline best tagging method existing direct bitext type tags 11 pairs average processing taggers language",
	"appearing state improve augmentation problems require dala rich natural advances morphologically network modem leaming based many dataset recent large datasets neural work russian given analysis achieve corresponding however extending words approach including standard show art one suggest tasks problem nlp labeled sentiment models apply improvements method synonyms deep processing results language",
	"language aims descriptions machine structure natural computerize research using consists special characteristics system lexicon based collect segmentation thai english isarn approximately 8000 linguistic words features field transcription essential contains support according nlp phonemes proposed dharma important trie rules translation applying speech study subparts moreover constructed individual parts alphabets processing collection word",
	"penalizes machine mining pulmonary hospital algorithms readmissions framework components responsible notes natural naïve created legislation using disease medical passage auc times obstructive many efforts recent learning hospitals select selection analyze states focused primarily within added feature rates readmission computational account excessive chronic united data unstructured recently field component federal reaching institutions diseases fast offers list 0690 though danger proposed predict clinical bayes statistically best chisquared uses government incurring exist maintaining processing target language",
	"organizations regional structure ratio links natural participation facing taken learning calculated wielded people relations society various correctly proposed sentences addition containing representing machine using contained clue 67 influence onto analysis identify change consequences shift involved china material international 2013 became coverage annexation economic continues led extracted aiib countries makes game modelled established existing uses capitalize us language article map extract expressions visible infrastructure represented europe accuracy corpus financial included 60 sphere uk bank country supervised investment maps modelling power balances complex preferences events articles processing extraction may crimea development orders written considered intuitively asian creation moves presents irrelevant news new within including strategic defined furthermore institutions automates gain growth players developing challenged highly revealed initiative method strategies big",
	"lm word ensemble sets understanding perform billion advances using paper 237 corpus single recent large benchmark 300 neural new modeling task perplexity scale 410 513 improving improves central show significantly various one well shows models model stateoftheart chose best record techniques goal language"
    ]
  },
  {
    "Networking": [
      "A computer network is a set of computers connected together for the purpose of sharing resources. The most common resource shared today is connection to the Internet. Other shared resources can include a printer or a file server",
      "Shielded twisted pair is a special kind of copper telephone wiring used in some business installations. An outer covering or shield is added to the ordinary twisted pair telephone wires; the shield functions as a ground.",
      "In this method the node decides the routing without seeking information from other nodes. The sending node does not know about the status of a particular link. The disadvantage is that the packet may be send through a congested route resulting in a delay.",
      "In this type some central node in the network gets entire information about the network topology, about the traffic and about other nodes. This then transmits this information to the respective routers. The advantage of this is that only one node is required to keep the information. The disadvantage is that if the central node goes down the entire network is down, i.e. single point of failure.",
      "In this method a packet is sent by the node to one of its neighbours randomly. This algorithm is highly robust. When the network is highly interconnected, this algorithm has the property of making excellent use of alternative routes. It is usually implemented by sending the packet onto the least queued link",
      "Flooding adapts the technique in which every incoming packet is sent on every outgoing line except the one on which it arrived. One problem with this method is that packets may go in a loop. As a result of this a node may receive several copies of a particular packet which is undesirable.\nSequence Numbers: Every packet is given a sequence number. When a node receives the packet it sees its source address and sequence number. If the node finds that it has sent the same packet earlier then it will not transmit the packet and will just discard it. \nHop Count: Every packet has a hop count associated with it. This is decremented(or incremented) by one by each node which sees it. When the hop count becomes zero(or a maximum possible value) the packet is dropped.\nSpanning Tree: The packet is sent only on those links that lead to the destination by constructing a spanning tree routed at the source. This avoids loops in transmission but is possible only when all the intermediate nodes have knowledge of the network topology.",
      "A communications satellite is an artificial satellite that relays and amplifies radio telecommunications signals via a transponder; it creates a communication channel between a source transmitter and a receiver at different locations on Earth.",
      "Adaptive routing, also called dynamic routing, is a process for determining the optimal path a data packet should follow through a network to arrive at a specific destination. Adaptive routing can be compared to a commuter taking a different route to work after learning that traffic on his usual route is backed up.\nAdaptive routing uses algorithms and routing protocols that read and respond to changes in network topology. In addition to Open Shortest Path First (OSPF), other routing protocols that facilitate adaptive routing include Intermediate System to Intermediate System (IS-IS) protocol for large networks such as the internet and routing information protocol (RIP) for short-distance transport.\nSimilar to GPS, which uses information about road conditions to redirect drivers, adaptive routing uses information about network congestion and node availability to direct packets. When a packet arrives at a node, the node uses information shared among network routers to calculate which path is most suitable. If the default path is congested, the packet is sent along a different path and the information is shared among network routers.\nThe purpose of adaptive routing is to help prevent packet delivery failure, improve network performance and relieve network congestion. Adaptive routing can cause nodes to become overloaded, however, due to the complex processing decisions they make. Because routers share information about the network topology, adaptive routing can be less secure than non-adaptive routing processes and require more bandwidth.\nAdaptive routing is an alternative to non-adaptive, static routing, which requires network engineers to manually configure fixed routes for packets. When a node is unavailable in a static routing environment, the packet must either wait for the node to become available again or the packet will fail to be delivered. Static routing is often used for simple, closed network topologies, while adaptive routing is often used for open, complex network topologies.",
      "Physical layer is the lowest layer of all. It is responsible for sending bits from one computer to another. This layer is not concerned with the meaning of the bits and deals with the physical connection to the network and with transmission and reception of signals.\n\nThis layer defines electrical and physical details represented as 0 or a 1. How many pins a network will contain, when the data can be transmitted or not and how the data would be synchronized.\n\n    Representation of Bits: Data in this layer consists of stream of bits. The bits must be encoded into signals for transmission. It defines the type of encoding i.e. how 0’s and 1’s are changed to signal.\n    Data Rate: This layer defines the rate of transmission which is the number of bits per second.\n    Synchronization: It deals with the synchronization of the transmitter and receiver. The sender and receiver are synchronized at bit level.\n    Interface: The physical layer defines the transmission interface between devices and transmission medium.\n    Line Configuration: This layer connects devices with the medium: Point to Point configuration and Multipoint configuration.\n    Topologies: Devices must be connected using the following topologies: Mesh, Star, Ring and Bus.\n    Transmission Modes: Physical Layer defines the direction of transmission between two devices: Simplex, Half Duplex, Full Duplex.\n    Deals with baseband and broadband transmission.",
      "Unshielded twisted pair is the most common kind of copper telephone wiring. Twisted pair is the ordinary copper wire that connects home and many business computers to the telephone company. To reduce crosstalk or electromagnetic induction between pairs of wires, two insulated copper wires are twisted around each other. Each signal on twisted pair requires both wires. Since some telephone sets or desktop locations require multiple connections, twisted pair is sometimes installed in two or more pairs, all within a single cable.",
      "The data link layer or layer 2 is the second layer of the seven-layer OSI model of computer networking. This layer is the protocol layer that transfers data between adjacent network nodes in a wide area network (WAN) or between nodes on the same local area network (LAN) segment.[1] The data link layer provides the functional and procedural means to transfer data between network entities and might provide the means to detect and possibly correct errors that may occur in the physical layer.\nThe data link layer is concerned with local delivery of frames between devices on the same LAN. Data-link frames, as these protocol data units are called, do not cross the boundaries of a local network. Inter-network routing and global addressing are higher-layer functions, allowing data-link protocols to focus on local delivery, addressing, and media arbitration. This way, the data link layer is analogous to a neighborhood traffic cop; it endeavors to arbitrate between parties contending for access to a medium, without concern for their ultimate destination. When devices attempt to use a medium simultaneously, frame collisions occur. Data-link protocols specify how devices detect and recover from such collisions, and may provide mechanisms to reduce or prevent them.\nExamples of data link protocols are Ethernet for local area networks (multi-node), the Point-to-Point Protocol (PPP), HDLC and ADCCP for point-to-point (dual-node) connections. In the Internet Protocol Suite (TCP/IP), the data link layer functionality is contained within the link layer, the lowest layer of the descriptive model.\nThe uppermost sublayer, LLC, multiplexes protocols running atop the data link layer, and optionally provides flow control, acknowledgment, and error notification. The LLC provides addressing and control of the data link. It specifies which mechanisms are to be used for addressing stations over the transmission medium and for controlling the data exchanged between the originator and recipient machines.\nMAC may refer to the sublayer that determines who is allowed to access the media at any one time (e.g. CSMA/CD). Other times it refers to a frame structure delivered based on MAC addresses inside.",
      "These algorithms do not base their routing decisions on measurements and estimates of the current traffic and topology. Instead the route to be taken in going from one node to the other is computed in advance, off-line, and downloaded to the routers when the network is booted. This is also known as static routing.",
      "In this type of routing, interconnected networks are viewed as a single network, where bridges, routers and gateways are just additional nodes.\n\n    Every node keeps information about every other node in the network\n    In case of adaptive routing, the routing calculations are done and updated for all the nodes. \n\nThe above two are also the disadvantages of non-hierarchical routing, since the table sizes and the routing calculations become too large as the networks get bigger. So this type of routing is feasible only for small networks.",
      "Network infrastructure is the hardware and software resources of an entire network that enable network connectivity, communication, operations and management of an enterprise network. It provides the communication path and services between users, processes, applications, services and external networks/the internet.\nNetwork infrastructure is typically part of the IT infrastructure found in most enterprise IT environments. The entire network infrastructure is interconnected, and can be used for internal communications, external communications or both.\nNetworking Hardware:\n\n    Routers\n    Switches\n    LAN cards\n    Wireless routers\n    Cables\nNetworking Software:\n\n    Network operations and management\n    Operating systems\n    Firewall\n    Network security applications\nNetwork Services:\n\n    T-1 Line\n    DSL\n    Satellite\n    Wireless protocols\n    IP addressing",
      "Broadcasting networks. The broadcast type of radio network is a network system which distributes programming to multiple stations simultaneously, or slightly delayed, for the purpose of extending total coverage beyond the limits of a single broadcast signal.",
      "The term bandwidth has a number of technical meanings but since the popularization of the internet, it has generally referred to the volume of information per unit of time that a transmission medium (like an internet connection) can handle.\n\nAn internet connection with a larger bandwidth can move a set amount of data (say, a video file) much faster than an internet connection with a lower bandwidth.\n\nBandwidth is typically expressed in bits per second, like 60 Mbps or 60 Mb/s to explain a data transfer rate of 60 million bits (megabits) every second.\nas the bandwidth increases so does the amount of data that can flow through in a given amount of time, just like as the diameter of the pipe increases, so does the amount of water that can flow through during a period of time.",
      "Its main aim is to establish, maintain and synchronize the interaction between communicating systems. Session layer manages and synchronize the conversation between two different applications. Transfer of data from one destination to another session layer streams of data are marked and are resynchronized properly, so that the ends of the messages are not cut prematurely and data loss is avoided.\n    Dialog Control : This layer allows two systems to start communication with each other in half-duplex or full-duplex.\n    Synchronization : This layer allows a process to add checkpoints which are considered as synchronization points into stream of data. Example: If a system is sending a file of 800 pages, adding checkpoints after every 50 pages is recommended. This ensures that 50 page unit is successfully received and acknowledged. This is beneficial at the time of crash as if a crash happens at page number 110; there is no need to retransmit 1 to100 pages.",
      "In this type of routing, certain restrictions are put on the type of packets accepted and sent. e.g.. The IIT- K router may decide to handle traffic pertaining to its departments only, and reject packets from other routes. This kind of routing is used for links with very low capacity or for security purposes.",
      "In computer science, the Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles).[1][2] A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of vertices. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. Versions of the algorithm can also be used for finding the transitive closure of a relation R {\\displaystyle R} R, or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.\nSuppose the shortest path between i and j using nodes 1,2,...n is known. Now, if node n+1 is allowed to be an intermediate node, then the shortest path under new conditions either passes through node n+1 or it doesn't. If it does not pass through the node n+1, then Di,j[n+1] is same as Di,j[n] .  Else, we find the cost of the new route, which is obtained from the sum,  Di,n+1[n] + Dn+1,j[n]. So we take the minimum of these two cases at each step.  After adding all the nodes to the set of intermediate nodes, we obtain the shortest paths between all pairs of nodes together.  The complexity of  Floyd-Warshall algorithm is O ( N3 ).",
      "A multipoint connection is a link between three or more devices. It is also known as Multi-drop configuration. The networks havjng multipoint configuration are called Broadcast Networks. In broadcast network, a message or a packet sent by any machine is received by all other machines in a network. The packet contains address field that specifies the receiver. Upon receiving a packet, every machine checks the address field of the packet. If the transmitted packet is for that particular machine, it processes it; otherwise it just ignores the packet.  Broadcast network provides the provision for broadcasting & multicasting. Broadcasting is the process in which a single packet is received and processed by all the machines in the network. It is made possible by using a special code in the address field of the packet. When a packet is sent to a subset of the machines i.e. only to few machines in the network it is known as multicasting. Historically, multipoint connections were used to attach central CPs to distributed dumb terminals. In today's LAN environments, multipoint connections link many network devices in various configurations.",
      "When a packet comes to a node, it tries to get rid of it as fast as it can, by putting it on the shortest output queue without regard to where that link leads. A variation of this algorithm is to combine static routing with the hot potato algorithm. When a packet arrives, the routing algorithm takes into account both the static weights of the links and the queue lengths.",
      "UDP -- like its cousin the Transmission Control Protocol (TCP) -- sits directly on top of the base Internet Protocol (IP). In general, UDP implements a fairly \"lightweight\" layer above the Internet Protocol. It seems at first site that similar service is provided by both UDP and IP, namely transfer of data.But we need UDP for multiplexing/demultiplexing of addresses.\n\nUDP's main purpose is to abstract network traffic in the form of datagrams. A datagram comprises one single \"unit\" of binary data; the first eight (8) bytes of a datagram contain the header information and the remaining bytes contain the data itself. \n\nUDP port numbers allow different applications to maintain their own \"channels\" for data; both UDP and TCP use this mechanism to support multiple applications sending and receiving data concurrently. The sending application (that could be a client or a server) sends UDP datagrams through the source port, and the recipient of the packet accepts this datagram through the destination port. Some applications use static port numbers that are reserved for or registered to the application. Other applications use dynamic (unregistered) port numbers. Because the UDP port headers are two bytes long, valid port numbers range from 0 to 65535; by convention, values above 49151 represent dynamic ports.\n\nThe datagram size is a simple count of the number of bytes contained in the header and data sections . Because the header length is a fixed size, this field essentially refers to the length of the variable-sized data portion (sometimes called the payload). The maximum size of a datagram varies depending on the operating environment. With a two-byte size field, the theoretical maximum size is 65535 bytes. However, some implementations of UDP restrict the datagram to a smaller number -- sometimes as low as 8192 bytes.\n\nUDP checksums work as a safety feature. The checksum value represents an encoding of the datagram data that is calculated first by the sender and later by the receiver. Should an individual datagram be tampered with (due to a hacker) or get corrupted during transmission (due to line noise, for example), the calculations of the sender and receiver will not match, and the UDP protocol will detect this error. The algorithm is not fool-proof, but it is effective in many cases. In UDP, check summing is optional -- turning it off squeezes a little extra performance from the system -- as opposed to TCP where checksums are mandatory. It should be remembered  that check summing is optional only for the sender, not the receiver. If the sender has used checksum then it is mandatory for the receiver to do so.\n\nUsage of the Checksum in UDP is optional. In case the sender does not use it, it sets the checksum field to all 0's. Now if the sender computes the checksum then the recipient must also compute the checksum an set the field accordingly. If the checksum is calculated and turns out to be all 1's then the sender sends all 1's instead of all 0's. This is since in the algorithm for checksum computation used by UDP, a checksum of all 1's if equivalent to a checksum of all 0's. Now the checksum field is unambiguous for the recipient, if it is all 0's then checksum has not been used, in any other case the checksum has to be computed.",
      "Given a graph and a source vertex src in graph, find shortest paths from src to all vertices in the given graph. The graph may contain negative weight edges.\nWe have discussed Dijkstra’s algorithm for this problem. Dijksra’s algorithm is a Greedy algorithm and time complexity is O(VLogV) (with the use of Fibonacci heap). Dijkstra doesn’t work for Graphs with negative weight edges, Bellman-Ford works for such graphs. Bellman-Ford is also simpler than Dijkstra and suites well for distributed systems. But time complexity of Bellman-Ford is O(VE), which is more than Dijkstra.\nLike other Dynamic Programming Problems, the algorithm calculate shortest paths in bottom-up manner. It first calculates the shortest distances for the shortest paths which have at-most one edge in the path. Then, it calculates shortest paths with at-nost 2 edges, and so on. After the ith iteration of outer loop, the shortest paths with at most i edges are calculated. There can be maximum |V| – 1 edges in any simple path, that is why the outer loop runs |v| – 1 times. The idea is, assuming that there is no negative weight cycle, if we have calculated shortest paths with at most i edges, then an iteration over all edges guarantees to give shortest path with at-most (i+1) edges\nFor zero hops,  the minimum length path has length of infinity, for every node.  For one hop the shortest-path length associated with a node is equal to the length of the edge between  that node and node 1. Hereafter, we increment the number of hops allowed, (from h to h+1 ) and find out whether a shorter path exists through each of the  other nodes.  If  it exists, say through node 'j',  then its length must be the sum of the lengths between these two nodes (i.e.  di,j ) and the shortest path between j and 1 obtainable in upto h paths. If such a path doesn't exist, then the path length remains the same. The algorithm is guaranteed to terminate, since there are utmost N nodes, and so N-1 paths. It has time complexity of O ( N3 ) .\nWe look at the distributed version which works on the premise that the information about far away nodes can be had from the adjoining links.\n\nThe algorithm works as follows.\n\n        Compute the link costs from the starting node to every directly connected node .\n        Select the cheapest links for every node (if there is more than one) .\n        For every directly connected node, compute the link costs for all these nodes.\n        Select the cheapest route for any node .\n        Repeat until all nodes have been processed.\n\nEvery node should have the information about it's immediate neighbors and over a period of time we will have information about other nodes. Within n units of time , where n is the diameter of the network, every node will have the complete information. We do not need to be synchronized i.e. do not need to exchange information at the same time.\n\nRouting algorithms based on Dijkstra's algorithm are called Link State Algorithms. Distance Vector Protocols are based on distributed Bellman's algorithm. In the former we are sending little information to many nodes while in the latter we send huge information to few neighbors.",
      "A wireless LAN (or WLAN, for wireless local area network, sometimes referred to as LAWN, for local area wireless network) is one in which a mobile user can connect to a local area network (LAN) through a wireless (radio) connection. The IEEE 802.11 group of standards specify the technologies for wireless LANs. 802.11 standards use the Ethernet protocol and CSMA/CA (carrier sense multiple access with collision avoidance) for path sharing and include an encryption method, the Wired Equivalent Privacy algorithm.\nHigh-bandwidth allocation for wireless will make possible a relatively low-cost wiring of classrooms in the United States. A similar frequency allocation has been made in Europe. Hospitals and businesses are also expected to install wireless LAN systems where existing LANs are not already in place.",
      "In this the node receives information from its neighbouring nodes and then takes the decision about which way to send the packet. The disadvantage is that if in between the the interval it receives information and sends the paket something changes then the packet may be delayed.",
      "In many networks however there are several paths between pairs of nodes that are almost equally good. Sometimes in order to improve the performance multiple paths between single pair of nodes are used. This technique is called multipath routing or bifurcated routing. In this each node maintains a table with one row for each possible destination node. A row gives the best, second best, third best, etc outgoing line for that destination, together with a relative weight. Before forwarding a packet, the node generates a random number and then chooses among the alternatives, using the weights as probabilities. The tables are worked out manually and loaded into the nodes before the network is brought up and not changed thereafter.",
      "Fiber optics, or optical fiber, refers to the medium and the technology associated with the transmission of information as light pulses along a glass or plastic strand or fiber. A fiber optic cable can contain a varying number of these glass fibers -- from a few up to a couple hundred. Surrounding the glass fiber core is another glass layer called cladding. A layer known as a buffer tube protects the cladding, and a jacket layer acts as the final protective layer for the individual strand. Fiber optics transmit data in the form of light particles -- or photons -- that pulse through a fiber optic cable. The glass fiber core and the cladding each have a different refractive index that bends incoming light at a certain angle. When light signals are sent through the fiber optic cable, they reflect off the core and cladding in a series of zig-zag bounces, adhering to a process called total internal reflection. The light signals do not travel at the speed of light because of the denser glass layers, instead traveling about 30% slower than the speed of light. To renew, or boost, the signal throughout its journey, fiber optics transmission sometimes requires repeaters at distant intervals to regenerate the optical signal by converting it to an electrical signal, processing that electrical signal and retransmitting the optical signal. Multimode fiber and single-mode fiber are the two primary types of fiber optic cable. Single-mode fiber is used for longer distances due to the smaller diameter of the glass fiber core, which lessens the possibility for attenuation -- the reduction in signal strength. The smaller opening isolates the light into a single beam, which offers a more direct route and allows the signal to travel a longer distance. Single-mode fiber also has a considerably higher bandwidth than multimode fiber. The light source used for single-mode fiber is typically a laser. Single-mode fiber is usually more expensive because it requires precise calculations to produce the laser light in a smaller opening. Multimode fiber is used for shorter distances because the larger core opening allows light signals to bounce and reflect more along the way. The larger diameter permits multiple light pulses to be sent through the cable at one time, which results in more data transmission. This also means that there is more possibility for signal loss, reduction or interference, however. Multimode fiber optics typically use an LED to create the light pulse.\n\nWhile copper wire cables were the traditional choice for telecommunication, networking and cable connections for years, fiber optics has become a common alternative. Most telephone company long-distance lines are now made of fiber optic cables. Optical fiber carries more information than conventional copper wire, due to its higher bandwidth and faster speeds. Because glass does not conduct electricity, fiber optics is not subject to electromagnetic interference and signal losses are minimized.\n\nIn addition, fiber optic cables can be submerged in water and are used in more at-risk environments like undersea cable. Fiber optic cables are also stronger, thinner and lighter than copper wire cables and do not need to be maintained or replaced as frequently. Copper wire is often cheaper than fiber optics, however, and is already installed in many areas where fiber optic cable hasn't been deployed. Glass fiber also requires more protection within an outer cable than copper, and installing new cabling is labor-intensive, as it typically is with any cable installation.",
      "Wireless transmission media send communications signals by using broadcast radio, cellular radio, microwaves, satellites, and infrared signals. Wireless transmission media are used when it is inconvenient, impractical, or impossible to install wires and cables.",
      "The Internet Protocol (IP) is the method or protocol by which data is sent from one computer to another on the Internet. Each computer (known as a host) on the Internet has at least one IP address that uniquely identifies it from all other computers on the Internet.\n\nInternet Protocol\nPosted by: Margaret Rouse\nWhatIs.com\n\nSponsored News\n\n    Six Steps to Modernizing Your B2B Architecture\n    –IBM\n    Relieving File Transfer Headaches in Your B2B Integration Efforts\n    –IBM\n    See More\n\nVendor Resources\n\n    CW Buyer's Guide: IPV6 Migration\n    –ComputerWeekly.com\n\nThe Internet Protocol (IP) is the method or protocol by which data is sent from one computer to another on the Internet. Each computer (known as a host) on the Internet has at least one IP address that uniquely identifies it from all other computers on the Internet. \nDownload this free guide\n2017 Trends in Unified Communications & Collaboration\n\nIrwin Lazar of Nemertes Research predicts the 6 collaboration trends we will see this year, and then we expand on some of those in this exclusive e-guide. Don’t miss out on key UCC opportunities – become a member now and get this complimentary download.\n\nWhen you send or receive data (for example, an e-mail note or a Web page), the message gets divided into little chunks called packets. Each of these packets contains both the sender's Internet address and the receiver's address. Any packet is sent first to a gateway computer that understands a small part of the Internet. The gateway computer reads the destination address and forwards the packet to an adjacent gateway that in turn reads the destination address and so forth across the Internet until one gateway recognizes the packet as belonging to a computer within its immediate neighborhood or domain. That gateway then forwards the packet directly to the computer whose address is specified.\n\nBecause a message is divided into a number of packets, each packet can, if necessary, be sent by a different route across the Internet. Packets can arrive in a different order than the order they were sent in. The Internet Protocol just delivers them. It's up to another protocol, the Transmission Control Protocol (TCP) to put them back in the right order.\nIP is a connectionless protocol, which means that there is no continuing connection between the end points that are communicating. Each packet that travels through the Internet is treated as an independent unit of data without any relation to any other unit of data. (The reason the packets do get put in the right order is because of TCP, the connection-oriented protocol that keeps track of the packet sequence in a message.) In the Open Systems Interconnection (OSI) communication model, IP is in layer 3, the Networking Layer.\n\nThe most widely used version of IP today is Internet Protocol Version 4 (IPv4). However, IP Version 6 (IPv6) is also beginning to be supported. IPv6 provides for much longer addresses and therefore for the possibility of many more Internet users. IPv6 includes the capabilities of IPv4 and any server that can support IPv6 packets can also support IPv4 packets.",
      "In this method the routing tables at each node gets modified by information from the incoming packets. One way to implement backward learning is to include the identity of the source node in each packet, together with a hop counter that is incremented on each hop. When a node receives a packet in a particular line, it notes down the number of hops it has taken to reach it from the source node. If the previous value of hop count stored in the node is better than the current one then nothing is done but if the current value is better then the value is updated for future use. The problem with this is that when the best route goes down then it cannot recall the second best route to a particular node. Hence all the nodes have to forget the stored informations periodically and start all over again",
      "Half-duplex data transmission means that data can be transmitted in both directions on a signal carrier, but not at the same time. For example, on a local area network using a technology that has half-duplex transmission, one workstation can send data on the line and then immediately receive data on the line from the same direction in which data was just transmitted. Like full-duplex transmission, half-duplex transmission implies a bidirectional line (one that can carry data in both directions).",
      "An ad-hoc network is a local area network (LAN) that is built spontaneously as devices connect. Instead of relying on a base station to coordinate the flow of messages to each node in the network, the individual network nodes forward packets to and from each other. In Latin, ad hoc literally means \"for this,\" meaning \"for this special purpose\" and also, by extension, improvised or impromptu.\nIn the Windows operating system, ad-hoc is a communication mode (setting) that allows computers to directly communicate with each other without a router.\nAn ad hoc network is a type of temporary computer-to-computer connection. In ad hoc mode, you can set up a wireless connection directly to another computer without having to connect to a Wi-Fi access point or router.\nAd hoc networks are useful when you need to share files or other data directly with another computer but don't have access to a Wi-Fi network (e.g., if you're a visiting client or partner to a different office and aren't able to get on their network).\n\nYou can also use Internet connection sharing with ad hoc mode to share your computer's Internet connection with other users.\n\nAnother feature of ad hoc networks is that more than one laptop can be connected to the ad hoc network, as long as all of the adapter cards are configured for ad hoc mode and connect to the same SSID (service state identifier). The computers need to be within 100 meters of each other.\n\nAlso, if you were the person who set up the ad hoc network, when you disconnect from the network, all of the other users will also be disconnected. An ad hoc network will also be deleted once everyone on it disconnects -- which can be good or bad, depending on your view; it's truly a spontaneous network.",
      "Twisted pair is the ordinary copper wire that connects home and many business computers to the telephone company. To reduce crosstalk or electromagnetic induction between pairs of wires, two insulated copper wires are twisted around each other. Each connection on twisted pair requires both wires. Since some telephone sets or desktop locations require multiple connections, twisted pair is sometimes installed in two or more pairs, all within a single cable. For some business locations, twisted pair is enclosed in a shield that functions as a ground. This is known as shielded twisted pair (STP). Ordinary wire to the home is unshielded twisted pair (UTP). Twisted pair is now frequently installed with two pairs to the home, with the extra pair making it possible for you to add another line (perhaps for modem use) when you need it.\n\nTwisted pair comes with each pair uniquely color coded when it is packaged in multiple pairs. Different uses such as analog, digital, and Ethernet require different pair multiples.\n\nAlthough twisted pair is often associated with home use, a higher grade of twisted pair is often used for horizontal wiring in LAN installations because it is less expensive than coaxial cable.\n\nThe wire you buy at a local hardware store for extensions from your phone or computer modem to a wall jack is not twisted pair. It is a side-by-side wire known as silver satin. The wall jack can have as many five kinds of hole arrangements or pinouts, depending on the kinds of wire the installation expects will be plugged in (for example, digital, analog, or LAN) . (That's why you may sometimes find when you carry your notebook computer to another location that the wall jack connections won't match your plug.)",
      "Coaxial cable is the kind of copper cable used by cable TV companies between the community antenna and user homes and businesses.Coaxial cable is called \"coaxial\" because it includes one physical channel that carries the signal surrounded (after a layer of insulation) by another concentric physical channel, both running along the same axis. The outer channel serves as a ground. Many of these cables or pairs of coaxial tubes can be placed in a single outer sheathing and, with repeaters, can carry information for a great distance.Coaxial cable is sometimes used by telephone companies from their central office to the telephone poles near users. It is also widely installed for use in business and corporation Ethernet and other types of local area network. Depending upon the carrier's technology and other factors, twisted pair copper wire and optical fiber may be used instead of coaxial cable. Coaxial cable was invented in 1880 by English engineer and mathematician Oliver Heaviside, who patented the invention and design that same year. AT&T established its first cross-continental coaxial transmission system in 1940. A coaxial cable, sometimes called a coax cable, provides an interference-free transmission path for high-frequency electrical signals. A key to coaxial cable design is tight control of cable dimensions and materials. Together, they ensure that the characteristic impedance of the cable takes on a fixed value. High-frequency signals are partially reflected at impedance mismatches, causing errors. Characteristic impedance is sensitive to signal frequency. Above 1 GHz, the cable maker must use a dielectric that doesn't attenuate the signal too much, or change the characteristic impedance in a way that creates signal reflections. Vendor specs should provide guidance.\nSimple coax cables carry only one connection. There are variations, such as twinax, that place multiple connectors at the center and take advantage of transmission line properties to pass the signal. Applications sometimes bundle multiple separate coax cables inside a protective plastic outer layer applied to prevent mechanical damage.\n\nConnectors for coax range from simple single connectors used on cable TV systems to complicated combinations of multiple thin coax links, mixed with power and other signal connections, housed in semi-custom bodies. These are commonly found in military electronics and avionics.\n\nMechanical stiffness can vary tremendously, depending on the internal construction and intended use of the coaxial cabling. For example, high-power cables are often made with thick insulation and are very stiff.\n\nSome cables are deliberately made with thick center wires, resulting in skin-effect resistance. It results from high-frequency signals traveling on the surface of the conductor, not throughout. If the center conductor is larger, it results in a stiff cable with low loss per meter.\n\nCoaxial cable was invented and patented in 1880 by English engineer and mathematician Oliver Heaviside. AT&T established its first cross-continental coaxial transmission system in 1940.",
      "The frequencies used are in the low-gigahertz range, which limits all communications to line-of-sight. You probably have seen terrestrial microwave equipment in the form of telephone relay towers, which are placed every few miles to relay telephone signals crosscountry.",
      "Delta routing is a hybrid of the centralized and isolated routing algorithms. Here each node computes the cost of each line (i.e some functions of the delay, queue length, utilization, bandwidth etc) and periodically sends a packet to the central node giving it these values which then computes the k best paths from node i to node j. Let Cij1 be the cost of the best i-j path, Cij2 the cost of the next best path and so on.If Cijn - Cij1 < delta, (Cijn - cost of n'th best i-j path, delta is some constant) then path n is regarded equivalent to the best i-j path since their cost differ by so little. When delta -> 0 this algorithm becomes centralized routing and when delta -> infinity all the paths become equivalent.",
      "A point-to-point connection is a direct link between two devices such as a computer and a printer. It uses dedicated link between the devices. The entire capacity of the link is used for the transmission between those two devices. Most of today's point-to-point connections are associated with modems and PSTN (Public Switched Telephone Network) communications. In point to point networks, there exist many connections between individual pairs of machines. Internet service providers (ISPs) have used PPP for customer dial-up access to the Internet, since IP packets cannot be transmitted over a modem line on their own, without some data link protocol. Two encapsulated forms of PPP, Point-to-Point Protocol over Ethernet (PPPoE) and Point-to-Point Protocol over ATM (PPPoA), are used most commonly by Internet Service Providers (ISPs) to establish a Digital Subscriber Line (DSL) Internet service connection with customers.  PPP is commonly used as a data link layer protocol for connection over synchronous and asynchronous circuits, where it has largely superseded the older Serial Line Internet Protocol (SLIP) and telephone company mandated standards (such as Link Access Protocol, Balanced (LAPB) in the X.25 protocol suite). PPP was designed to work with numerous network layer protocols, including Internet Protocol (IP), TRILL, Novell's Internetwork Packet Exchange (IPX), NBF and AppleTalk.",
      "In the Open Systems Interconnection (OSI) communications model, the Transport layer ensures the reliable arrival of messages and provides error checking mechanisms and data flow controls. The Transport layer provides services for both \"connection-mode\" transmissions and for \"connectionless-mode\" transmissions. For connection-mode transmissions, a transmission may be sent or arrive in the form of packets that need to be reconstructed into a complete message at the other end.\nThe main aim of transport layer is to be delivered the entire message from source to destination. Transport layer ensures whole message arrives intact and in order, ensuring both error control and flow control at the source to destination level. It decides if data transmission should be on parallel path or single path\n\nTransport layer breaks the message (data) into small units so that they are handled more efficiently by the network layer and ensures that message arrives in order by checking error and flow control.\n\n    Service Point Addressing : Transport Layer header includes service point address which is port address. This layer gets the message to the correct process on the computer unlike Network Layer, which gets each packet to the correct computer.\n    Segmentation and Reassembling : A message is divided into segments; each segment contains sequence number, which enables this layer in reassembling the message. Message is reassembled correctly upon arrival at the destination and replaces packets which were lost in transmission.\n    Connection Control : It includes 2 types :\n        Connectionless Transport Layer : Each segment is considered as an independent packet and delivered to the transport layer at the destination machine.\n        Connection Oriented Transport Layer : Before delivering packets, connection is made with transport layer at the destination machine.\n    Flow Control : In this layer, flow control is performed end to end.\n    Error Control : Error Control is performed end to end in this layer to ensure that the complete message arrives at the receiving transport layer without any error. Error Correction is done through retransmission.",
      "In this method of routing the nodes are divided into regions based on hierarchy. A particular node can communicate with nodes at the same hierarchial level or the nodes at a lower level and directly under it. Here, the path from any source to a destination is fixed and is exactly one if the heirarchy is a tree.\nThis is essentially a 'Divide and Conquer' strategy. The network is divided into different regions and a router for a particular region knows only about its own domain and other routers. Thus, the network is viewed at two levels:\n\n    The Sub-network level, where each node in a region has information about its peers in the same region and about the region's interface with other regions. Different regions may have different 'local' routing algorithms. Each local algorithm handles the traffic between nodes of the same region and also directs the outgoing packets to the appropriate interface.\n    The Network Level, where each region is considered as a single node connected to its interface nodes. The routing algorithms at this level handle the routing of packets between two interface nodes, and is isolated from intra-regional transfer. \n\nNetworks can be organized in hierarchies of many levels; e.g. local networks of a city at one level, the cities of a country at a level above it, and finally the network of all nations.\n\nIn Hierarchical routing, the interfaces need to store information about:\n\n    All nodes in its region which are at one level below it.\n    Its peer interfaces.\n    At least one interface at a level above it, for outgoing packages. \n\nAdvantages of Hierarchical Routing :\n\n    Smaller sizes of routing tables.\n    Substantially lesser calculations and updates of routing tables. \n\nDisadvantage :\n\n    Once the hierarchy is imposed on the network, it is followed and possibility of direct paths is ignored. This may lead to sub optimal routing.",
      "Simplex Channel: We know that the message source is the transmitter, and the destination is the receiver. A channel whose direction of transmission is unchanging is called as a simplex channel. In other words, a type of data transmission, which is taken place only in one direction (from one antenna to the other only), for example, a radio station is a simplex channel because it always transmits the signal to its listeners and never allows them to transmit back. A television set up can also be considered as the simplex type. The advantage of simplex mode of transmission is, since the data can be transmitted only in one direction, the entire band width can be used.",
      "Here, the central question dealt with is 'How to determine the optimal path for routing ?' Various algorithms are used to determine the optimal routes with respect to some predetermined criteria. A network is represented as a graph, with its terminals as nodes and the links as edges. A 'length' is associated with each edge, which represents the cost of using the link for transmission. Lower the cost, more suitable is the link. The cost is determined depending upon the criteria to be optimized. Some of the important ways of determining the cost are:\n\n    Minimum number of hops: If each link is given a unit cost, the shortest path is the one with minimum number of hops. Such a route is easily obtained by a breadth first search method. This is easy to implement but ignores load, link capacity etc.\n    Transmission and Propagation Delays: If the cost is fixed as a function of transmission and propagation delays, it will reflect the link capacities and the geographical distances. However these costs are essentially static and do not consider the varying load conditions.\n    Queuing Delays: If the cost of a link is determined through its queuing delays, it takes care of the varying load conditions, but not of the propagation delays. \n\nIdeally, the cost parameter should consider all the above mentioned factors, and it should be updated periodically to reflect the changes in the loading conditions. However, if the routes are changed according to the load, the load changes again. This feedback effect between routing and load can lead to undesirable oscillations and sudden swings.",
      "This protocol discusses a mechanism that gateways and hosts use to communicate control or error information.The Internet protocol provides unreliable,connectionless datagram service,and that a datagram travels from gateway to gateway until it reaches one that can deliver it directly to its final destination. If a gateway cannot route or deliver a datagram,or if the gateway detects an unusual condition, like network congestion, that affects its ability to forward the datagram, it needs to instruct the original source to take action to avoid or correct the problem. The Internet Control Message Protocol allows gateways to send error or control messages to other gateways or hosts;ICMP provides communication between the Internet Protocol software on one machine and the Internet Protocol software on another. This is a special purpose message mechanism added by the designers to the TCP/IP protocols. This is to allow gateways in an internet to report errors or provide information about unexpecter circumstances. The IP protocol itself contains nothing to help the sender test connectivity or learn about failures.\nICMP only reports error conditions to the original source; the source must relate errors to individual application programs and take action to correct problems. It provides a way for gateway to report the error It does not fully specify the action to be taken for each possible error. ICMP is restricted to communicate with the original source but not intermediate sources. \nICMP messages travel across the internet in the data portion of an IP datagram,which itself travels across the internet in the data portion of an IP datagram,which itself travels across each physical network in the data portion of a frame.Datagrams carryin ICMP messages are routed exactly like datagrams carrying information for users;there is no additional reliability or priority.An exception is made to the error handling procedures if an IP datagram carrying an ICMP messages are not generated for errors that result from datagrams carrying ICMP error messages.\nIt has three fields;an 8-bit integer message TYPE field that identifies the message,an 8-bit CODE field that provides further information about the message type,and a 16-bit CHECKSUM field(ICMP uses the same additive checksum algorithm as IP,but the ICMP checksum only covers the ICMP message).In addition , ICMP messages that report errors always include the header and first 64 data bits of the datagram causing the problem. The ICMP TYPE field defines the meaning of the message as well as its format.",
      "The primary goal of this layer is to take care of the syntax and semantics of the information exchanged between two communicating systems. Presentation layer takes care that the data is sent in such a way that the receiver will understand the information (data) and will be able to use the data. Languages (syntax) can be different of the two communicating systems. Under this condition presentation layer plays a role translator.\n\n    Translation : Before being transmitted, information in the form of characters and numbers should be changed to bit streams. The presentation layer is responsible for interoperability between encoding methods as different computers use different encoding methods. It translates data between the formats the network requires and the format the computer.\n    Encryption : It carries out encryption at the transmitter and decryption at the receiver.\n    Compression : It carries out data compression to reduce the bandwidth of the data to be transmitted. The primary role of Data compression is to reduce the number of bits to be 0transmitted. It is important in transmitting multimedia such as audio, video, text etc.",
      "Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. Dijkstra's algorithm to find the shortest path between a and b. It picks the unvisited vertex with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor's distance if smaller. Mark visited (set to red) when done with neighbors.\nLet the closest node to 1 at some step be i. Then i is shifted to P. Now, for each node j , the closest path to 1 either passes through i or it doesn't.  In the first case Dj remains the same. In the second case, the revised estimate of Dj is the sum Di  +  di,j . So we take the minimum of these two cases and update Dj accordingly.  As each of the nodes get transferred to set P, the estimates get closer to the lowest possible value. When a node is transferred, its shortest path length is known. So finally all the nodes are in P and the Dj 's represent the minimum costs. The algorithm is guaranteed to terminate in N-1 iterations and  its complexity is O( N2 ).\n At the end each node will be  labeled (see Figure.1) with its distance from source node along the best known path. Initially, no paths are known, so all nodes are labeled with infinity. As the algorithm proceeds and paths are found, the labels may change reflecting better paths. Initially, all labels are tentative. When it is discovered that a label represents the shortest possible path from the source to that node, it is made permanent and never changed thereafter.\n\nLook at the weighted undirected graph of  Figure.1(a), where the weights represent, for example, distance. We want to find shortest path from  A to D. We start by making node A as permanent, indicated by a filled in circle. Then we examine each of the nodes adjacent to A (the working node), relabeling each one with the distance to A. Whenever a node is relabeled, we also label it with the node from which the probe was made so that we can construct the final path later. Having examined each of the nodes adjacent to A, we examine all the tentatively labeled nodes in the whole graph and make the one with the smallest label permanent, as shown in Figure.1(b). This one becomes new working node.\n\nWe now start at B, and examine all nodes adjacent to it. If the sum of the label on B and the distance from B to the node being considered is less than the label on the node, we have a shorter path, so the node is relabeled. After all the nodes adjacent to the working node have been inspected and the tentative labels changed if possible, the entire graph is searched for the tentatively labeled node with the smallest value. This node is made permanent and becomes the working node for the next round. The Figure. 1 shows the first five steps of the algorithm.",
      "ATM (Asynchronous Transfer Mode) has been advocated as an important technology for all types of services and networks. Most people believe that ATM will be the standard for the future B-ISDN (Broadband Integrated Services Digital Network). From the service point of view, ATM combines both the data and multimedia information into the wired networks while scales well from backbones to the customer premises networks. To ensure the success of ATM, lots of the design issues have been standardized by ATM Forum. \nA system reference model for WATM is shown in Figure 1. The overall system consists of a fixed ATM network infrastructure and a radio access segment. In the fixed ATM network, the switches, which communicate directly with wireless station or wireless end user devices, are mobility enhanced ATM switches. These switches setup connections on behalf of the wireless devices. They serve as the \"entrance\" to the infrastructure wired ATM networks. The other ATM switching elements in the wired ATM networks remain unchanged.\nIn fixed wireless LANs, or network interconnection via satellite or microwaves links, the end user devices and switching devices are fixed. They establish connections with each other via wireless channel, not through cable. In these kinds of applications, the data transmissions are wireless, yet without mobility. Since the user devices do not roam around, some design issues, e.g. handover, location management, and re-routing, are not presented. \nTo support wireless communication, new wireless channel specific physical, medium access and data link layers are need to be added below the ATM network layer. These layers are called Radio Access Layer in the WATM network. The following sections address the design issues of the Radio Access Layer.\nWhile a fixed station may own an 25 Mbit/s up to 155 Mbit/s data rate ATM link, a 25 Mbit/s data link in a wireless environment is currently difficult to implement. A several GHz spectrum would be required to provide high speed wireless transmission. Currently, 5 GHz band is considered to be used to provide 51 Mbit/s channel with advanced modulation and special coding techniques. Although 155 Mbit/s is unreachable due to the limitation of today's techniques, people believe that it will soon be available in the 60 GHz band and 622 Mbit/s would be reached in the not-too-distant future. Based on this belief, two separate PHY layer specifications are recommended, one for 5 GHz band, one for the 60 GHz band since they will require different operations.",
      "TCP was specifically designed to provide a reliable end to end byte stream over an unreliable internetwork. Each machine supporting TCP has a TCP transport entity either a user process or part of the kernel that manages TCP streams and interface to IP layer. A TCP entity accepts user data streams from local processes, breaks them up into pieces not exceeding 64KB and sends each piece as a separate IP datagram. Client Server mechanism is not necessary for TCP to behave properly.\n\nThe IP layer gives no guarantee that datagram will be delivered properly, so it is up to TCP to timeout and retransmit, if needed. Duplicate, lost and out of sequence packets are handled using the sequence number, acknowledgements, retransmission, timers, etc to provide a reliable service. Connection is a must for this service.Bit errors are taken care of by the CRC checksum. One difference from usual sequence numbering is that each byte is given a number instead of each packet. This is done so that at the time of transmission in case of loss, data of many small packets can be combined together to get a larger packet, and hence smaller overhead.\n\nTCP connection is a duplex connection. That means there is no difference between two sides once the connection is established.\n The \"three-way handshake\" is the procedure used to establish a connection. This procedure normally is initiated by one TCP and responded to by another TCP. The procedure also works if two TCP simultaneously initiate the procedure. When simultaneous attempt occurs, each TCP receives a \"SYN\" segment which carries no acknowledgment after it has sent a \"SYN\". Of course, the arrival of an old duplicate \"SYN\" segment can potentially make it appear, to the recipient, that a simultaneous connection initiation is in progress. Proper use of \"reset\" segments can disambiguate these cases.\n\nThe three-way handshake reduces the possibility of false connections. It is the implementation of a trade-off between memory and messages to provide information for this checking.\n\nThe simplest three-way handshake is shown in figure below. The figures should be interpreted in the following way. Each line is numbered for reference purposes. Right arrows (-->) indicate departure of a TCP segment from TCP A to TCP B, or arrival of a segment at B from A. Left arrows (<--), indicate the reverse. Ellipsis (...) indicates a segment which is still in the network (delayed). TCP states represent the state AFTER the departure or arrival of the segment (whose contents are shown in the center of each line). Segment contents are shown in abbreviated form, with sequence number, control flags, and ACK field. Other fields such as window, addresses, lengths, and text have been left out in the interest of clarity.\nPiggybacking of acknowledments:The ACK for the last received packet need not be sent as a new packet, but gets a free ride on the next outgoing data frame(using the ACK field in the frame header). The technique is temporarily delaying outgoing ACKs so that they can be hooked on the next outgoing data frame is known as piggybacking. But ACK can't be delayed for a long time if receiver(of the packet to be acknowledged) does not have any data to send.\nFlow and congestion control:TCP takes care of flow control by ensuring that both ends have enough resources and both can handle the speed of data transfer of each other so that none of them gets overloaded with data. The term congestion control is used in almost the same context except that resources and speed of each router is also taken care of. The main concern is network resources in the latter case.\nMultiplexing / Demultiplexing: Many application can be sending/receiving data at the same time. Data from all of them has to be multiplexed together. On receiving some data from lower layer, TCP has to decide which application is the recipient. This is called demultiplexing. TCP uses the concept of port number to do this.",
      "Full-duplex data transmission means that data can be transmitted in both directions on a signal carrier at the same time. For example, on a local area network with a technology that has full-duplex transmission, one workstation can be sending data on the line while another workstation is receiving data. Full-duplex transmission necessarily implies a bidirectional line (one that can move data in both directions).",
      "An application layer is an abstraction layer that specifies the shared protocols and interface methods used by hosts in a communications network. The application layer abstraction is used in both of the standard models of computer networking: the Internet Protocol Suite (TCP/IP) and the Open Systems Interconnection model (OSI model).\n\nIn TCP/IP, the application layer contains the communications protocols and interface methods used in process-to-process communications across an Internet Protocol (IP) computer network. The application layer only standardizes communication and depends upon the underlying transport layer protocols to establish host-to-host data transfer channels and manage the data exchange in a client-server or peer-to-peer networking model. Though the TCP/IP application layer does not describe specific rules or data formats that applications must consider when communicating, the original specification (in RFC 1123) does rely on and recommend the robustness principle for application design.\nThere are commonly reoccurring problems that occur in the design and implementation of communication protocols and can be addressed by patterns from several different pattern languages: Pattern Language for Application-level Communication Protocols (CommDP),[2][3] Service Design Patterns,[4] Patterns of Enterprise Application Architecture,[5] Pattern-Oriented Software Architecture: A Pattern Language for Distributed Computing.[6] The first of these pattern languages focuses on the design of protocols and not their implementations. The others address issues in either both areas or just the latter.",
      "Source routing is similar in concept to virtual circuit routing. It is implemented as under:\n\n    Initially, a path between nodes wishing to communicate is found out, either by flooding or by any other suitable method.\n    This route is then specified in the header of each packet routed between these two nodes. A route may also be specified partially, or in terms of some intermediate hops. \n\nAdvantages:\n\n    Bridges do not need to lookup their routing tables since the path is already specified in the packet itself.\n    The throughput of the bridges is higher, and this may lead to better utilization of bandwidth, once a route is established. \n\nDisadvantages:\n\n    Establishing the route at first needs an expensive search method like flooding.\n    To cope up with dynamic relocation of nodes in a network, frequent updates of tables are required, else all packets would be sent in wrong direction. This too is expensive."
    ]
  },
  {
    "Mobile_and_sensor_systems": [
      "A gyroscope identifies up/down, left/right and rotation around three axes for more complex orientation details.\n\nVibration gyro sensor manufacturers are using a variety of materials and structures in an effort to devise compact, high-accuracy gyro sensors that have good characteristics, including:\nscale factor\ntemperature-frequency coefficient\ncompact size\n shock resistance\nstability\n noise characteristics \nVibration gyro sensors sense angular velocity from the Coriolis force applied to a vibrating object.",
      "A touch sensor is a type of equipment that captures and records physical touch or embrace on a device and/or object. It enables a device or object to detech touch, typically by a human user or operator. A touch sensor may also be called a touch detector.\n\n A touch sensor primarily works when an object or individual gets in physical contact with it. Unlike a button or other more manual control, touch sensors are more sensitive, and are often able to respond differently to different kinds of touch, such as tapping, swiping and pinching. Touch sensors are used in consumer tech devices such as smartphones and tablet computers. Typically, touch sensors are used as a means to take input from the user. Each physical stroke that a touch sensor records is sent to a processing unit/software that processes it accordingly. For example, when navigating through a smartphone or using an application, the touch sensor captures the human touches or the applied pressure across the screen. Each interaction with the user across the screen might have a different meaning for the device and/or the application.",
      "a sensor is a piece of tech that measures a physical quantity and converts it into a signal. This signal can then be used by an app or read by an individual.\nenvironmental sensors, which measure things like temperature, barometric pressure and light, although these are more commonly found in higher-end phones. These are used to do everything from tell you when your skin is getting hot through to boosting brightness levels in dark environments.",
      "A proximity sensor detects when the the phone is held to the face to make or take a call, so the touch screen display can be disabled to avoid unintended input.\nProximity sensing is the ability of a robot to tell when it is near an object, or when something is near it. This sense keeps a robot from running into things. It can also be used to measure the distance from a robot to some object.\nThe simplest proximity sensors do not measure distance. A bumper can be passive, simply making the robot bounce away from things it hits. More often, a bumper has a switch that closes when it makes contact, sending a signal to the controller causing the robot to back away. When whiskers hit something, they vibrate. This can be detected, and a signal sent to the robot controller.\nA photoelectric proximity sensor uses a light-beam generator, a photodetector, a special amplifier, and a microprocessor. The light beam reflects from an object and is picked up by the photodetector. The light beam is modulated at a specific frequency, and the detector has a frequency-sensitive amplifier that responds only to light modulated at that frequency. This prevents false imaging that might otherwise be caused by lamps or sunlight. If the robot is approaching a light-reflecting object, its microprocessor senses that the reflected beam is getting stronger. The robot can then steer clear of the object. This method of proximity sensing won't work for black objects, or for things like windows or mirrors approached at a sharp angle.\nAn acoustic proximity sensor works on the same principle as sonar. A pulsed signal, having a frequency somewhat above the range of human hearing, is generated by an oscillator . This signal is fed to a transducer that emits ultrasound pulses at various frequencies in a coded sequence. These pulses reflect from nearby objects and are returned to another transducer, which converts the ultrasound back into high-frequency pulses. The return pulses are amplified and sent to the robot controller. The delay between the transmitted and received pulses is timed, and this will give an indication of the distance to the obstruction. The pulse coding prevents errors that might otherwise occur because of confusion between adjacent pulses.\nA capacitive proximity sensor uses a radio-frequency ( RF ) oscillator, a frequency detector, and a metal plate connected into the oscillator circuit. The oscillator is designed so that a change in the capacitance of the plate, with respect to the environment, causes the frequency to change. This change is sensed by the frequency detector, which sends a signal to the apparatus that controls the robot. In this way, a robot can avoid bumping into things. Objects that conduct electricity to some extent, such as house wiring, animals, cars, or refrigerators, are sensed more easily by capacitive transducers than are things that do not conduct, like wood-frame beds and dry masonry walls.",
      "A humidity sensor (or hygrometer) senses, measures and reports the relative humidity in the air. It therefore measures both moisture and air temperature. Relative humidity is the ratio of actual moisture in the air to the highest amount of moisture that can be held at that air temperature. The warmer the air temperature is, the more moisture it can hold. Humidity / dew sensors use capacitive measurement, which relies on electrical capacitance. Electrical capacity is the ability of two nearby electrical conductors to create an electrical field between them. The sensor is composed of two metal plates and contains a non-conductive polymer film between them. This film collects moisture from the air, which causes the voltage between the two plates to change. These voltage changes are converted into digital readings showing the level of moisture in the air.\nThere are many different kinds of humidity / dew sensors and at Future Electronics we stock many of the most common types categorized by accuracy, operating temperature range, humidity range, supply voltage, packaging type and supply current. The parametric filters on our website can help refine your search results depending on the required specifications. \nThe most common sizes for supply voltage are 3 to 5.5 V and 4.75 to 5.25 V. We also carry humidity / dew sensors with supply voltage as high as 15 V. Supply current can be between 100 µA and 15 mA, with the most common humidity / dew sensor chips using a supply current of 100 µA, 500 µA and 2.8 to 4 mA.\nFuture Electronics has a full chip selection of humidity / dew sensors from several manufacturers that can be used to design a relative humidity sensor, temperature and humidity monitor, moisture sensor, humidity sensor IC (integrated circuit), humidity sensor switch, digital home humidity sensor, wireless humidity sensor, digital humidity meter, soil moisture sensor, dew point sensor, remote humidity sensor or for any other application that needs humidity measurement. Simply choose from the humidity / dew sensor technical attributes below and your search results will quickly be narrowed to match your specific humidity / dew sensor application needs.",
      "A wireless sensor network (WSN) is a wireless network consisting of spatially distributed autonomous devices using sensors to monitor physical or environmental conditions. A WSN system incorporates a gateway that provides wireless connectivity back to the wired world and distributed nodes (see Figure 1). The wireless protocol you select depends on your application requirements.\n\nA mobile wireless sensor network (MWSN)[1] can simply be defined as a wireless sensor network (WSN) in which the sensor nodes are mobile. MWSNs are a smaller, emerging field of research in contrast to their well-established predecessor. MWSNs are much more versatile than static sensor networks as they can be deployed in any scenario and cope with rapid topology changes. However, many of their applications are similar, such as environment monitoring or surveillance. Commonly, the nodes consist of a radio transceiver and a microcontroller powered by a battery, as well as some kind of sensor for detecting light, heat, humidity, temperature, etc.",
      "A Mobile Sensing System (MSS) requires a user level Application (App) running on the phone for reading an internal phone's sensor, or external sensors in the WSN and reporting sensed data to the Web. To do this, the phone operating system must offer an Application Programming Interface (API) to manage the data reading and reporting.\nMost mobile phones include the following sensors: GPS for outdoors localization, an Accelerometer Sensor (AS) to measure acceleration, a Compass Sensor (CS) to determine the angle by which the phone is rotated relative to the Earth's magnetic North Pole, a Gyroscope Sensor (GS) to measure the angular rate of how quickly the object turns, an Image Sensor (IS) to capture images and record videos, an Ambient Light Sensor (ALS) to detect how much luminance is present, a Proximity Sensor (PS) to detect how close the phone is to the user's body, Touch Sensors (TS) to detect the presence and location of a touch with a finger or stylus pen within the display area, and a Temperature Sensor (TS), Humidity Sensor (HS) and Atmospheric Pressure Sensor (APS) to detect real-time environmental temperature, humidity and atmosphere pressure, respectively. AS, CS, GS and GPS are mainly used in mobile sensing.",
      "Mobile wireless sensor network. ... A mobile wireless sensor network (MWSN) can simply be defined as a wireless sensor network (WSN) in which the sensor nodes are mobile. MWSNs are a smaller, emerging field of research in contrast to their well-established predecessor.",
      "A light sensor detects data about lighting levels in the environment to adapt the display accordingly.\n\n\n\nBased on a patented dual-diode implementation, the Ambient Light Sensing (ALS) devices approximate the human eye response to light under a variety of lighting conditions. The devices have a wide range of performance allowing accurate ALS measurements in lighting environments ranging from low-light to bright sunlight up to 220K lux. This family of devices is particularly useful for display management dimming or brightness control with the purpose of reducing power consumption, extending battery life, and providing the optimum viewing in diverse lighting conditions.",
      "By using the GPS sensor, you can get current information from the GPS receiver, such as latitude and longitude coordinates, bearing, and speed\nLatitude\nLongitude\nAltitude\nTime of fix (the time the reading is taken)\nStatus (when currently receiving data)\nNum Sats (the number of satellite links)\nBearing (the current direction or heading)\n\n\nWith a Global Positioning System (GPS) receiver connected to your computer or smartphone, you can see your current location on the MapPoint map. When a GPS receiver is installed, MapPoint checks for your location once every second and displays it on the map. You can configure the map to remain centered on your current location, to rotate to follow your travel direction, and to display a trail that shows where you have traveled. You can also use your GPS receiver to provide your location to the navigation feature when you are following directions that you obtained by creating a route with MapPoint. \nA GPS receiver takes in data from a constellation of 24 satellites that orbit the Earth. These satellites are arranged so that at least four are always visible in the sky from anywhere on Earth. A GPS receiver attempts to locate signals from at least three satellites, but preferably four or more. With these signals, your latitude and longitude, altitude, speed, and direction can be determined anywhere on Earth and in any weather. \nLocations are calculated through a process called three-dimensional trilateration, a mathematical formula that uses the positions of the satellites and their distance from the Earth (based on the amount of time the signal takes to reach the receiver) to determine the point at which the satellite signals and the surface of the Earth intersect. After the GPS receiver acquires the satellite signals, the receiver continually recalculates your position as you travel and provides that data to MapPoint as a series of latitude and longitude pairs that can be shown on a map.",
      "Temperature sensor that uses an external diode-connected transistor as the sensing element to measure temperatures external to the sensor (for example, on a circuit board or on the die of a CPU). Generally produces a digital output. \n\n    transistor, diode, temperature, thermal, sensing, remote diode, integrated temperature sensor, cpu, sensor, resistor, rtd, thermocouple\n\n     A temperature sensor formed by the junction of two dissimilar metals. A thermocouple produces a voltage proportional to the difference in temperature between the hot junction and the lead wire (cold) junction. \n\n     A remotely located PN junction used as a temperature sensing device, usually located on an integrated circuit other than the one doing the measurement. \n     A Resistance Temperature Detector (RTD) is a device with a significant temperature coefficient (that is, its resistance varies with temperature). It is used as a temperature measurement device, usually by passing a low-level current through it and measuring the voltage drop. A thermistor is a common type of RTD.",
      "An accelerometer detects acceleration, tilt and vibration to determine movement and orientation.\n\nMost accelerometers are Micro-Electro-Mechanical Sensors (MEMS). The basic principle of operation behind the MEMS accelerometer is the displacement of a small proof mass etched into the silicon surface of the integrated circuit and suspended by small beams. Consistent with Newton's second law of motion (F = ma), as an acceleration is applied to the device, a force develops which displaces the mass. The support beams act as a spring, and the fluid (usually air) trapped inside the IC acts as a damper, resulting in a second order lumped physical system. This is the source of the limited operational bandwidth and non-uniform frequency response of accelerometers.\n\nThere are several different principles upon which an analog accelerometer can be built. Two very common types utilize capacitive sensing and the piezoelectric effect to sense the displacement of the proof mass proportional to the applied acceleration.\n\nAccelerometers that implement capacitive sensing output a voltage dependent on the distance between two planar surfaces. One or both of these “plates” are charged with an electrical current. Changing the gap between the plates changes the electrical capacity of the system, which can be measured as a voltage output. This method of sensing is known for its high accuracy and stability. Capacitive accelerometers are also less prone to noise and variation with temperature, typically dissipate less power, and can have larger bandwidths due to internal feedback circuitry. (Elwenspoek 1993)\n\n\n\nPiezoelectric sensing of acceleration is natural, as acceleration is directly proportional to force. When certain types of crystal are compressed, charges of opposite polarity accumulate on opposite sides of the crystal. This is known as the piezoelectric effect. In a piezoelectric accelerometer, charge accumulates on the crystal and is translated and amplified into either an output current or voltage.\n\nPiezoelectric accelerometers only respond to AC phenomenon such as vibration or shock. They have a wide dynamic range, but can be expensive depending on their quality (Doscher 2005)\n\nPiezo-film based accelerometers are best used to measure AC phenomenon such as vibration or shock, rather than DC phenomenon such as the acceleration of gravity. They are inexpensive, and respond to other phenomenon such as temperature, sound, and pressure (Doscher 2005)",
      "An Image Sensor is a device that converts an optical image to an electric signal. It is used mostly in digital cameras and other imaging devices. It is a set of charge-coupled devices (CCD) or CMOS sensors such as active-pixel sensors\n An image sensor is a device used primarily in standalone or embedded digital cameras and imaging devices. Typically, when light strikes the lens of a camera, the image sensor captures that light, converts it into an electronic signal and then transmits it to the camera or imaging device processor, which transforms the electronic signal into a digital image. \n\nCharged coupled device (CCD)\nComplementary metal oxide semiconductor (CMOS)",
      "the earth's magnetic field can be respresented as a dipole situated about 440 km off center and inclined 11 degrees to the planet's rotation axis. The strength of the field is 0,5 Gauss, which is quite small. There exist only a few kinds of magnetometers able to read such a weak magnetic field strength\n\nmechanical magnetic compasses\nfluxgate compasses\nhall-effect compasses\nmagnetoelastic compasses\nmagnetoresistive compasses"
    ]
  }
]
